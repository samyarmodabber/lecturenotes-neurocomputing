
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Reservoir computing &#8212; Neurocomputing</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/4-neurocomputing/4-Reservoir.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1. Introduction to Python" href="../5-exercises/ex1-Python.html" />
    <link rel="prev" title="2. Hopfield networks" href="2-Hopfield.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-neurocomputing/4-neurocomputing/4-Reservoir.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Reservoir computing" />
<meta property="og:description" content="Reservoir computing  Slides: pdf  The concept of Reservoir Computing (RC) was developed simultaneously by two researchers at the beginning of the 2000s. RC buil" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-neurocomputing/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neurocomputing</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Reservoir computing
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/4-neurocomputing/4-Reservoir.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#echo-state-networks-esn">
   3.1. Echo-state networks (ESN)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structure">
     3.1.1. Structure
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dynamics-and-edge-of-chaos">
     3.1.2. Dynamics and edge of chaos
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#universal-approximation">
     3.1.3. Universal approximation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applications">
     3.1.4. Applications
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#taming-chaos-optional">
   3.2. Taming chaos (optional)
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="reservoir-computing">
<h1><span class="section-number">3. </span>Reservoir computing<a class="headerlink" href="#reservoir-computing" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/4.5-Reservoir.pdf">pdf</a></p>
<p>The concept of <strong>Reservoir Computing</strong> (RC) was developed simultaneously by two researchers at the beginning of the 2000s.
RC builds on the idea of Hopfield networks but focuses on the dynamics rather than on the fixed points.
Herbert Jaeger (Uni Bremen) introduced <strong>echo-state networks</strong> (ESN) using rate-coded neurons <a class="bibtex reference internal" href="../zreferences.html#jaeger2001" id="id1">[Jaeger, 2001]</a>.
Wolfgang Maass (TU Graz) introduced <strong>liquid state machines</strong> (LSM) using spiking neurons <a class="bibtex reference internal" href="../zreferences.html#maass2002" id="id2">[Maass et al., 2002]</a>.</p>
<div class="section" id="echo-state-networks-esn">
<h2><span class="section-number">3.1. </span>Echo-state networks (ESN)<a class="headerlink" href="#echo-state-networks-esn" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/l0fRz4IbMTY' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="structure">
<h3><span class="section-number">3.1.1. </span>Structure<a class="headerlink" href="#structure" title="Permalink to this headline">¶</a></h3>
<p>An ESN is a set of <strong>recurrent</strong> units (sparsely connected) exhibiting complex spatiotemporal dynamics.</p>
<div class="figure align-default" id="id12">
<a class="reference internal image-reference" href="../_images/RC-principle2.png"><img alt="../_images/RC-principle2.png" src="../_images/RC-principle2.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.82 </span><span class="caption-text">Structure of an echo-state network. Source: <a class="bibtex reference internal" href="../zreferences.html#tanaka2019a" id="id3">[Tanaka et al., 2019]</a>.</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p>Rate-coded neurons in the reservoir integrate inputs and recurrent connections using an ODE:</p>
<div class="math notranslate nohighlight">
\[
    \tau \, \frac{dx_j(t)}{dt} + x_j(t) = \sum_i W^\text{IN}_{ij} \, I_i(t) + \sum_i W_{ij} \, r_i(t) + \xi(t)
\]</div>
<p>The output of a neuron uses the tanh function (between -1 and 1):</p>
<div class="math notranslate nohighlight">
\[
    r_j(t) = \tanh(x_j(t))
\]</div>
<p><strong>Readout neurons</strong> (or output neurons) transform linearly the activity in the reservoir:</p>
<div class="math notranslate nohighlight">
\[
    z_k(t) = \sum_j W^\text{OUT}_{jk} \, r_j(t) 
\]</div>
<p>In the original version of the ESN, only the readout weights are learned, not the recurrent ones.
One can use <strong>supervised learning</strong> to train the readout neurons to reproduce desired targets.</p>
<p>Inputs <span class="math notranslate nohighlight">\(\mathbf{I}(t)\)</span> bring the <strong>recurrent units</strong> in a given state (like the bias in Hopfield networks).
The recurrent connections inside the reservoir create different <strong>dynamics</strong> <span class="math notranslate nohighlight">\(\mathbf{r}(t)\)</span> depending on the strength of the weight matrix.
Readout neurons <strong>linearly</strong> transform the recurrent dynamics into temporal outputs <span class="math notranslate nohighlight">\(\mathbf{z}(t)\)</span>.
<strong>Supervised learning</strong> (perceptron, LMS) trains the readout weights to reproduce a target <span class="math notranslate nohighlight">\(\mathbf{t}(t)\)</span>.
It is similar to a MLP with one hidden layer, but the hidden layer has dynamics.</p>
<p>Reservoirs only need a few hundreds of units in the reservoir to learn complex functions (e.g. <span class="math notranslate nohighlight">\(N=200\)</span>).
The recurrent weights are initialized randomly using a <strong>normal distribution</strong> with mean 0 and deviation <span class="math notranslate nohighlight">\(\frac{g}{\sqrt{N}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[w_{ij} \sim \mathcal{N}(0, \frac{g}{\sqrt{N}})\]</div>
<p><span class="math notranslate nohighlight">\(g\)</span> is a <strong>scaling factor</strong> characterizing the strength of the recurrent connections, what leads to different dynamics.</p>
<p>The recurrent weight matrix is often <strong>sparse</strong>:  A subset of the possible connections <span class="math notranslate nohighlight">\(N \times N\)</span> has non-zero weights.
Typically, only 10% of the possible connections are created.</p>
</div>
<div class="section" id="dynamics-and-edge-of-chaos">
<h3><span class="section-number">3.1.2. </span>Dynamics and edge of chaos<a class="headerlink" href="#dynamics-and-edge-of-chaos" title="Permalink to this headline">¶</a></h3>
<p>Depending on the value of <span class="math notranslate nohighlight">\(g\)</span>, the dynamics of the reservoir exhibit different attractors.
Let’s have a look at the activity of a few neurons after the presentation of a short input.</p>
<p>When <span class="math notranslate nohighlight">\(g&lt;1\)</span>, the network has no dynamics: the activity quickly fades to 0 when the input is removed.</p>
<p><img alt="" src="../_images/reservoir-dynamics-0.png" /></p>
<p>For <span class="math notranslate nohighlight">\(g=1\)</span>, the reservoir exhibits some <strong>transcient dynamics</strong> but eventually fades to 0 (echo-state property).</p>
<p><img alt="" src="../_images/reservoir-dynamics-1.png" /></p>
<p>For <span class="math notranslate nohighlight">\(g=1.5\)</span>, the reservoir exhibits many <strong>stable attractors</strong> due to its rich dynamics (Hopfield-like).</p>
<p><img alt="" src="../_images/reservoir-dynamics-2.png" /></p>
<p>For higher values of <span class="math notranslate nohighlight">\(g\)</span>, there are no stable attractors anymore: <strong>chaotic behavior</strong>.</p>
<p><img alt="" src="../_images/reservoir-dynamics-3.png" /></p>
<p>For <span class="math notranslate nohighlight">\(g = 1.5\)</span>, different inputs (initial states) lead to different attractors.</p>
<p><img alt="" src="../_images/reservoir-dynamics-represent.png" /></p>
<p>The weight matrix must have a scaling factor above 1 to exhibit non-zero attractors.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/reservoir-dynamics-attractor.png"><img alt="../_images/reservoir-dynamics-attractor.png" src="../_images/reservoir-dynamics-attractor.png" style="width: 60%;" /></a>
</div>
<p>For a single input, the attractor is always the same, even in the presence of noise or perturbations.</p>
<p><img alt="" src="../_images/reservoir-dynamics-reproduce1.png" /></p>
<p>In the chaotic regime, the slightest uncertainty on the initial conditions (or the presence of noise) produces very different trajectories on the long-term.</p>
<p><img alt="" src="../_images/reservoir-dynamics-reproduce2.png" /></p>
<p>The chaotic regime appears for <span class="math notranslate nohighlight">\(g &gt; 1.5\)</span>. <span class="math notranslate nohighlight">\(g=1.5\)</span> is the <strong>edge of chaos</strong>: the dynamics are very rich, but the network is not chaotic yet.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/reservoir-dynamics-chaos.png"><img alt="../_images/reservoir-dynamics-chaos.png" src="../_images/reservoir-dynamics-chaos.png" style="width: 60%;" /></a>
</div>
<div class="admonition-lorenz-attractor admonition">
<p class="admonition-title">Lorenz attractor</p>
<p>The <strong>Lorenz attractor</strong> is a famous example of a chaotic attractor.
The position <span class="math notranslate nohighlight">\(x, y, z\)</span> of a particle is describe by a set of 3 <strong>deterministic</strong> ordinary differential equations:</p>
<div class="math notranslate nohighlight">
\[\frac{dx}{dt} = \sigma \, (y -  x)\]</div>
<div class="math notranslate nohighlight">
\[\frac{dy}{dt} = x \, (\rho - z) - y\]</div>
<div class="math notranslate nohighlight">
\[\frac{dz}{dt} = x\, y - \beta \, z\]</div>
<p>The resulting trajectories over time have complex dynamics and are <strong>chaotic</strong>: the slightest change in the initial conditions generates different trajectories.</p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/dP3qAq9RNLg' frameborder='0' allowfullscreen></iframe></div>
</div>
</div>
<div class="section" id="universal-approximation">
<h3><span class="section-number">3.1.3. </span>Universal approximation<a class="headerlink" href="#universal-approximation" title="Permalink to this headline">¶</a></h3>
<p>Using the reservoir as input, the linear readout neurons can be trained to reproduce <strong>any non-linear</strong> target signal over time:</p>
<div class="math notranslate nohighlight">
\[
    z_k(t) = \sum_j W^\text{OUT}_{jk} \, r_j(t) 
\]</div>
<p>As it is a regression problem, the <strong>delta learning rule</strong> (LMS) is often enough.</p>
<div class="math notranslate nohighlight">
\[
    \Delta W^\text{OUT}_{jk} = \eta \, (t_k(t) - z_k(t)) \, r_j(t) 
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/reservoir-fit.png"><img alt="../_images/reservoir-fit.png" src="../_images/reservoir-fit.png" style="width: 70%;" /></a>
</div>
<p>Reservoirs are <strong>universal approximators</strong>: given enough neurons in the reservoir and dynamics at the edge of the chaos, a RC network can approximate any non-linear function between an input signal <span class="math notranslate nohighlight">\(\mathbf{I}(t)\)</span> and a target signal <span class="math notranslate nohighlight">\(\mathbf{t}(t)\)</span>.</p>
<p>The reservoir projects a low-dimensional input into a high-dimensional <strong>spatio-temporal feature space</strong> where trajectories becomes linearly separable. The reservoir increases the distance between the input patterns.
Input patterns are separated in both space (neurons) and time: the readout neurons need much less weights than the equivalent MLP: <strong>better generalization and faster learning</strong>.
The only drawback is that it does not deal very well with high-dimensional inputs (images).</p>
<div class="figure align-default" id="id13">
<a class="reference internal image-reference" href="../_images/rc-patternseparation.png"><img alt="../_images/rc-patternseparation.png" src="../_images/rc-patternseparation.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.83 </span><span class="caption-text">Spatio-temporal pattern separation. Source: Seoane, L. F. (2019). Evolutionary aspects of reservoir computing. Philosophical Transactions of the Royal Society B. doi:10.1098/rstb.2018.0377.</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<p>The output of the readout neurons can be <strong>fed back</strong> into the reservoir to stabilize the trajectories:</p>
<div class="math notranslate nohighlight">
\[
    \tau \, \frac{dx_j(t)}{dt} + x_j(t) = \sum_i W^\text{IN}_{ij} \, I_i(t) + \sum_i W_{ij} \, r_i(t) + \sum_i W^\text{FB}_{kj} \, z_k(t) + \xi(t)
\]</div>
<p>This makes the reservoir much more robust to perturbations, especially at the edge of chaos.
The trajectories are more stable (but still highly dynamical), making the job of the readout neurons easier.</p>
</div>
<div class="section" id="applications">
<h3><span class="section-number">3.1.4. </span>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h3>
<p><strong>Forecasting</strong>: ESN are able to predict the future of chaotic systems (stock market, weather) much better than static NN.</p>
<div class="figure align-default" id="id14">
<a class="reference internal image-reference" href="../_images/rc-forecasting.png"><img alt="../_images/rc-forecasting.png" src="../_images/rc-forecasting.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.84 </span><span class="caption-text">Forecasting. Source: <a class="reference external" href="https://towardsdatascience.com/predicting-stock-prices-with-echo-state-networks-f910809d23d4">https://towardsdatascience.com/predicting-stock-prices-with-echo-state-networks-f910809d23d4</a></span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Physics:</strong> RC networks can be used to predict the evolution of chaotic systems (Lorenz, Mackey-Glass, Kuramoto-Sivashinsky) at very long time scales (8 times the Lyapunov time).</p>
<div class="figure align-default" id="id15">
<a class="reference internal image-reference" href="../_images/rc-flame.gif"><img alt="../_images/rc-flame.gif" src="../_images/rc-flame.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.85 </span><span class="caption-text">Prediction of chaotic systems. Source: Pathak, J., Hunt, B., Girvan, M., Lu, Z., and Ott, E. (2018). Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach. Physical Review Letters 120, 024102–024102. doi:10.1103/PhysRevLett.120.024102.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p><strong>NLP:</strong> RC networks can grasp the dynamics of language, i.e. its <strong>grammar</strong>.
RC networks can be trained to produce <strong>predicates</strong> (“hit(Mary, John)”) from sentences (“Mary hit John” or “John was hit by Mary”)</p>
<div class="figure align-default" id="id16">
<a class="reference internal image-reference" href="../_images/rc-hinaut.png"><img alt="../_images/rc-hinaut.png" src="../_images/rc-hinaut.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.86 </span><span class="caption-text">Reservoirs for language understanding. Source: Hinaut, X., and Dominey, P. F. (2013). Real-Time Parallel Processing of Grammatical Structure in the Fronto-Striatal System: A Recurrent Network Simulation Study Using Reservoir Computing. PLOS ONE 8, e52946. doi:10.1371/journal.pone.0052946.</span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/AUbJAupkU4M' frameborder='0' allowfullscreen></iframe></div>
<p>The cool thing with reservoirs is that they do not have to be simulated by classical von Neumann architectures (CPU, GPU).
Anything able to exhibit dynamics at the edge of chaos can be used: VLSI (memristors), magnetronics, photonics (lasers), spintronics (nanoscale electronics)…  This can limit drastically the energy consumption of ML algorithms (200W for a GPU). Even biological or physical systems can be used…</p>
<div class="figure align-default" id="id17">
<a class="reference internal image-reference" href="../_images/rc-memristor.jpg"><img alt="../_images/rc-memristor.jpg" src="../_images/rc-memristor.jpg" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.87 </span><span class="caption-text">Memristor-based RC networks. Source: <a class="bibtex reference internal" href="../zreferences.html#tanaka2019a" id="id4">[Tanaka et al., 2019]</a>.</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id18">
<a class="reference internal image-reference" href="../_images/liquidbrain.png"><img alt="../_images/liquidbrain.png" src="../_images/liquidbrain.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.88 </span><span class="caption-text">A bucket of water can be used as a reservoir. Different motors provide inputs to the reservoir by creating weights. The surface of the bucket is recorded and used as an input to a linear algorithm. It can learn non-linear operations (XOR) or even speech recognition.. Source: Fernando, C., and Sojakka, S. (2003). Pattern Recognition in a Bucket. in Advances in Artificial Life Lecture Notes in Computer Science. doi:10.1007/978-3-540-39432-7_63.</span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
<p>Real biological neurons can be kept alive in a culture and stimulated /recorded to implement a reservoir.</p>
<div class="figure align-default" id="id19">
<a class="reference internal image-reference" href="../_images/rc-culture2.jpg"><img alt="../_images/rc-culture2.jpg" src="../_images/rc-culture2.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.89 </span><span class="caption-text">Reservoir of biological neurons. Source: Frega, M., Tedesco, M., Massobrio, P., Pesce, M., and Martinoia, S. (2014). Network dynamics of 3D engineered neuronal cultures: a new experimental model for in-vitro electrophysiology. Scientific Reports 4, 1–14. doi:10.1038/srep05489.</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<p>Escherichia Coli bacteria change their mRNA in response to various external factors (temperature, chemical products, etc) and interact with each other.
Their mRNA encode a dynamical trajectory reflecting the inputs.
By placing them on a microarray, one can linearly learn to perform non-linear operations on the inputs.</p>
<div class="figure align-default" id="id20">
<a class="reference internal image-reference" href="../_images/rc-ecoli.png"><img alt="../_images/rc-ecoli.png" src="../_images/rc-ecoli.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.90 </span><span class="caption-text">Reservoir of e-coli bacteria. Source: Jones, B., Stekel, D., Rowe, J., and Fernando, C. (2007). Is there a Liquid State Machine in the Bacterium Escherichia Coli? in 2007 IEEE Symposium on Artificial Life, 187–191. doi:10.1109/ALIFE.2007.367795.</span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</div>
<p>ESN use the <code class="docutils literal notranslate"><span class="pre">tanh</span></code> activation function (between -1 and +1) and the weights can take any value.
In the brain, neurons are either excitatory (positive outgoing weights) or inhibitory (negative outgoing weights), never both (<strong>Dale’s law</strong>).
Firing rates (outputs) are positive by definition.
It is possible to build ESN with a ratio 80% / 20% of excitatory and inhibitory cells, using ReLU transfer functions. A bit less stable, but works.</p>
<div class="figure align-default" id="id21">
<a class="reference internal image-reference" href="../_images/EI.png"><img alt="../_images/EI.png" src="../_images/EI.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.91 </span><span class="caption-text">Excitatory-inhibitory reservoir. Source: Mastrogiuseppe, F., and Ostojic, S. (2017). Intrinsically-generated fluctuating activity in excitatory-inhibitory networks. PLOS Computational Biology 13, e1005498–e1005498. doi:10.1371/journal.pcbi.1005498.</span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</div>
<p>RC networks can be used to model different areas, including the cerebellum, the olfactory system, the hippocampus, cortical columns, etc.
The brain has a highly dynamical recurrent architecture, so RC provides a good model of brain dynamics.</p>
<div class="figure align-default" id="id22">
<a class="reference internal image-reference" href="../_images/rc-biology.jpg"><img alt="../_images/rc-biology.jpg" src="../_images/rc-biology.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.92 </span><span class="caption-text">Reservoirs are useful in computational neuroscience. Source: Cayco-Gajic, N. A., and Silver, R. A. (2019). Re-evaluating Circuit Mechanisms Underlying Pattern Separation. Neuron 101, 584–602. doi:10.1016/j.neuron.2019.01.044.</span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="taming-chaos-optional">
<h2><span class="section-number">3.2. </span>Taming chaos (optional)<a class="headerlink" href="#taming-chaos-optional" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/BWJZ_mibooc' frameborder='0' allowfullscreen></iframe></div>
<p>In classical RC networks, the recurrent weights are fixed and only the readout weights are trained.
The reservoir dynamics are fixed by the recurrent weights, we cannot change them.
Dynamics can be broken by external perturbations or high-amplitude noise.
The <strong>edge of chaos</strong> is sometimes too close.
If we could learn the recurrent weights, we could force the reservoir to have fixed and robust trajectories.</p>
<div class="figure align-default" id="id23">
<a class="reference internal image-reference" href="../_images/rc-sussillo1.png"><img alt="../_images/rc-sussillo1.png" src="../_images/rc-sussillo1.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.93 </span><span class="caption-text">Classical RC networks have fixed recurrent weights. Source: <a class="bibtex reference internal" href="../zreferences.html#sussillo2009" id="id5">[Sussillo &amp; Abbott, 2009]</a>.</span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</div>
<p>Below a classical network is trained to reproduce handwriting.
The two readout neurons produce a sequence of <span class="math notranslate nohighlight">\((x, y)\)</span> positions for the pen.
It works quite well when the input is not perturbed.
If some perturbation enters the reservoir, the trajectory is lost.</p>
<p><img alt="" src="../_images/rc-buonomano1.png" /></p>
<p>We have an error signal <span class="math notranslate nohighlight">\(\mathbf{t}_t - \mathbf{z}_t\)</span> at each time step.
Why can’t we just apply backpropagation (through time) on the recurrent weights?</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(W, W^\text{OUT}) = \mathbb{E}_{t} [(\mathbf{t}_t - \mathbf{z}_t)^2]\]</div>
<div class="figure align-default" id="id24">
<a class="reference internal image-reference" href="../_images/rc-sussillo2.png"><img alt="../_images/rc-sussillo2.png" src="../_images/rc-sussillo2.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.94 </span><span class="caption-text">Learning the recurrent weights can stabilize trajectories at the edge of chaos. Source: <a class="bibtex reference internal" href="../zreferences.html#sussillo2009" id="id6">[Sussillo &amp; Abbott, 2009]</a>.</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</div>
<p>BPTT is too unstable: the slightest weight change impacts the whole dynamics.
In <strong>FORCE learning</strong> <a class="bibtex reference internal" href="../zreferences.html#sussillo2009" id="id7">[Sussillo &amp; Abbott, 2009]</a>, complex optimization methods such as <strong>recursive least squares</strong> (RLS) have to be used.
For the readout weights:</p>
<div class="math notranslate nohighlight">
\[
    \Delta W^\text{OUT} = - \eta \, (\mathbf{t}_t - \mathbf{z}_t) \times P \times \mathbf{r}_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(P\)</span> is the inverse correlation matrix of the input :</p>
<div class="math notranslate nohighlight">
\[
    P = (\mathbf{r}_t \times \mathbf{r}_t^T)^{-1}
\]</div>
<div class="figure align-default" id="id25">
<a class="reference internal image-reference" href="../_images/RC-results.png"><img alt="../_images/RC-results.png" src="../_images/RC-results.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.95 </span><span class="caption-text">FORCE learning allows to train readout weights to reproduce compley or even chaotic signals. Source: <a class="bibtex reference internal" href="../zreferences.html#sussillo2009" id="id8">[Sussillo &amp; Abbott, 2009]</a>.</span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
<p>For the recurrent weights, we need also an error term <a class="bibtex reference internal" href="../zreferences.html#laje2013" id="id9">[Laje &amp; Buonomano, 2013]</a>.
It is computed by recording the dynamics during an <strong>initialization trial</strong> <span class="math notranslate nohighlight">\(\mathbf{r}^*_t\)</span> and force the recurrent weights to reproduce these dynamics in the learning trials:</p>
<div class="math notranslate nohighlight">
\[
    \Delta W = - \eta \, (\mathbf{r}^*_t - \mathbf{r}_t) \times P \times \mathbf{r}_t
\]</div>
<p><span class="math notranslate nohighlight">\(P\)</span> is the correlation matrix of recurrent activity. See <a class="reference external" href="https://github.com/ReScience-Archives/Vitay-2016">https://github.com/ReScience-Archives/Vitay-2016</a> for a reimplementation.</p>
<p>This allows to stabilize trajectories in the chaotic reservoir (<strong>taming chaos</strong>) and generate complex patterns even in the presence of perturbations.</p>
<div class="figure align-default" id="id26">
<a class="reference internal image-reference" href="../_images/buonomano2.png"><img alt="../_images/buonomano2.png" src="../_images/buonomano2.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.96 </span><span class="caption-text">FORCE learning allows to train recurrent weights. Source: <a class="bibtex reference internal" href="../zreferences.html#laje2013" id="id10">[Laje &amp; Buonomano, 2013]</a>.</span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id27">
<a class="reference internal image-reference" href="../_images/buonomano1.png"><img alt="../_images/buonomano1.png" src="../_images/buonomano1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.97 </span><span class="caption-text">FORCE learning stabilizes trajectories. Source: <a class="bibtex reference internal" href="../zreferences.html#laje2013" id="id11">[Laje &amp; Buonomano, 2013]</a>.</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./4-neurocomputing"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="2-Hopfield.html" title="previous page"><span class="section-number">2. </span>Hopfield networks</a>
    <a class='right-next' id="next-link" href="../5-exercises/ex1-Python.html" title="next page"><span class="section-number">1. </span>Introduction to Python</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>