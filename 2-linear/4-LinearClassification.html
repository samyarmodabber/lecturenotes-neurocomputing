
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Linear classification &#8212; Neurocomputing</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/2-linear/4-LinearClassification.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Multi-class classification" href="5-Multiclassification.html" />
    <link rel="prev" title="3. Regularization" href="3-Regularization.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-neurocomputing/2-linear/4-LinearClassification.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Linear classification" />
<meta property="og:description" content="Linear classification  Slides: pdf  Hard linear classification  &lt;div class=&#39;embed-container&#39;&gt;&lt;iframe src=&#39;https://www.youtube.com/embed/jZoJHIHi6Qw&#39; frameborder" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-neurocomputing/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neurocomputing</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2-linear/4-LinearClassification.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hard-linear-classification">
   4.1. Hard linear classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perceptron-algorithn">
     4.1.1. Perceptron algorithn
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-gradient-descent">
     4.1.2. Stochastic Gradient descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
   4.2. Maximum Likelihood Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#soft-linear-classification-logistic-regression">
   4.3. Soft linear classification : Logistic regression
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-classification">
<h1><span class="section-number">4. </span>Linear classification<a class="headerlink" href="#linear-classification" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/2.4-LinearClassification.pdf">pdf</a></p>
<div class="section" id="hard-linear-classification">
<h2><span class="section-number">4.1. </span>Hard linear classification<a class="headerlink" href="#hard-linear-classification" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/jZoJHIHi6Qw' frameborder='0' allowfullscreen></iframe></div>
<p>The training data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is composed of <span class="math notranslate nohighlight">\(N\)</span> examples <span class="math notranslate nohighlight">\((\mathbf{x}_i, t_i)_{i=1..N}\)</span> , with a d-dimensional input vector <span class="math notranslate nohighlight">\(\mathbf{x}_i \in \Re^d\)</span> and a binary output <span class="math notranslate nohighlight">\(t_i \in \{-1, +1\}\)</span>. The data points where <span class="math notranslate nohighlight">\(t = + 1\)</span> are called the <strong>positive class</strong>, the other the <strong>negative class</strong>.</p>
<div class="figure align-default" id="id1">
<a class="reference internal image-reference" href="../_images/classification-animation1.png"><img alt="../_images/classification-animation1.png" src="../_images/classification-animation1.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.1 </span><span class="caption-text">Binary linear classification of 2D data.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>For example, the inputs <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> can be images (one dimension per pixel) and the positive class corresponds to cats (<span class="math notranslate nohighlight">\(t_i = +1\)</span>), the negative class to dogs (<span class="math notranslate nohighlight">\(t_i = -1\)</span>).</p>
<div class="figure align-default" id="id2">
<a class="reference internal image-reference" href="../_images/cats-dogs.jpg"><img alt="../_images/cats-dogs.jpg" src="../_images/cats-dogs.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.2 </span><span class="caption-text">Binary linear classification of cats vs. dogs images. Source: <a class="reference external" href="http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe">http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe</a></span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>We want to find the hyperplane <span class="math notranslate nohighlight">\((\mathbf{w}, b)\)</span> of <span class="math notranslate nohighlight">\(\Re^d\)</span> that correctly separates the two classes.</p>
<div class="figure align-default" id="id3">
<a class="reference internal image-reference" href="../_images/classification-animation2.png"><img alt="../_images/classification-animation2.png" src="../_images/classification-animation2.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.3 </span><span class="caption-text">The hyperplane separates the input space into two regions.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>For a point <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{D}\)</span>, <span class="math notranslate nohighlight">\(\langle \mathbf{w} \cdot \mathbf{x} \rangle +b\)</span>  is the projection of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>  onto the hyperplane <span class="math notranslate nohighlight">\((\mathbf{w}, b)\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\langle \mathbf{w} \cdot \mathbf{x} \rangle +b &gt; 0\)</span>, the point is
above the hyperplane.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\langle \mathbf{w} \cdot \mathbf{x} \rangle +b &lt; 0\)</span>, the point is
below the hyperplane.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\langle \mathbf{w} \cdot \mathbf{x} \rangle +b = 0\)</span>, the point is
on the hyperplane.</p></li>
</ul>
<div class="figure align-default" id="id4">
<a class="reference internal image-reference" href="../_images/projection1.svg"><img alt="../_images/projection1.svg" src="../_images/projection1.svg" width="80%" /></a>
<p class="caption"><span class="caption-number">Fig. 4.4 </span><span class="caption-text">Projection on an hyperplane.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>By looking at the <strong>sign</strong> of <span class="math notranslate nohighlight">\(\langle \mathbf{w} \cdot \mathbf{x} \rangle +b\)</span>, we can predict the class.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{sign}(x) = \begin{cases} +1 \; \text{if} \; x \geq 0 \\ -1 \; \text{if} \; x &lt; 0 \\ \end{cases}\end{split}\]</div>
<p>Binary linear classification can therefore be made by a single <strong>artificial neuron</strong> using the sign transfer function.</p>
<div class="math notranslate nohighlight">
\[
y = f_{\mathbf{w}, b} (\mathbf{x}) = \text{sign} ( \langle \mathbf{w} \cdot \mathbf{x} \rangle +b )  = \text{sign} ( \sum_{j=1}^d w_j \, x_j +b )
\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is the weight vector and <span class="math notranslate nohighlight">\(b\)</span> is the bias.</p>
<p>Linear classification is the process of finding an hyperplane <span class="math notranslate nohighlight">\((\mathbf{w}, b)\)</span> that correctly separates the two classes. If such an hyperplane can be found, the training set is said <strong>linearly separable</strong>. Otherwise, the problem is <strong>non-linearly separable</strong> and other methods have to be applied (MLP, SVM…).</p>
<div class="figure align-default" id="id5">
<a class="reference internal image-reference" href="../_images/linearlyseparable.png"><img alt="../_images/linearlyseparable.png" src="../_images/linearlyseparable.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.5 </span><span class="caption-text">Linearly and non-linearly spearable datasets.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="perceptron-algorithn">
<h3><span class="section-number">4.1.1. </span>Perceptron algorithn<a class="headerlink" href="#perceptron-algorithn" title="Permalink to this headline">¶</a></h3>
<p>The Perceptron algorithm tries to find the weights and biases minimizing the <strong>mean square error</strong> (<em>mse</em>) or <strong>quadratic loss</strong>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{w}, b) = \mathbb{E}_\mathcal{D} [(t_i - y_i)^2] \approx \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i)^2\]</div>
<p>When the prediction <span class="math notranslate nohighlight">\(y_i\)</span> is the same as the data <span class="math notranslate nohighlight">\(t_i\)</span> for all examples in the training set (perfect classification), the mse is minimal and equal to 0. We can apply gradient descent to find this minimum.</p>
<div class="math notranslate nohighlight">
\[
    \Delta \mathbf{w} = - \eta \, \nabla_\mathbf{w} \, \mathcal{L}(\mathbf{w}, b)
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta b = - \eta \, \nabla_b \, \mathcal{L}(\mathbf{w}, b)
\]</div>
<p>Let’s search for the partial derivative of the quadratic error function with respect to the weight vector:</p>
<div class="math notranslate nohighlight">
\[
    \nabla_\mathbf{w} \, \mathcal{L}(\mathbf{w}, b) = \nabla_\mathbf{w} \,  \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2 = \frac{1}{N} \, \sum_{i=1}^{N} \nabla_\mathbf{w} \,  (t_i - y_i )^2 = \frac{1}{N} \, \sum_{i=1}^{N} \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b)
\]</div>
<p>Everything is similar to linear regression until we get:</p>
<div class="math notranslate nohighlight">
\[
    \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b) = - 2 \, (t_i - y_i) \, \nabla_\mathbf{w} \, \text{sign}( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle +b)
\]</div>
<p>In order to continue with the chain rule, we would need to differentiate <span class="math notranslate nohighlight">\(\text{sign}(x)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b) = - 2 \, (t_i - y_i) \, \text{sign}'( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle +b) \,  \mathbf{x}_i 
\]</div>
<p>But the sign function is <strong>not</strong> differentiable… We will simply pretend that the sign() function is linear, with a derivative of 1:</p>
<div class="math notranslate nohighlight">
\[
    \nabla_\mathbf{w} \,  \mathcal{l}_i (\mathbf{w}, b) = - 2 \, (t_i - y_i) \,   \mathbf{x}_i 
\]</div>
<p>The update rule for the weight vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and the bias <span class="math notranslate nohighlight">\(b\)</span> is therefore the same as in linear regression:</p>
<div class="math notranslate nohighlight">
\[
    \Delta \mathbf{w} =  \eta \, \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i) \, \mathbf{x}_i
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta b = \eta \, \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )
\]</div>
<p>By applying gradient descent on the quadratic error function, one obtains the following algorithm:</p>
<div class="admonition-batch-perceptron admonition">
<p class="admonition-title">Batch perceptron</p>
<ul class="simple">
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(M\)</span> epochs:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{dw} = 0 \qquad db = 0\)</span></p></li>
<li><p><strong>for</strong> each sample <span class="math notranslate nohighlight">\((\mathbf{x}_i, t_i)\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(y_i =  \text{sign}( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{dw} = \mathbf{dw} + (t_i - y_i) \, \mathbf{x}_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(db = db + (t_i - y_i)\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\Delta \mathbf{w} = \eta \, \frac{1}{N} \, \mathbf{dw}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta b = \eta \, \frac{1}{N} \, db\)</span></p></li>
</ul>
</li>
</ul>
</div>
<p>This is called the <strong>batch</strong> version of the Perceptron algorithm. If the data is linearly separable and <span class="math notranslate nohighlight">\(\eta\)</span> is well chosen, it converges to the minimum of the mean square error.</p>
<div class="figure align-default" id="id6">
<a class="reference internal image-reference" href="../_images/classification-animation.gif"><img alt="../_images/classification-animation.gif" src="../_images/classification-animation.gif" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.6 </span><span class="caption-text">Batch perceptron algorithm.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>The <strong>Perceptron algorithm</strong> was invented by the psychologist Frank Rosenblatt in 1958. It was the first algorithmic neural network able to learn linear classification.</p>
<div class="admonition-online-perceptron-algorithm admonition">
<p class="admonition-title">Online perceptron algorithm</p>
<ul class="simple">
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(M\)</span> epochs:</p>
<ul>
<li><p><strong>for</strong> each sample <span class="math notranslate nohighlight">\((\mathbf{x}_i, t_i)\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(y_i =  \text{sign}( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta \mathbf{w} = \eta \, (t_i - y_i) \, \mathbf{x}_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta b = \eta \, (t_i - y_i)\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<p>This algorithm iterates over all examples of the training set and applies the <strong>delta learning rule</strong> to each of them immediately, not at the end on the whole training set. One could check whether there are still classification errors on the training set at the end of each epoch and stop the algorithm. The delta learning rule depends as always on the learning rate <span class="math notranslate nohighlight">\(\eta\)</span>, the error made by the prediction (<span class="math notranslate nohighlight">\(t_i - y_i\)</span>) and the input <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>.</p>
<div class="figure align-default" id="id7">
<a class="reference internal image-reference" href="../_images/classification-animation-online.gif"><img alt="../_images/classification-animation-online.gif" src="../_images/classification-animation-online.gif" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.7 </span><span class="caption-text">Online perceptron algorithm.</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="stochastic-gradient-descent">
<h3><span class="section-number">4.1.2. </span>Stochastic Gradient descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>The mean square error is defined as the <strong>expectation</strong> over the data:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{w}, b) = \mathbb{E}_\mathcal{D} [(t_i - y_i)^2]\]</div>
<p><strong>Batch learning</strong> uses the whole training set as samples to estimate the mse:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{w}, b) \approx \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i)^2\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta \mathbf{w} = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i ) \, \mathbf{x_i}
\]</div>
<p><strong>Online learning</strong> uses a single sample to estimate the mse:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{w}, b) \approx (t_i - y_i)^2\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta \mathbf{w} = \eta \, (t_i - y_i) \, \mathbf{x_i}
\]</div>
<p>Batch learning has less bias (central limit theorem) and is less sensible to noise in the data, but is very slow. Online learning converges faster, but can be instable and overfits (high variance).</p>
<p>In practice, we use a trade-off between batch and online learning called <strong>Stochastic Gradient Descent (SGD)</strong> or <strong>Minibatch Gradient Descent</strong>.</p>
<p>The training set is randomly split at each epoch into small chunks of data (a <strong>minibatch</strong>, usually 32 or 64 examples) and the batch learning rule is applied on each chunk.</p>
<div class="math notranslate nohighlight">
\[
    \Delta \mathbf{w} = \eta \, \frac{1}{K} \sum_{i=1}^{K} (t_i - y_i) \, \mathbf{x_i}
\]</div>
<p>If the <strong>batch size</strong> is well chosen, SGD is as stable as batch learning and as fast as online learning. The minibatches are randomly selected at each epoch (i.i.d).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Online learning is a stochastic gradient descent with a batch size of 1.</p>
</div>
</div>
</div>
<div class="section" id="maximum-likelihood-estimation">
<h2><span class="section-number">4.2. </span>Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/9Hw6nLMiPiI' frameborder='0' allowfullscreen></iframe></div>
<p>Let’s consider <span class="math notranslate nohighlight">\(N\)</span> <strong>samples</strong> <span class="math notranslate nohighlight">\(\{x_i\}_{i=1}^N\)</span> independently taken from a <strong>normal distribution</strong> <span class="math notranslate nohighlight">\(X\)</span>. The probability density function (pdf) of a normal distribution is:</p>
<div class="math notranslate nohighlight">
\[
    f(x ; \mu, \sigma) =  \frac{1}{\sqrt{2\pi \sigma^2}} \, \exp{- \frac{(x - \mu)^2}{2\sigma^2}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the mean of the distribution and <span class="math notranslate nohighlight">\(\sigma\)</span> its standard deviation.</p>
<div class="figure align-default" id="id8">
<a class="reference internal image-reference" href="../_images/MLE2.png"><img alt="../_images/MLE2.png" src="../_images/MLE2.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.8 </span><span class="caption-text">Normal distributions with different parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> explain the data with different likelihoods.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
<p>The problem is to find the values of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> which explain best the observations <span class="math notranslate nohighlight">\(\{x_i\}_{i=1}^N\)</span>.</p>
<p>The idea of MLE is to maximize the joint density function for all observations. This function is expressed by the <strong>likelihood function</strong>:</p>
<div class="math notranslate nohighlight">
\[
    L(\mu, \sigma) = P( x ; \mu , \sigma  )  = \prod_{i=1}^{N} f(x_i ; \mu, \sigma )
\]</div>
<p>When the pdf takes high values for all samples, it is quite likely that the samples come from this particular distribution. The likelihood function reflects the probability that the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> explain the observations <span class="math notranslate nohighlight">\(\{x_i\}_{i=1}^N\)</span>.</p>
<p>We therefore search for the values <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> which <strong>maximize</strong> the likelihood function.</p>
<div class="math notranslate nohighlight">
\[
    \text{max}_{\mu, \sigma} \quad L(\mu, \sigma) = \prod_{i=1}^{N} f(x_i ; \mu, \sigma )
\]</div>
<p>For the normal distribution, the likelihood function is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    L(\mu, \sigma) &amp; = \prod_{i=1}^{N} f(x_i ; \mu, \sigma ) \\
                   &amp; = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi \sigma^2}} \, \exp{- \frac{(x_i - \mu)^2}{2\sigma^2}}\\
                   &amp; =  (\frac{1}{\sqrt{2\pi \sigma^2}})^N \, \prod_{i=1}^{N} \exp{- \frac{(x_i - \mu)^2}{2\sigma^2}}\\
                   &amp; =  (\frac{1}{\sqrt{2\pi \sigma^2}})^N \, \exp{- \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{2\sigma^2}}\\
\end{aligned}
\end{split}\]</div>
<p>To find the maximum of <span class="math notranslate nohighlight">\(L(\mu, \sigma)\)</span>, we need to search where the gradient is equal to zero:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    \dfrac{\partial L(\mu, \sigma)}{\partial \mu} = 0 \\
    \dfrac{\partial L(\mu, \sigma)}{\partial \sigma} = 0 \\
\end{cases}
\end{split}\]</div>
<p>The likelihood function is complex to differentiate, so we consider its logarithm <span class="math notranslate nohighlight">\(l(\mu, \sigma) = \log(L(\mu, \sigma))\)</span> which has a maximum for the same value of <span class="math notranslate nohighlight">\((\mu, \sigma)\)</span> as the log function is monotonic.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    l(\mu, \sigma) &amp; = \log(L(\mu, \sigma)) \\
                   &amp; =  \log \left((\frac{1}{\sqrt{2\pi \sigma^2}})^N \, \exp{- \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{2\sigma^2}} \right)\\
                   &amp; =  - \frac{N}{2} \log (2\pi \sigma^2) - \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{2\sigma^2}\\
\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(l(\mu, \sigma)\)</span> is called the <strong>log-likelihood</strong> function. The maximum of the log-likelihood function respects:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \frac{\partial l(\mu, \sigma)}{\partial \mu} &amp; = \frac{\sum_{i=1}^{N}(x_i - \mu)}{\sigma^2} = 0 \\
    \frac{\partial l(\mu, \sigma)}{\partial \sigma} &amp; = - \frac{N}{2} \frac{4 \pi \sigma}{2 \pi \sigma^2} + \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{\sigma^3} \\
                                                    &amp; = - \frac{N}{\sigma} + \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{\sigma^3} = 0\\
\end{aligned}
\end{split}\]</div>
<p>We obtain:</p>
<div class="math notranslate nohighlight">
\[
    \mu = \frac{1}{N} \sum_{i=1}^{N} x_i  \qquad\qquad    \sigma^2 = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu)^2
\]</div>
<p>Unsurprisingly, the mean and variance of the normal distribution which best explains the data are the mean and variance of the data…</p>
<p>The same principle can be applied to estimate the parameters of any distribution: normal, exponential, Bernouilli, Poisson, etc… When a machine learning method has an probabilistic interpretation (i.e. it outputs probabilities), MLE can be used to find its parameters. One can use global optimization like here, or gradient descent to estimate the parameters iteratively.</p>
</div>
<div class="section" id="soft-linear-classification-logistic-regression">
<h2><span class="section-number">4.3. </span>Soft linear classification : Logistic regression<a class="headerlink" href="#soft-linear-classification-logistic-regression" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/_Zc-k9pXVvE' frameborder='0' allowfullscreen></iframe></div>
<p>In logistic regression, we want to perform a regression, but where the targets <span class="math notranslate nohighlight">\(t_i\)</span> are bounded betwen 0 and 1. We can use a logistic function instead of a linear function in order to transform the net activation into an output:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
    y = \sigma(w \, x + b )  = \frac{1}{1+\exp(-w \, x - b )}
\end{aligned}
\]</div>
<p>Logistic regression can be used in binary classification if we consider <span class="math notranslate nohighlight">\(y = \sigma(w \, x + b )\)</span> as the probability that the example belongs to the positive class (<span class="math notranslate nohighlight">\(t=1\)</span>).</p>
<div class="math notranslate nohighlight">
\[
    P(t = 1 | x; w, b) = y ; \qquad P(t = 0 | x; w, b) = 1 - y
\]</div>
<p>The output <span class="math notranslate nohighlight">\(t\)</span> therefore comes from a Bernouilli distribution <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> of parameter <span class="math notranslate nohighlight">\(p = y = f_{w, b}(x)\)</span>. The probability density function (pdf) is:</p>
<div class="math notranslate nohighlight">
\[f(t | x; w, b) = y^t \, (1- y)^{1-t}\]</div>
<p>If we consider our training samples <span class="math notranslate nohighlight">\((x_i, t_i)\)</span> as independently taken from this distribution, our task is to find the parameterized distribution that best explains the data, which means to find the parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> maximizing the <strong>likelihood</strong> that the samples <span class="math notranslate nohighlight">\(t\)</span> come from a Bernouilli distribution when <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(w\)</span>  and <span class="math notranslate nohighlight">\(b\)</span> are given. We only need to apply <strong>Maximum Likelihood Estimation</strong> (MLE) on this Bernouilli distribution!</p>
<p>The likelihood function for logistic regression is :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    L( w, b) &amp;= P( t | x; w,  b )  = \prod_{i=1}^{N} f(t_i | x_i;  w,  b ) \\
    &amp;= \prod_{i=1}^{N}  y_i^{t_i} \, (1- y_i)^{1-t_i}
\end{aligned}
\end{split}\]</div>
<p>The likelihood function is quite hard to differentiate, so we take the <strong>log-likelihood</strong> function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    l( w, b) &amp;= \log L( w, b) \\
    &amp;=  \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]\\
\end{aligned}
\end{split}\]</div>
<p>or even better: the <strong>negative log-likelihood</strong> which will be minimized using gradient descent:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}( w, b) =  - \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]
\]</div>
<p>We then search for the minimum of the negative log-likelihood function by computing its gradient (here for a single sample):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \frac{\partial \mathcal{l}_i(w, b)}{\partial w}
        &amp;= -\frac{\partial}{\partial w} [ t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i) ] \\
        &amp;= - t_i \, \frac{\partial}{\partial w} \log y_i - (1 - t_i) \, \frac{\partial}{\partial w}\log( 1- y_i) \\
        &amp;= - t_i \, \frac{\frac{\partial}{\partial w} y_i}{y_i} - (1 - t_i) \, \frac{\frac{\partial}{\partial w}( 1- y_i)}{1- y_i} \\
        &amp;= - t_i \, \frac{y_i \, (1 - y_i) \, x_i}{y_i} + (1 - t_i) \, \frac{y_i \, (1-y_i) \, x_i}{1 - y_i}\\
        &amp;= - ( t_i - y_i ) \, x_i\\
\end{aligned}
\end{split}\]</div>
<p>We obtain the same gradient as the linear perceptron, but with a non-linear output function! Logistic regression is therefore a regression method used for classification. It uses a non-linear transfer function <span class="math notranslate nohighlight">\(\sigma(x)=\frac{1}{1+\exp(-x)}\)</span> applied on the net activation:</p>
<div class="math notranslate nohighlight">
\[
    y_i = \sigma(\langle \mathbf{w} \cdot \mathbf{x}_i \rangle + b )
\]</div>
<p>The continuous output <span class="math notranslate nohighlight">\(y\)</span> is interpreted as the probability of belonging to the positive class.</p>
<div class="math notranslate nohighlight">
\[
   P(t_i = 1 | \mathbf{x}_i; \mathbf{w}, b) = y_i ; \qquad P(t_i = 0 | \mathbf{x}_i; \mathbf{w}, b) = 1 - y_i
\]</div>
<p>We minimize the <strong>negative log-likelihood</strong> loss function:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\mathbf{w}, b) =  - \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]
\]</div>
<p>Gradient descent leads to the delta learning rule, using the class as a target and the probability as a prediction:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{cases}
    \Delta \mathbf{w} = \eta \, ( t_i - y_i ) \, \mathbf{x}_i \\
    \\
    \Delta b = \eta \, ( t_i - y_i ) \\
    \end{cases}
\end{split}\]</div>
<div class="admonition-logistic-regression admonition">
<p class="admonition-title">Logistic regression</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{w} = 0 \qquad b = 0\)</span></p></li>
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(M\)</span> epochs:</p>
<ul>
<li><p><strong>for</strong> each sample <span class="math notranslate nohighlight">\((\mathbf{x}_i, t_i)\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(y_i =  \sigma( \langle \mathbf{w} \cdot \mathbf{x}_i \rangle  + b)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta \mathbf{w} = \eta \, (t_i - y_i) \, \mathbf{x}_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta b = \eta \, (t_i - y_i)\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<p>Logistic regression works just like linear classification, except in the way the prediction is done. To know to which class <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> belongs, simply draw a random number between 0 and 1:</p>
<ul class="simple">
<li><p>if it is smaller than <span class="math notranslate nohighlight">\(y_i\)</span> (probability <span class="math notranslate nohighlight">\(y_i\)</span>), it belongs to the positive class.</p></li>
<li><p>if it is bigger than <span class="math notranslate nohighlight">\(y_i\)</span> (probability <span class="math notranslate nohighlight">\(1-y_i\)</span>), it belongs to the negative class.</p></li>
</ul>
<p>Alternatively, you can put a <strong>hard limit</strong> at 0.5:</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(y_i &gt; 0.5\)</span> then the class is positive.</p></li>
<li><p>if <span class="math notranslate nohighlight">\(y_i &lt; 0.5\)</span> then the class is negative.</p></li>
</ul>
<div class="figure align-default" id="id9">
<a class="reference internal image-reference" href="../_images/logisticregression-animation.gif"><img alt="../_images/logisticregression-animation.gif" src="../_images/logisticregression-animation.gif" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.9 </span><span class="caption-text">Logistic regression for soft classification. The confidence scores tells how certain the classification is.</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<p>Logistic regression also provides a <strong>confidence score</strong>:  the closer <span class="math notranslate nohighlight">\(y\)</span> is from 0 or 1, the more confident we can be that the classification is correct. This is particularly important in <strong>safety critical</strong> applications: if you detect the positive class but with a confidence of 0.51, you should perhaps not trust the prediction. If the confidence score is 0.99, you can probably trust the prediction.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2-linear"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="3-Regularization.html" title="previous page"><span class="section-number">3. </span>Regularization</a>
    <a class='right-next' id="next-link" href="5-Multiclassification.html" title="next page"><span class="section-number">5. </span>Multi-class classification</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>