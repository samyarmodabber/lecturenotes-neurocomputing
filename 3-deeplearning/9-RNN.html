
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9. Recurrent neural networks &#8212; Neurocomputing</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/9-RNN.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1. Limits of deep learning" href="../4-neurocomputing/1-Limits.html" />
    <link rel="prev" title="8. Generative adversarial networks" href="8-GAN.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/9-RNN.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Recurrent neural networks" />
<meta property="og:description" content="Recurrent neural networks  Slides: pdf  Recurrent neural networks  &lt;div class=&#39;embed-container&#39;&gt;&lt;iframe src=&#39;https://www.youtube.com/embed/IIeWpVMgriI&#39; framebor" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-neurocomputing/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neurocomputing</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   9. Recurrent neural networks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3-deeplearning/9-RNN.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   9.1. Recurrent neural networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problem-with-feedforward-neural-networks">
     9.1.1. Problem with feedforward neural networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recurrent-neural-network">
     9.1.2. Recurrent neural network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bptt-backpropagation-through-time">
     9.1.3. BPTT: Backpropagation through time
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vanishing-gradients">
     9.1.4. Vanishing gradients
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#long-short-term-memory-networks-lstm">
   9.2. Long short-term memory networks - LSTM
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#state-conveyor-belt">
     9.2.1. State conveyor belt
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forget-gate">
     9.2.2. Forget gate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#input-gate">
     9.2.3. Input gate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#candidate-state">
     9.2.4. Candidate state
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-gate">
     9.2.5. Output gate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lstm-layer">
     9.2.6. LSTM layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     9.2.7. Vanishing gradients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#peephole-connections">
     9.2.8. Peephole connections
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gru-gated-recurrent-unit">
     9.2.9. GRU: Gated Recurrent Unit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bidirectional-lstm">
     9.2.10. Bidirectional LSTM
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#word2vec">
   9.3. word2vec
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications-of-rnns">
   9.4. Applications of RNNs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-of-lstm-architectures">
     9.4.1. Classification of LSTM architectures
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-caption-generation">
     9.4.2. Image caption generation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#next-character-word-prediction">
     9.4.3. Next character/word prediction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sentiment-analysis">
     9.4.4. Sentiment analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#question-answering-scene-understanding">
     9.4.5. Question answering / Scene understanding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#seq2seq">
     9.4.6. seq2seq
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attentional-recurrent-networks">
   9.5. Attentional recurrent networks
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="recurrent-neural-networks">
<h1><span class="section-number">9. </span>Recurrent neural networks<a class="headerlink" href="#recurrent-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/3.9-RNN.pdf">pdf</a></p>
<div class="section" id="id1">
<h2><span class="section-number">9.1. </span>Recurrent neural networks<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/IIeWpVMgriI' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="problem-with-feedforward-neural-networks">
<h3><span class="section-number">9.1.1. </span>Problem with feedforward neural networks<a class="headerlink" href="#problem-with-feedforward-neural-networks" title="Permalink to this headline">¶</a></h3>
<p><strong>Feedforward neural networks</strong> learn to associate an input vector to an output.</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = F_\theta(\mathbf{x})\]</div>
<p>If you present a sequence of inputs <span class="math notranslate nohighlight">\(\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_t\)</span> to a feedforward network, the outputs will be independent from each other:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y}_0 = F_\theta(\mathbf{x}_0)\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{y}_1 = F_\theta(\mathbf{x}_1)\]</div>
<div class="math notranslate nohighlight">
\[\dots\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{y}_t = F_\theta(\mathbf{x}_t)\]</div>
<p>Many problems depend on time series, such as predicting the future of a time series by knowing its past values:</p>
<div class="math notranslate nohighlight">
\[x_{t+1} = F_\theta(x_0, x_1, \ldots, x_t)\]</div>
<p>Example: weather prediction, financial prediction, predictive maintenance, natural language processing, video analysis…</p>
<p>A naive solution is to <strong>aggregate</strong> (concatenate) inputs over a sufficiently long window and use it as a new input vector for the feedforward network.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X} = \begin{bmatrix}\mathbf{x}_{t-T} &amp; \mathbf{x}_{t-T+1} &amp; \ldots &amp; \mathbf{x}_t \\ \end{bmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{y}_t = F_\theta(\mathbf{X})\]</div>
<ul class="simple">
<li><p><strong>Problem 1:</strong> How long should the window be?</p></li>
<li><p><strong>Problem 2:</strong> Having more input dimensions increases dramatically the complexity of the classifier (VC dimension), hence the number of training examples required to avoid overfitting.</p></li>
</ul>
</div>
<div class="section" id="recurrent-neural-network">
<h3><span class="section-number">9.1.2. </span>Recurrent neural network<a class="headerlink" href="#recurrent-neural-network" title="Permalink to this headline">¶</a></h3>
<p>A <strong>recurrent neural network</strong> (RNN) uses it previous output as an additional input (<em>context</em>). All vectors have a time index <span class="math notranslate nohighlight">\(t\)</span> denoting the time at which this vector was computed.</p>
<p>The input vector at time <span class="math notranslate nohighlight">\(t\)</span> is <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span>, the output vector is <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{h}_t = \sigma(W_x \times \mathbf{x}_t + W_h \times \mathbf{h}_{t-1} + \mathbf{b})
\]</div>
<p><span class="math notranslate nohighlight">\(\sigma\)</span> is a transfer function, usually logistic or tanh. The input <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> and previous output <span class="math notranslate nohighlight">\(\mathbf{h}_{t-1}\)</span> are multiplied by <strong>learnable weights</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(W_x\)</span> is the input weight matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_h\)</span> is the recurrent weight matrix.</p></li>
</ul>
<div class="figure align-default" id="id21">
<a class="reference internal image-reference" href="../_images/RNN-rolled.png"><img alt="../_images/RNN-rolled.png" src="../_images/RNN-rolled.png" style="width: 30%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.1 </span><span class="caption-text">Recurrent neural network. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</div>
<p>One can <strong>unroll</strong> a recurrent network: the output <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span> depends on the whole history of inputs from <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> to <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \mathbf{h}_t &amp; = \sigma(W_x \times \mathbf{x}_t + W_h \times \mathbf{h}_{t-1} + \mathbf{b}) \\
                 &amp;\\
                 &amp; = \sigma(W_x \times \mathbf{x}_t + W_h \times \sigma(W_x \times \mathbf{x}_{t-1} + W_h \times \mathbf{h}_{t-2} + \mathbf{b})  + \mathbf{b}) \\
                 &amp;\\
                 &amp; = f_{W_x, W_h, \mathbf{b}} (\mathbf{x}_0, \mathbf{x}_1, \dots,\mathbf{x}_t) \\
\end{aligned}
\end{split}\]</div>
<p>A RNN is considered as part of <strong>deep learning</strong>, as there are many layers of weights between the first input <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> and the output <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span>. The only difference with a DNN is that the weights <span class="math notranslate nohighlight">\(W_x\)</span> and <span class="math notranslate nohighlight">\(W_h\)</span> are <strong>reused</strong> at each time step.</p>
<div class="figure align-default" id="id22">
<a class="reference internal image-reference" href="../_images/RNN-unrolled.png"><img alt="../_images/RNN-unrolled.png" src="../_images/RNN-unrolled.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.2 </span><span class="caption-text">Recurrent neural network unrolled. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="bptt-backpropagation-through-time">
<h3><span class="section-number">9.1.3. </span>BPTT: Backpropagation through time<a class="headerlink" href="#bptt-backpropagation-through-time" title="Permalink to this headline">¶</a></h3>
<p>The function between the history of inputs and the output at time <span class="math notranslate nohighlight">\(t\)</span> is differentiable: we can simply apply gradient descent to find the weights! This variant of backpropagation is called <strong>Backpropagation Through Time</strong> (BPTT). Once the loss between <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span> and its desired value is computed, one applies the <strong>chain rule</strong> to find out how to modify the weights <span class="math notranslate nohighlight">\(W_x\)</span> and <span class="math notranslate nohighlight">\(W_h\)</span> using the history <span class="math notranslate nohighlight">\((\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_t)\)</span>.</p>
<p>Let’s compute the gradient accumulated between <span class="math notranslate nohighlight">\(\mathbf{h}_{t-1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}_{t}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \mathbf{h}_{t} &amp; = \sigma(W_x \times \mathbf{x}_{t} + W_h \times \mathbf{h}_{t-1} + \mathbf{b}) \\
\end{aligned}
\end{split}\]</div>
<p>As for feedforward networks, the gradient of the loss function is decomposed into two parts:</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial W_x} =
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial \mathbf{h}_t} \times
    \frac{\partial \mathbf{h}_t}{\partial W_x}
\]</div>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial W_h} =
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial \mathbf{h}_t} \times
    \frac{\partial \mathbf{h}_t}{\partial W_h}
\]</div>
<p>The first part only depends on the loss function (mse, cross-entropy):</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{L}(W_x, W_h)}{\partial \mathbf{h}_t} = - (\mathbf{t}_{t}- \mathbf{h}_{t})
\]</div>
<p>The second part depends on the RNN itself:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \mathbf{h}_{t} &amp; = \sigma(W_x \times \mathbf{x}_{t} + W_h \times \mathbf{h}_{t-1} + \mathbf{b}) \\
\end{aligned}
\end{split}\]</div>
<p>The gradients w.r.t the two weight matrices are given by this <strong>recursive</strong> relationship (product rule):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \frac{\partial \mathbf{h}_t}{\partial W_x} &amp; = \mathbf{h'}_{t} \times (\mathbf{x}_t + W_h \times \frac{\partial \mathbf{h}_{t-1}}{\partial W_x})\\
    &amp; \\
    \frac{\partial \mathbf{h}_t}{\partial W_h} &amp; = \mathbf{h'}_{t} \times (\mathbf{h}_{t-1} + W_h \times \frac{\partial \mathbf{h}_{t-1}}{\partial W_h})\\
\end{aligned}
\end{split}\]</div>
<p>The derivative of the transfer function is noted <span class="math notranslate nohighlight">\(\mathbf{h'}_{t}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{h'}_{t} = \begin{cases}
        \mathbf{h}_{t} \, (1 - \mathbf{h}_{t}) \quad \text{ for logistic}\\
        (1 - \mathbf{h}_{t}^2) \quad \text{ for tanh.}\\
    \end{cases}
\end{split}\]</div>
<p>If we <strong>unroll</strong> the gradient, we obtain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \frac{\partial \mathbf{h}_t}{\partial W_x} &amp; = \mathbf{h'}_{t} \, (\mathbf{x}_t + W_h \times \mathbf{h'}_{t-1} \, (\mathbf{x}_{t-1} + W_h \times \mathbf{h'}_{t-2} \, (\mathbf{x}_{t-2} + W_h \times \ldots (\mathbf{x}_0))))\\
    &amp; \\
    \frac{\partial \mathbf{h}_t}{\partial W_h} &amp; = \mathbf{h'}_{t} \, (\mathbf{h}_{t-1} + W_h \times \mathbf{h'}_{t-1} \, (\mathbf{h}_{t-2} + W_h \times \mathbf{h'}_{t-2} \, \ldots (\mathbf{h}_{0})))\\
\end{aligned}
\end{split}\]</div>
<p>When updating the weights at time <span class="math notranslate nohighlight">\(t\)</span>, we need to store in memory:</p>
<ul class="simple">
<li><p>the complete history of inputs <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span>, … <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span>.</p></li>
<li><p>the complete history of outputs <span class="math notranslate nohighlight">\(\mathbf{h}_0\)</span>, <span class="math notranslate nohighlight">\(\mathbf{h}_1\)</span>, … <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span>.</p></li>
<li><p>the complete history of derivatives <span class="math notranslate nohighlight">\(\mathbf{h'}_0\)</span>, <span class="math notranslate nohighlight">\(\mathbf{h'}_1\)</span>, … <span class="math notranslate nohighlight">\(\mathbf{h'}_t\)</span>.</p></li>
</ul>
<p>before computing the gradients iteratively, starting from time <span class="math notranslate nohighlight">\(t\)</span> and accumulating gradients <strong>backwards</strong> in time until <span class="math notranslate nohighlight">\(t=0\)</span>. Each step backwards in time adds a bit to the gradient used to update the weights.</p>
<p>In practice, going back to <span class="math notranslate nohighlight">\(t=0\)</span> at each time step requires too many computations, which may not be needed. <strong>Truncated BPTT</strong> only updates the gradients up to <span class="math notranslate nohighlight">\(T\)</span> steps before: the gradients are computed backwards from <span class="math notranslate nohighlight">\(t\)</span> to <span class="math notranslate nohighlight">\(t-T\)</span>. The partial derivative in <span class="math notranslate nohighlight">\(t-T-1\)</span> is considered 0. This limits the <strong>horizon</strong> of BPTT: dependencies longer than <span class="math notranslate nohighlight">\(T\)</span> will not be learned, so it has to be chosen carefully for the task. <span class="math notranslate nohighlight">\(T\)</span> becomes yet another hyperparameter of your algorithm…</p>
<div class="figure align-default" id="id23">
<a class="reference internal image-reference" href="../_images/truncated_backprop.png"><img alt="../_images/truncated_backprop.png" src="../_images/truncated_backprop.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.3 </span><span class="caption-text">Truncated backpropagation through time. Source: <a class="reference external" href="https://r2rt.com/styles-of-truncated-backpropagation.html">https://r2rt.com/styles-of-truncated-backpropagation.html</a>.</span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="vanishing-gradients">
<h3><span class="section-number">9.1.4. </span>Vanishing gradients<a class="headerlink" href="#vanishing-gradients" title="Permalink to this headline">¶</a></h3>
<p>BPTT is able to find <strong>short-term dependencies</strong> between inputs and outputs: perceiving the inputs <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span> allows to respond correctly at <span class="math notranslate nohighlight">\(t = 3\)</span>.</p>
<div class="figure align-default" id="id24">
<a class="reference internal image-reference" href="../_images/RNN-shorttermdependencies.png"><img alt="../_images/RNN-shorttermdependencies.png" src="../_images/RNN-shorttermdependencies.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.4 </span><span class="caption-text">RNN can learn short-term dependencies. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</div>
<p>But it fails to detect <strong>long-term dependencies</strong> because of:</p>
<ul class="simple">
<li><p>the truncated horizon <span class="math notranslate nohighlight">\(T\)</span> (for computational reasons).</p></li>
<li><p>the <strong>vanishing gradient problem</strong> <a class="bibtex reference internal" href="../zreferences.html#hochreiter1991" id="id2">[Hochreiter, 1991]</a>.</p></li>
</ul>
<div class="figure align-default" id="id25">
<a class="reference internal image-reference" href="../_images/RNN-longtermdependencies.png"><img alt="../_images/RNN-longtermdependencies.png" src="../_images/RNN-longtermdependencies.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.5 </span><span class="caption-text">RNN cannot learn long-term dependencies. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
<p>Let’s look at the gradient w.r.t to the input weights:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \frac{\partial \mathbf{h}_t}{\partial W_x} &amp; = \mathbf{h'}_{t} \, (\mathbf{x}_t + W_h \times \frac{\partial \mathbf{h}_{t-1}}{\partial W_x})\\
    &amp; \\
\end{aligned}
\end{split}\]</div>
<p>At each iteration backwards in time, the gradients are multiplied by <span class="math notranslate nohighlight">\(W_h\)</span>. If you search how <span class="math notranslate nohighlight">\(\frac{\partial \mathbf{h}_t}{\partial W_x}\)</span> depends on <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, you obtain something like:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \frac{\partial \mathbf{h}_t}{\partial W_x} &amp; \approx \prod_{k=0}^t \mathbf{h'}_{k} \, ((W_h)^t \, \mathbf{x}_0 + \dots) \\
\end{aligned}
\end{split}\]</div>
<p>If <span class="math notranslate nohighlight">\(|W_h| &gt; 1\)</span>, <span class="math notranslate nohighlight">\(|(W_h)^t|\)</span> increases exponentially with <span class="math notranslate nohighlight">\(t\)</span>: the gradient <strong>explodes</strong>. If <span class="math notranslate nohighlight">\(|W_h| &lt; 1\)</span>, <span class="math notranslate nohighlight">\(|(W_h)^t|\)</span> decreases exponentially with <span class="math notranslate nohighlight">\(t\)</span>:  the gradient <strong>vanishes</strong>.</p>
<p><strong>Exploding gradients</strong> are relatively easy to deal with: one just clips the norm of the gradient to a maximal value.</p>
<div class="math notranslate nohighlight">
\[
    || \frac{\partial \mathcal{L}(W_x, W_h)}{\partial W_x}|| \gets \min(||\frac{\partial \mathcal{L}(W_x, W_h)}{\partial W_x}||, \text{MAX_GRAD})
\]</div>
<p>But there is no solution to the <strong>vanishing gradient problem</strong> for regular RNNs: the gradient fades over time (backwards) and no long-term dependency can be learned.
This is the same problem as for feedforward deep networks: a RNN is just a deep network rolled over itself.
Its depth (number of layers) corresponds to the maximal number of steps back in time.
In order to limit vanishing gradients and learn long-term dependencies, one has to use a more complex structure for the layer.
This is the idea behind <strong>long short-term memory</strong> (LSTM) networks.</p>
</div>
</div>
<div class="section" id="long-short-term-memory-networks-lstm">
<h2><span class="section-number">9.2. </span>Long short-term memory networks - LSTM<a class="headerlink" href="#long-short-term-memory-networks-lstm" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/jvBOgXSebV0' frameborder='0' allowfullscreen></iframe></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All figures in this section are taken from this great blog post by Christopher Olah, which is worth a read:</p>
<p><a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a></p>
</div>
<div class="figure align-default" id="id26">
<a class="reference internal image-reference" href="../_images/LSTM3-SimpleRNN.png"><img alt="../_images/LSTM3-SimpleRNN.png" src="../_images/LSTM3-SimpleRNN.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.6 </span><span class="caption-text">RNN layer. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id27">
<a class="reference internal image-reference" href="../_images/LSTM3-chain1.png"><img alt="../_images/LSTM3-chain1.png" src="../_images/LSTM3-chain1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.7 </span><span class="caption-text">LSTM layer. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
<p>A <strong>LSTM layer</strong>  <a class="bibtex reference internal" href="../zreferences.html#hochreiter1997" id="id3">[Hochreiter &amp; Schmidhuber, 1997]</a> is a RNN layer with the ability to control what it memorizes. In addition to the input <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> and output <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span>, it also has a <strong>state</strong> <span class="math notranslate nohighlight">\(\mathbf{C}_t\)</span> which is maintained over time. The state is the memory of the layer (sometimes called context). It also contains three multiplicative <strong>gates</strong>:</p>
<ul class="simple">
<li><p>The <strong>input gate</strong> controls which inputs should enter the memory.</p>
<ul>
<li><p><em>are they worth remembering?</em></p></li>
</ul>
</li>
<li><p>The <strong>forget gate</strong> controls which memory should be forgotten.</p>
<ul>
<li><p><em>do I still need them?</em></p></li>
</ul>
</li>
<li><p>The <strong>output gate</strong> controls which part of the memory should be used to produce the output.</p>
<ul>
<li><p><em>should I respond now? Do I have enough information?</em></p></li>
</ul>
</li>
</ul>
<p>The <strong>state</strong> <span class="math notranslate nohighlight">\(\mathbf{C}_t\)</span> can be seen as an accumulator integrating inputs (and previous outputs) over time.
The gates <strong>learn</strong> to open and close through learnable weights.</p>
<div class="section" id="state-conveyor-belt">
<h3><span class="section-number">9.2.1. </span>State conveyor belt<a class="headerlink" href="#state-conveyor-belt" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id28">
<a class="reference internal image-reference" href="../_images/LSTM3-C-line.png"><img alt="../_images/LSTM3-C-line.png" src="../_images/LSTM3-C-line.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.8 </span><span class="caption-text">State conveyor belt. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id28" title="Permalink to this image">¶</a></p>
</div>
<p>By default, the cell state <span class="math notranslate nohighlight">\(\mathbf{C}_t\)</span> stays the same over time (<em>conveyor belt</em>). It can have the same number of dimensions as the output <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span>, but does not have to. Its content can be erased by multiplying it with a vector of 0s, or preserved by multiplying it by a vector of 1s. We can use a <strong>sigmoid</strong> to achieve this:</p>
<div class="figure align-default" id="id29">
<a class="reference internal image-reference" href="../_images/LSTM3-gate.png"><img alt="../_images/LSTM3-gate.png" src="../_images/LSTM3-gate.png" style="width: 30%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.9 </span><span class="caption-text">Element-wise multiplication with a vector using using the logistic/sigmoid function. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id29" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="forget-gate">
<h3><span class="section-number">9.2.2. </span>Forget gate<a class="headerlink" href="#forget-gate" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id30">
<a class="reference internal image-reference" href="../_images/LSTM3-focus-f.png"><img alt="../_images/LSTM3-focus-f.png" src="../_images/LSTM3-focus-f.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.10 </span><span class="caption-text">Forget gate. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id30" title="Permalink to this image">¶</a></p>
</div>
<p>Forget weights <span class="math notranslate nohighlight">\(W_f\)</span> and a sigmoid function are used to decide if the state should be preserved or not.</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{f}_t = \sigma(W_f \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_f)
\]</div>
<p><span class="math notranslate nohighlight">\([\mathbf{h}_{t-1}; \mathbf{x}_t]\)</span> is simply the concatenation of the two vectors <span class="math notranslate nohighlight">\(\mathbf{h}_{t-1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span>. <span class="math notranslate nohighlight">\(\mathbf{f}_t\)</span> is a vector of values between 0 and 1, one per dimension of the cell state <span class="math notranslate nohighlight">\(\mathbf{C}_t\)</span>.</p>
</div>
<div class="section" id="input-gate">
<h3><span class="section-number">9.2.3. </span>Input gate<a class="headerlink" href="#input-gate" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id31">
<a class="reference internal image-reference" href="../_images/LSTM3-focus-i.png"><img alt="../_images/LSTM3-focus-i.png" src="../_images/LSTM3-focus-i.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.11 </span><span class="caption-text">Input gate. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id31" title="Permalink to this image">¶</a></p>
</div>
<p>Similarly, the input gate uses a sigmoid function to decide if the state should be updated or not.</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{i}_t = \sigma(W_i \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_i)
\]</div>
<p>As for RNNs, the input <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> and previous output <span class="math notranslate nohighlight">\(\mathbf{h}_{t-1}\)</span> are combined to produce a <strong>candidate state</strong> <span class="math notranslate nohighlight">\(\tilde{\mathbf{C}}_t\)</span> using the tanh transfer function.</p>
<div class="math notranslate nohighlight">
\[
    \tilde{\mathbf{C}}_t = \text{tanh}(W_C \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_c)
\]</div>
</div>
<div class="section" id="candidate-state">
<h3><span class="section-number">9.2.4. </span>Candidate state<a class="headerlink" href="#candidate-state" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id32">
<a class="reference internal image-reference" href="../_images/LSTM3-focus-C.png"><img alt="../_images/LSTM3-focus-C.png" src="../_images/LSTM3-focus-C.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.12 </span><span class="caption-text">Candidate state. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id32" title="Permalink to this image">¶</a></p>
</div>
<p>The new state <span class="math notranslate nohighlight">\(\mathbf{C}_t\)</span> is computed as a part of the previous state <span class="math notranslate nohighlight">\(\mathbf{C}_{t-1}\)</span> (element-wise multiplication with the forget gate <span class="math notranslate nohighlight">\(\mathbf{f}_t\)</span>) plus a part of the candidate state <span class="math notranslate nohighlight">\(\tilde{\mathbf{C}}_t\)</span> (element-wise multiplication with the input gate <span class="math notranslate nohighlight">\(\mathbf{i}_t\)</span>).</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t
\]</div>
<p>Depending on the gates, the new state can be equal to the previous state (gates closed), the candidate state (gates opened) or a mixture of both.</p>
</div>
<div class="section" id="output-gate">
<h3><span class="section-number">9.2.5. </span>Output gate<a class="headerlink" href="#output-gate" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id33">
<a class="reference internal image-reference" href="../_images/LSTM3-focus-o.png"><img alt="../_images/LSTM3-focus-o.png" src="../_images/LSTM3-focus-o.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.13 </span><span class="caption-text">Output gate. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id33" title="Permalink to this image">¶</a></p>
</div>
<p>The output gate decides which part of the new state will be used for the output.</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{o}_t = \sigma(W_o \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_o)
\]</div>
<p>The output not only influences the decision, but also how the gates will updated at the next step.</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{h}_t = \mathbf{o}_t \odot \text{tanh} (\mathbf{C}_t)
\]</div>
</div>
<div class="section" id="lstm-layer">
<h3><span class="section-number">9.2.6. </span>LSTM layer<a class="headerlink" href="#lstm-layer" title="Permalink to this headline">¶</a></h3>
<p>The function between <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span> is quite complicated, with many different weights, but everything is differentiable: BPTT can be applied.</p>
<div class="figure align-default" id="id34">
<a class="reference internal image-reference" href="../_images/LSTM-cell2.png"><img alt="../_images/LSTM-cell2.png" src="../_images/LSTM-cell2.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.14 </span><span class="caption-text">LSTM layer. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id34" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Equations:</strong></p>
<ul class="simple">
<li><p><strong>Forget gate</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathbf{f}_t = \sigma(W_f \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_f)\]</div>
<ul class="simple">
<li><p><strong>Input gate</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathbf{i}_t = \sigma(W_i \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_i)\]</div>
<ul class="simple">
<li><p><strong>Output gate</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathbf{o}_t = \sigma(W_o \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_o)\]</div>
<ul class="simple">
<li><p><strong>Candidate state</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\tilde{\mathbf{C}}_t = \text{tanh}(W_C \times [\mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_c)\]</div>
<ul class="simple">
<li><p><strong>New state</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t\]</div>
<ul class="simple">
<li><p><strong>Output</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathbf{h}_t = \mathbf{o}_t \odot \text{tanh} (\mathbf{C}_t)\]</div>
</div>
<div class="section" id="id4">
<h3><span class="section-number">9.2.7. </span>Vanishing gradients<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>How do LSTM solve the vanishing gradient problem? Not all inputs are remembered by the LSTM: the input gate controls what comes in. If only <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span> are needed to produce <span class="math notranslate nohighlight">\(\mathbf{h}_{t+1}\)</span>, they will be the only ones stored in the state, the other inputs are ignored.</p>
<p>If the state stays constant between <span class="math notranslate nohighlight">\(t=1\)</span> and <span class="math notranslate nohighlight">\(t\)</span>, the gradient of the error will not vanish when backpropagating from <span class="math notranslate nohighlight">\(t\)</span> to <span class="math notranslate nohighlight">\(t=1\)</span>, because nothing happens!</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{C}_t = \mathbf{C}_{t-1} \rightarrow \frac{\partial \mathbf{C}_t}{\partial \mathbf{C}_{t-1}} = 1
\]</div>
<p>The gradient is multiplied by exactly one when the gates are closed.</p>
<p>LSTM are particularly good at learning long-term dependencies, because the gates protect the cell from vanishing gradients. Its problem is how to find out which inputs (e.g. <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span>) should enter or leave the state memory.</p>
<p>Truncated BPTT is used to train all weights: the weights for the candidate state (as for RNN), and the weights of the three gates. LSTM are also subject to overfitting. Regularization (including dropout) can be used. The weights (also for the gates) can be convolutional.
The gates also have a bias, which can be fixed (but hard to find). LSTM layers can be stacked to detect dependencies at different scales (deep LSTM network).</p>
</div>
<div class="section" id="peephole-connections">
<h3><span class="section-number">9.2.8. </span>Peephole connections<a class="headerlink" href="#peephole-connections" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id35">
<a class="reference internal image-reference" href="../_images/LSTM3-var-peepholes.png"><img alt="../_images/LSTM3-var-peepholes.png" src="../_images/LSTM3-var-peepholes.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.15 </span><span class="caption-text">Peephole connections. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id35" title="Permalink to this image">¶</a></p>
</div>
<p>A popular variant of LSTM adds <strong>peephole connections</strong> <a class="bibtex reference internal" href="../zreferences.html#gers2000" id="id5">[Gers &amp; Schmidhuber, 2000]</a>, where the three gates have additionally access to the state <span class="math notranslate nohighlight">\(\mathbf{C}_{t-1}\)</span>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-ba0b4e13-e655-41fe-b4dc-520be175e692">
<span class="eqno">(9.1)<a class="headerlink" href="#equation-ba0b4e13-e655-41fe-b4dc-520be175e692" title="Permalink to this equation">¶</a></span>\[\begin{align}
    \mathbf{f}_t &amp;= \sigma(W_f \times [\mathbf{C}_{t-1}; \mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_f) \\
    &amp;\\
    \mathbf{i}_t &amp;= \sigma(W_i \times [\mathbf{C}_{t-1}; \mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_i) \\
    &amp;\\
    \mathbf{o}_t &amp;= \sigma(W_o \times [\mathbf{C}_{t}; \mathbf{h}_{t-1}; \mathbf{x}_t] + \mathbf{b}_o) \\
\end{align}\]</div>
<p>It usually works better, but adds more weights.</p>
</div>
<div class="section" id="gru-gated-recurrent-unit">
<h3><span class="section-number">9.2.9. </span>GRU: Gated Recurrent Unit<a class="headerlink" href="#gru-gated-recurrent-unit" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id36">
<a class="reference internal image-reference" href="../_images/LSTM3-var-GRU.png"><img alt="../_images/LSTM3-var-GRU.png" src="../_images/LSTM3-var-GRU.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.16 </span><span class="caption-text">Gated recurrent unit (GRU). Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</span><a class="headerlink" href="#id36" title="Permalink to this image">¶</a></p>
</div>
<p>Another variant is called the <strong>Gated Recurrent Unit</strong> (GRU) <a class="bibtex reference internal" href="../zreferences.html#chung2014" id="id6">[Chung et al., 2014]</a>.
It uses directly the output <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span> as a state, and the forget and input gates are merged into a single gate <span class="math notranslate nohighlight">\(\mathbf{r}_t\)</span>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-46bce3f7-7579-46be-ab99-3e11f95a104a">
<span class="eqno">(9.2)<a class="headerlink" href="#equation-46bce3f7-7579-46be-ab99-3e11f95a104a" title="Permalink to this equation">¶</a></span>\[\begin{align}
    \mathbf{z}_t &amp;= \sigma(W_z \times [\mathbf{h}_{t-1}; \mathbf{x}_t]) \\
    &amp;\\
    \mathbf{r}_t &amp;= \sigma(W_r \times [\mathbf{h}_{t-1}; \mathbf{x}_t]) \\
    &amp;\\
    \tilde{\mathbf{h}}_t &amp;= \text{tanh} (W_h \times [\mathbf{r}_t \odot \mathbf{h}_{t-1}; \mathbf{x}_t])\\
    &amp; \\
    \mathbf{h}_t &amp;= (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t\\
\end{align}\]</div>
<p>It does not even need biases (mostly useless in LSTMs anyway). Much simpler to train as the LSTM, and almost as powerful.</p>
</div>
<div class="section" id="bidirectional-lstm">
<h3><span class="section-number">9.2.10. </span>Bidirectional LSTM<a class="headerlink" href="#bidirectional-lstm" title="Permalink to this headline">¶</a></h3>
<p>A <strong>bidirectional LSTM</strong> learns to predict the output in two directions:</p>
<ul class="simple">
<li><p>The <strong>feedforward</strong> line learns using the past context (classical LSTM).</p></li>
<li><p>The <strong>backforward</strong> line learns using the future context (inputs are reversed).</p></li>
</ul>
<p>The two state vectors are then concatenated at each time step to produce the output. Only possible offline, as the future inputs must be known. Works better than LSTM on many problems, but slower.</p>
<div class="figure align-default" id="id37">
<a class="reference internal image-reference" href="../_images/bi_lstm.jpg"><img alt="../_images/bi_lstm.jpg" src="../_images/bi_lstm.jpg" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.17 </span><span class="caption-text">Bidirectional LSTM. Source: <a class="reference external" href="http://www.paddlepaddle.org/doc/demo/sentiment_analysis/sentiment_analysis.html">http://www.paddlepaddle.org/doc/demo/sentiment_analysis/sentiment_analysis.html</a>.</span><a class="headerlink" href="#id37" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="word2vec">
<h2><span class="section-number">9.3. </span>word2vec<a class="headerlink" href="#word2vec" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/8ub1oavRfIE' frameborder='0' allowfullscreen></iframe></div>
<p>The most famous application of RNNs is <strong>Natural Language Processing</strong> (NLP): text understanding, translation, etc… Each word of a sentence has to be represented as a vector <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> in order to be fed to a LSTM. Which representation should we use?</p>
<p>The naive solution is to use <strong>one-hot encoding</strong>, one element of the vector corresponding to one word of the dictionary.</p>
<div class="figure align-default" id="id38">
<a class="reference internal image-reference" href="../_images/onehotvec.png"><img alt="../_images/onehotvec.png" src="../_images/onehotvec.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.18 </span><span class="caption-text">One-hot encoding of words. Source: <a class="reference external" href="https://cdn-images-1.medium.com/max/1600/1*ULfyiWPKgWceCqyZeDTl0g.png">https://cdn-images-1.medium.com/max/1600/1*ULfyiWPKgWceCqyZeDTl0g.png</a>.</span><a class="headerlink" href="#id38" title="Permalink to this image">¶</a></p>
</div>
<p>One-hot encoding is not a good representation for words:</p>
<ul class="simple">
<li><p>The vector size will depend on the number of words of the language:</p>
<ul>
<li><p>English:  171,476 (Oxford English Dictionary), 470,000 (Merriam-Webster)… 20,000 in practice.</p></li>
<li><p>French: 270,000 (TILF).</p></li>
<li><p>German: 200,000 (Duden).</p></li>
<li><p>Chinese: 370,000 (Hanyu Da Cidian).</p></li>
<li><p>Korean:   1,100,373 (Woori Mal Saem)</p></li>
</ul>
</li>
<li><p>Semantically related words have completely different representations (“endure” and “tolerate”).</p></li>
<li><p>The representation is extremely <strong>sparse</strong> (a lot of useless zeros).</p></li>
</ul>
<div class="figure align-default" id="id39">
<a class="reference internal image-reference" href="../_images/audio-image-text.png"><img alt="../_images/audio-image-text.png" src="../_images/audio-image-text.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.19 </span><span class="caption-text">Audio and image data are dense, i.e. each input dimension carries information. One-hot encoded sentences are sparse. Source: <a class="reference external" href="https://www.tensorflow.org/tutorials/representation/word2vec">https://www.tensorflow.org/tutorials/representation/word2vec</a>.</span><a class="headerlink" href="#id39" title="Permalink to this image">¶</a></p>
</div>
<p><strong>word2vec</strong> <a class="bibtex reference internal" href="../zreferences.html#mikolov2013" id="id7">[Mikolov et al., 2013]</a> learns word <strong>embeddings</strong> by trying to predict the current word based on the context (CBOW, continuous bag-of-words) or the context based on the current word (skip-gram). See <a class="reference external" href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a> and  <a class="reference external" href="https://www.tensorflow.org/tutorials/representation/word2vec">https://www.tensorflow.org/tutorials/representation/word2vec</a> for more information.</p>
<p>It uses a three-layer autoencoder-like NN, where the hidden layer (latent space) will learn to represent the one-hot encoded words in a dense manner.</p>
<div class="figure align-default" id="id40">
<a class="reference internal image-reference" href="../_images/word2vec-training.png"><img alt="../_images/word2vec-training.png" src="../_images/word2vec-training.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.20 </span><span class="caption-text">Word2vec is an autoencoder trained to reproduce the context of a word using one-hot encoded vectors. Source: <a class="reference external" href="https://jaxenter.com/deep-learning-search-word2vec-147782.html">https://jaxenter.com/deep-learning-search-word2vec-147782.html</a>.</span><a class="headerlink" href="#id40" title="Permalink to this image">¶</a></p>
</div>
<p><strong>word2vec</strong> has three parameters:</p>
<ul class="simple">
<li><p>the <strong>vocabulary size</strong>: number of words in the dictionary.</p></li>
<li><p>the <strong>embedding size</strong>: number of neurons in the hidden layer.</p></li>
<li><p>the <strong>context size</strong>: number of surrounding words to predict.</p></li>
</ul>
<p>It is trained on huge datasets of sentences (e.g. Wikipedia). After learning, the hidden layer represents an <strong>embedding vector</strong>, which is a dense and compressed representation of each possible word (dimensionality reduction). Semantically close words (“endure” and “tolerate”) tend to appear in similar contexts, so their embedded representations will be close (Euclidian distance). One can even perform arithmetic operations on these vectors!</p>
<blockquote>
<div><p>queen = king + woman - man</p>
</div></blockquote>
<div class="figure align-default" id="id41">
<a class="reference internal image-reference" href="../_images/linear-relationships.png"><img alt="../_images/linear-relationships.png" src="../_images/linear-relationships.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.21 </span><span class="caption-text">Arithmetic operations on word2vec representations. Source: <a class="reference external" href="https://www.tensorflow.org/tutorials/representation/word2vec">https://www.tensorflow.org/tutorials/representation/word2vec</a>.</span><a class="headerlink" href="#id41" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="applications-of-rnns">
<h2><span class="section-number">9.4. </span>Applications of RNNs<a class="headerlink" href="#applications-of-rnns" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/FiSqycJE_i0' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="classification-of-lstm-architectures">
<h3><span class="section-number">9.4.1. </span>Classification of LSTM architectures<a class="headerlink" href="#classification-of-lstm-architectures" title="Permalink to this headline">¶</a></h3>
<p>Several architectures are possible using recurrent neural networks:</p>
<div class="figure align-default" id="id42">
<a class="reference internal image-reference" href="../_images/lstm-diagrams.jpg"><img alt="../_images/lstm-diagrams.jpg" src="../_images/lstm-diagrams.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.22 </span><span class="caption-text">Arithmetic operations on word2vec representations. Source: <a class="reference external" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>.</span><a class="headerlink" href="#id42" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p><strong>One to One</strong>: classical feedforward network.</p>
<ul>
<li><p>Image <span class="math notranslate nohighlight">\(\rightarrow\)</span> Label.</p></li>
</ul>
</li>
<li><p><strong>One to Many</strong>: single input, many outputs.</p>
<ul>
<li><p>Image <span class="math notranslate nohighlight">\(\rightarrow\)</span> Text.</p></li>
</ul>
</li>
<li><p><strong>Many to One</strong>: sequence of inputs, single output.</p>
<ul>
<li><p>Video / Text <span class="math notranslate nohighlight">\(\rightarrow\)</span> Label.</p></li>
</ul>
</li>
<li><p><strong>Many to Many</strong>: sequence to sequence.</p>
<ul>
<li><p>Text <span class="math notranslate nohighlight">\(\rightarrow\)</span> Text.</p></li>
<li><p>Video <span class="math notranslate nohighlight">\(\rightarrow\)</span> Text.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="image-caption-generation">
<h3><span class="section-number">9.4.2. </span>Image caption generation<a class="headerlink" href="#image-caption-generation" title="Permalink to this headline">¶</a></h3>
<p><strong>Show and Tell</strong> <a class="bibtex reference internal" href="../zreferences.html#vinyals2015" id="id8">[Vinyals et al., 2015]</a> uses the last FC layer of a CNN to feed a LSTM layer and generate words. The pretrained CNN (VGG16, ResNet50) is used as a <strong>feature extractor</strong>. Each word of the sentence is encoded/decoded using word2vec. The output of the LSTM at time <span class="math notranslate nohighlight">\(t\)</span> becomes its new input at time <span class="math notranslate nohighlight">\(t+1\)</span>.</p>
<div class="figure align-default" id="id43">
<a class="reference internal image-reference" href="../_images/showtell.jpg"><img alt="../_images/showtell.jpg" src="../_images/showtell.jpg" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.23 </span><span class="caption-text">Show and Tell <a class="bibtex reference internal" href="../zreferences.html#vinyals2015" id="id9">[Vinyals et al., 2015]</a>.</span><a class="headerlink" href="#id43" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Show, attend and tell</strong> <a class="bibtex reference internal" href="../zreferences.html#xu2015" id="id10">[Xu et al., 2015]</a> uses attention to focus on specific parts of the image when generating the sentence.</p>
<div class="figure align-default" id="id44">
<a class="reference internal image-reference" href="../_images/showattendtell.png"><img alt="../_images/showattendtell.png" src="../_images/showattendtell.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.24 </span><span class="caption-text">Show, attend and tell <a class="bibtex reference internal" href="../zreferences.html#xu2015" id="id11">[Xu et al., 2015]</a>.</span><a class="headerlink" href="#id44" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id45">
<a class="reference internal image-reference" href="../_images/showattendtell-res.png"><img alt="../_images/showattendtell-res.png" src="../_images/showattendtell-res.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.25 </span><span class="caption-text">Caption generation using Show, attend and tell <a class="bibtex reference internal" href="../zreferences.html#xu2015" id="id12">[Xu et al., 2015]</a>.</span><a class="headerlink" href="#id45" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="next-character-word-prediction">
<h3><span class="section-number">9.4.3. </span>Next character/word prediction<a class="headerlink" href="#next-character-word-prediction" title="Permalink to this headline">¶</a></h3>
<p>Characters or words are fed one by one into a LSTM. The desired output is the next character or word in the text.</p>
<p>Example:</p>
<ul class="simple">
<li><p>Inputs: <strong>To, be, or, not, to</strong></p></li>
<li><p>Output: <strong>be</strong></p></li>
</ul>
<p>The text below was generated by a LSTM having read the entire writings of William Shakespeare, learning to predict the next letter (see <a class="reference external" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>). Each generated character is used as the next input.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PANDARUS:
Alas, I think he shall be come approached and the day
When little srain would be attain&#39;d into being never fed,
And who is but a chain and subjects of his death,
I should not sleep.

Second Senator:
They are away this miseries, produced upon my soul,
Breaking and strongly should be buried, when I perish
The earth and thoughts of many states.

DUKE VINCENTIO:
Well, your wit is in the care of side and that.

Second Lord:
They would be ruled after this chamber, and
my fair nues begun out of the fact, to be conveyed,
Whose noble souls I&#39;ll have the heart of the wars.

Clown:
Come, sir, I will make did behold your worship.
</pre></div>
</div>
<div class="admonition-sunspring-scifi-movie admonition">
<p class="admonition-title">Sunspring SciFi movie</p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/LY7x2Ihqjmc' frameborder='0' allowfullscreen></iframe></div>
<p>More info: <a class="reference external" href="http://www.thereforefilms.com/sunspring.html">http://www.thereforefilms.com/sunspring.html</a></p>
</div>
</div>
<div class="section" id="sentiment-analysis">
<h3><span class="section-number">9.4.4. </span>Sentiment analysis<a class="headerlink" href="#sentiment-analysis" title="Permalink to this headline">¶</a></h3>
<p>Sentiment analysis consists of attributing a value (positive or negative) to a text. A 1D convolutional layers “slides” over the text, each word being encoded using word2vec. The bidirectional LSTM computes a state vector for the complete text. A classifier (fully connected layer) learns to predict the sentiment of the text (positive/negative).</p>
<div class="figure align-default" id="id46">
<a class="reference internal image-reference" href="../_images/sentimentanalysis.jpg"><img alt="../_images/sentimentanalysis.jpg" src="../_images/sentimentanalysis.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.26 </span><span class="caption-text">Sentiment analysis using bidirectional LSTMs. Source: <a class="reference external" href="https://offbit.github.io/how-to-read/">https://offbit.github.io/how-to-read/</a>.</span><a class="headerlink" href="#id46" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="question-answering-scene-understanding">
<h3><span class="section-number">9.4.5. </span>Question answering / Scene understanding<a class="headerlink" href="#question-answering-scene-understanding" title="Permalink to this headline">¶</a></h3>
<p>A LSTM can learn to associate an image (static) plus a question (sequence) with the answer (sequence). The image is abstracted by a CNN pretrained for object recognition.</p>
<div class="figure align-default" id="id47">
<a class="reference internal image-reference" href="../_images/questionanswering.jpg"><img alt="../_images/questionanswering.jpg" src="../_images/questionanswering.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.27 </span><span class="caption-text">Question answering. Source: <a class="bibtex reference internal" href="../zreferences.html#malinowski2015" id="id13">[Malinowski et al., 2015]</a>.</span><a class="headerlink" href="#id47" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="seq2seq">
<h3><span class="section-number">9.4.6. </span>seq2seq<a class="headerlink" href="#seq2seq" title="Permalink to this headline">¶</a></h3>
<p>The <strong>state vector</strong> obtained at the end of a sequence can be reused as an initial state for another LSTM. The goal of the <strong>encoder</strong> is to find a compressed representation of a sequence of inputs. The goal of the <strong>decoder</strong> is to generate a sequence from that representation. <strong>Sequence-to-sequence</strong> (seq2seq <a class="bibtex reference internal" href="../zreferences.html#sutskever2014" id="id14">[Sutskever et al., 2014]</a>) models are recurrent autoencoders.</p>
<div class="figure align-default" id="id48">
<a class="reference internal image-reference" href="../_images/seq2seq.jpeg"><img alt="../_images/seq2seq.jpeg" src="../_images/seq2seq.jpeg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.28 </span><span class="caption-text">seq2seq architecture. Source: <a class="bibtex reference internal" href="../zreferences.html#sutskever2014" id="id15">[Sutskever et al., 2014]</a>.</span><a class="headerlink" href="#id48" title="Permalink to this image">¶</a></p>
</div>
<p>The <strong>encoder</strong> learns for example to predict the next word in a sentence in French. The <strong>decoder</strong> learns to associate the <strong>final state vector</strong> to the corresponding English sentence. seq2seq allows automatic text translation between many languages given enough data. Modern translation tools are based on seq2seq, but with attention.</p>
</div>
</div>
<div class="section" id="attentional-recurrent-networks">
<h2><span class="section-number">9.5. </span>Attentional recurrent networks<a class="headerlink" href="#attentional-recurrent-networks" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/fD7DIXenij0' frameborder='0' allowfullscreen></iframe></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All videos in this section are taken from the great blog post by Jay Alammar:</p>
<p><a class="reference external" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></p>
</div>
<p>The problem with seq2seq is that it <strong>compresses</strong> the complete input sentence into a single state vector.</p>
<div class='embed-container'><iframe src='https://jalammar.github.io/images/seq2seq_6.mp4' frameborder='0' allowfullscreen loop autoplay></iframe></div>
<p>For long sequences, the beginning of the sentence may not be present in the final state vector:</p>
<ul class="simple">
<li><p>Truncated BPTT, vanishing gradients.</p></li>
<li><p>When predicting the last word, the beginning of the paragraph might not be necessary.</p></li>
</ul>
<p>Consequence: there is not enough information in the state vector to start translating. A solution would be to concatenate the <strong>state vectors</strong> of all steps of the encoder and pass them to the decoder.</p>
<div class='embed-container'><iframe src='https://jalammar.github.io/images/seq2seq_7.mp4' frameborder='0' allowfullscreen loop autoplay></iframe></div>
<ul class="simple">
<li><p><strong>Problem 1:</strong> it would make a lot of elements in the state vector of the decoder (which should be constant).</p></li>
<li><p><strong>Problem 2:</strong> the state vector of the decoder would depend on the length of the input sequence.</p></li>
</ul>
<p>Attentional mechanisms <a class="bibtex reference internal" href="../zreferences.html#bahdanau2016" id="id16">[Bahdanau et al., 2016]</a> let the decoder decide (by learning) which state vectors it needs to generate each word at each step.</p>
<p>The <strong>attentional context vector</strong> of the decoder <span class="math notranslate nohighlight">\(A^\text{decoder}_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> is a weighted average of all state vectors <span class="math notranslate nohighlight">\(C^\text{encoder}_i\)</span> of the encoder.</p>
<div class="math notranslate nohighlight">
\[A^\text{decoder}_t = \sum_{i=0}^T a_i \, C^\text{encoder}_i\]</div>
<div class='embed-container'><iframe src='https://jalammar.github.io/images/seq2seq_9.mp4' frameborder='0' allowfullscreen loop autoplay></iframe></div>
<p>The coefficients <span class="math notranslate nohighlight">\(a_i\)</span> are called the <strong>attention scores</strong> : how much attention is the decoder paying to each of the encoder’s state vectors? The attention scores <span class="math notranslate nohighlight">\(a_i\)</span> are computed as a <strong>softmax</strong> over the scores <span class="math notranslate nohighlight">\(e_i\)</span> (in order to sum to 1):</p>
<div class="math notranslate nohighlight">
\[a_i = \frac{\exp e_i}{\sum_j \exp e_j} \Rightarrow A^\text{decoder}_t = \sum_{i=0}^T \frac{\exp e_i}{\sum_j \exp e_j} \, C^\text{encoder}_i\]</div>
<div class='embed-container'><iframe src='https://jalammar.github.io/images/attention_process.mp4' frameborder='0' allowfullscreen loop autoplay></iframe></div>
<p>The score <span class="math notranslate nohighlight">\(e_i\)</span> is computed using:</p>
<ul class="simple">
<li><p>the previous output of the decoder <span class="math notranslate nohighlight">\(\mathbf{h}^\text{decoder}_{t-1}\)</span>.</p></li>
<li><p>the corresponding state vector <span class="math notranslate nohighlight">\(C^\text{encoder}_i\)</span> of the encoder at step <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p>attentional weights <span class="math notranslate nohighlight">\(W_a\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[e_i = \text{tanh}(W_a \, [\mathbf{h}^\text{decoder}_{t-1}; C^\text{encoder}_i])\]</div>
<p>Everything is differentiable, these attentional weights can be learned with BPTT.</p>
<p>The attentional context vector <span class="math notranslate nohighlight">\(A^\text{decoder}_t\)</span> is concatenated with the previous output <span class="math notranslate nohighlight">\(\mathbf{h}^\text{decoder}_{t-1}\)</span> and used as the next input <span class="math notranslate nohighlight">\(\mathbf{x}^\text{decoder}_t\)</span> of the decoder:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^\text{decoder}_t = [\mathbf{h}^\text{decoder}_{t-1} ; A^\text{decoder}_t]\]</div>
<div class='embed-container'><iframe src='https://jalammar.github.io/images/attention_tensor_dance.mp4' frameborder='0' allowfullscreen loop autoplay></iframe></div>
<div class="figure align-default" id="id49">
<a class="reference internal image-reference" href="../_images/seq2seq-attention5.png"><img alt="../_images/seq2seq-attention5.png" src="../_images/seq2seq-attention5.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.29 </span><span class="caption-text">Seq2seq architecture with attention <a class="bibtex reference internal" href="../zreferences.html#bahdanau2016" id="id17">[Bahdanau et al., 2016]</a>. Source: <a class="reference external" href="https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263">https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263</a>.</span><a class="headerlink" href="#id49" title="Permalink to this image">¶</a></p>
</div>
<p>The attention scores or <strong>alignment scores</strong> <span class="math notranslate nohighlight">\(a_i\)</span> are useful to interpret what happened. They show which words in the original sentence are the most important to generate the next word.</p>
<div class="figure align-default" id="id50">
<a class="reference internal image-reference" href="../_images/seq2seq-attention7.png"><img alt="../_images/seq2seq-attention7.png" src="../_images/seq2seq-attention7.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.30 </span><span class="caption-text">Alignment scores during translation. Source: <a class="reference external" href="https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263">https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263</a>.</span><a class="headerlink" href="#id50" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Attentional mechanisms</strong> are now central to NLP. The whole <strong>history</strong> of encoder states is passed to the decoder, which learns to decide which part is the most important using <strong>attention</strong>. This solves the bottleneck of seq2seq architectures, at the cost of much more operations. They require to use fixed-length sequences (generally 50 words).</p>
<div class="figure align-default" id="id51">
<a class="reference internal image-reference" href="../_images/seq2seq-comparison.png"><img alt="../_images/seq2seq-comparison.png" src="../_images/seq2seq-comparison.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.31 </span><span class="caption-text">Comparison of seq2seq and seq2seq with attention. Source: <a class="reference external" href="https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263">https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263</a>.</span><a class="headerlink" href="#id51" title="Permalink to this image">¶</a></p>
</div>
<p>Google Neural Machine Translation (GNMT <a class="bibtex reference internal" href="../zreferences.html#wu2016" id="id18">[Wu et al., 2016]</a>) uses an attentional recurrent NN, with bidirectional GRUs, 8 recurrent layers on 8 GPUs for both the encoder and decoder.</p>
<div class="figure align-default" id="id52">
<a class="reference internal image-reference" href="../_images/google-nmt-lstm1.png"><img alt="../_images/google-nmt-lstm1.png" src="../_images/google-nmt-lstm1.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.32 </span><span class="caption-text">Google Neural Machine Translation (GNMT <a class="bibtex reference internal" href="../zreferences.html#wu2016" id="id19">[Wu et al., 2016]</a>)</span><a class="headerlink" href="#id52" title="Permalink to this image">¶</a></p>
</div>
<p>Attentional mechanisms are so powerful that recurrent networks are not even needed anymore. <strong>Transformer networks</strong> <a class="bibtex reference internal" href="../zreferences.html#vaswani2017" id="id20">[Vaswani et al., 2017]</a> use <strong>self-attention</strong> in a purely feedforward architecture and outperform recurrent architectures. See <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a> for more explanations. Used in Google BERT and OpenAI GPT-2/3 for text understanding (e.g. search engine queries).</p>
<div class="figure align-default" id="id53">
<a class="reference internal image-reference" href="../_images/transformer_resideual_layer_norm_3.png"><img alt="../_images/transformer_resideual_layer_norm_3.png" src="../_images/transformer_resideual_layer_norm_3.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 9.33 </span><span class="caption-text">Transformer network. Source: <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></span><a class="headerlink" href="#id53" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3-deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="8-GAN.html" title="previous page"><span class="section-number">8. </span>Generative adversarial networks</a>
    <a class='right-next' id="next-link" href="../4-neurocomputing/1-Limits.html" title="next page"><span class="section-number">1. </span>Limits of deep learning</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>