

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Deep neural networks &#8212; Neurocomputing</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/2-DNN.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1. Introduction to Python" href="../5-exercises/ex1-Python.html" />
    <link rel="prev" title="1. Artificial neural networks" href="1-NN.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/2-DNN.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Deep neural networks" />
<meta property="og:description" content="Deep neural networks  Slides: pdf  Why deep neural networks?  The universal approximation theorem (Cybenko, 1989) states that a shallow network can approximate " />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-neurocomputing/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neurocomputing</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Deep neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3-deeplearning/2-DNN.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-deep-neural-networks">
   2.1. Why deep neural networks?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bad-convergence">
   2.2. Bad convergence
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimizers">
     2.2.1. Optimizers
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#stochastic-gradient-descent">
       2.2.1.1. Stochastic gradient descent
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sgd-with-momentum">
       2.2.1.2. SGD with momentum
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sgd-with-nesterov-momentum">
       2.2.1.3. SGD with Nesterov momentum
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#rmsprop">
       2.2.1.4. RMSprop
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adam">
       2.2.1.5. Adam
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#comparison-of-modern-optimizers">
       2.2.1.6. Comparison of modern optimizers
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameters-annealing">
     2.2.2. Hyperparameters annealing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameter-search">
     2.2.3. Hyperparameter search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#long-training-times">
   2.3. Long training times
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#importance-of-normalization">
     2.3.1. Importance of normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-normalization">
     2.3.2. Batch normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weight-initialization">
     2.3.3. Weight initialization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overfitting">
   2.4. Overfitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l2-and-l1-regularization">
     2.4.1. L2 and L1 Regularization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     2.4.2. Dropout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-augmentation">
     2.4.3. Data augmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#early-stopping">
     2.4.4. Early-Stopping
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vanishing-gradient">
   2.5. Vanishing gradient
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principle">
     2.5.1. Principle
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivative-of-the-activation-function">
     2.5.2. Derivative of the activation function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-neural-networks-in-practice">
   2.6. Deep neural networks in practice
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="deep-neural-networks">
<h1><span class="section-number">2. </span>Deep neural networks<a class="headerlink" href="#deep-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/3.2-DeepNN.pdf">pdf</a></p>
<div class="section" id="why-deep-neural-networks">
<h2><span class="section-number">2.1. </span>Why deep neural networks?<a class="headerlink" href="#why-deep-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>The <strong>universal approximation theorem</strong> (Cybenko, 1989) states that a <strong>shallow</strong> network can approximate any mapping function between inputs and outputs. However, if the mapping function is too complex, a shallow network may need too many hidden neurons.</p>
<p>The hidden neurons extract <strong>features</strong> in the input space: typical characteristics of the input which, when combined by the output neurons, allow to solve the classification task. Problem: the features are not hierarchically organized and cannot become complex enough.</p>
<p>Shallow networks can not work directly with raw images: noise, translation, rotation, scaling… One needs first to extract complex and useful features from the input images in order to classify them correctly.</p>
<p>A MLP with more than one hidden layer is a <strong>deep neural network</strong>. The different layers extract increasingly complex features.</p>
<p><img alt="" src="../_images/deeplearning.png" /></p>
<p>In practice, training a deep network is not as easy as the theory would suggest. Four main problems have to be solved:</p>
<ol class="simple">
<li><p><strong>Bad convergence</strong>: the loss function has many local minima.</p>
<ul class="simple">
<li><p>Momentum, adaptive optimizers, annealing…</p></li>
</ul>
</li>
<li><p><strong>Long training time</strong>: deep networks use gradient descent-like optimizers, an iterative method whose speed depends on initialization.</p>
<ul class="simple">
<li><p>Normalized initialization, batch normalization…</p></li>
</ul>
</li>
<li><p><strong>Overfitting</strong>: deep networks have a lot of free parameters, so they tend to learn by heart the training set.</p>
<ul class="simple">
<li><p>Regularisation, dropout, data augmentation, early-stopping…</p></li>
</ul>
</li>
<li><p><strong>Vanishing gradient</strong>: the first layers may not receive sufficient gradients early in training.</p>
<ul class="simple">
<li><p>ReLU activation function, unsupervised pre-training, residual networks…</p></li>
</ul>
</li>
</ol>
</div>
<div class="section" id="bad-convergence">
<h2><span class="section-number">2.2. </span>Bad convergence<a class="headerlink" href="#bad-convergence" title="Permalink to this headline">¶</a></h2>
<p>The loss function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> of a deep neural network has usually not a single global minimum, but many local minima: irregular <strong>loss landscape</strong>.</p>
<div class="figure align-default" id="id8">
<a class="reference internal image-reference" href="../_images/losslandscape.png"><img alt="../_images/losslandscape.png" src="../_images/losslandscape.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.37 </span><span class="caption-text">Visualizing the loss landscape of neural nets <a class="bibtex reference internal" href="../zreferences.html#li2018" id="id1">[LXT+18]</a>.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
<p>Gradient descent gets stuck in local minima by design. One could perform different weight initializations, in order to find per chance an initial position close enough from the global minimum, but this is <strong>inefficient</strong>.</p>
<div class="section" id="optimizers">
<h3><span class="section-number">2.2.1. </span>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="stochastic-gradient-descent">
<h4><span class="section-number">2.2.1.1. </span>Stochastic gradient descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">¶</a></h4>
<p>What we actually want to minimize is the <strong>mathematical expectation</strong> of the square error (or any other loss) on the distribution of the data.</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta) = \mathbb{E}_\mathcal{D} (||\textbf{t} - \textbf{y}||^2)
\]</div>
<p>We do not have access to the true distribution of the data, so we have to estimate it through sampling.</p>
<ul class="simple">
<li><p><strong>Batch gradient descent</strong> estimates the loss function by sampling the whole training set:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta) \approx \frac{1}{N} \sum_{i=1}^N ||\textbf{t}_i - \textbf{y}_i||^2
\]</div>
<p>The estimated gradient is then unbiased (exact) and has no variance. Batch GD gets stuck in local minima.</p>
<ul class="simple">
<li><p><strong>Online gradient descent</strong> estimates the loss function by sampling a single example:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta) \approx ||\textbf{t}_i - \textbf{y}_i||^2
\]</div>
<p>The estimated gradient has a high variance (never right) but is unbiased on average. Online GD avoids local minima, but also global minima (unstable)…</p>
<ul class="simple">
<li><p><strong>Stochastic gradient descent</strong> samples <strong>minibatches</strong> of <span class="math notranslate nohighlight">\(K\)</span> ~ 100 examples to approximate the mathematical expectation.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta) = E_\mathcal{D} (||\textbf{t} - \textbf{y}||^2) \approx \frac{1}{K} \sum_{i=1}^K ||\textbf{t}_i - \textbf{y}_i||^2
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta \theta = - \eta \, \nabla_\theta  \, \mathcal{L}(\theta)
\]</div>
<p>This sampled loss has a high <strong>variance</strong>: take another minibatch and the gradient of the loss function will likely be very different. If the <strong>batch size</strong> is big enough, the estimated gradient is wrong, but usable on average (unbiased). The <strong>high variance</strong> of the estimated gradient helps getting out of local minimum: because our estimation of the gradient is often <strong>wrong</strong>, we get out of the local minima although we should have stayed in it. The <em>true</em> gradient is 0 for a local minimum, but its sampled value may not, so the parameters will be updated and hopefully get out of the local minimum. Which <strong>batch size</strong> works the best for your data? You need to use <strong>cross-validation</strong>, but beware that big batch sizes increase memory consumption, what can be a problem on GPUs.</p>
<p>Another issue with stochastic gradient descent is that it uses the same learning rate for all parameters. In <strong>ravines</strong> (which are common around minima), some parameters (or directions) have a higher influence on the loss function than others.</p>
<div class="figure align-default" id="id9">
<a class="reference internal image-reference" href="../_images/ravine.png"><img alt="../_images/ravine.png" src="../_images/ravine.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.38 </span><span class="caption-text">Ravine in the loss function. Source: <a class="reference external" href="https://distill.pub/2017/momentum/">https://distill.pub/2017/momentum/</a>.</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<p>In the example above, you may want to go faster in the “horizontal” direction than in the “vertical” one, although the gradient is very small in the horizontal direction. With a fixed high learning rate for all parameters, SGD would start oscillating for the steep parameters, while being still very slow for the flat ones.  The high variance of the sampled gradient is detrimental to performance as it can lead to oscillations. Most modern optimizers have a <strong>parameter-dependent adaptive learning rate</strong>.</p>
</div>
<div class="section" id="sgd-with-momentum">
<h4><span class="section-number">2.2.1.2. </span>SGD with momentum<a class="headerlink" href="#sgd-with-momentum" title="Permalink to this headline">¶</a></h4>
<p>One solution is to <strong>smooth</strong> the gradients over time (i.e. between minibatches), in order to avoid that one parameter is increased by one minibatch and decreased by the next one. The momentum method uses a <strong>moving average</strong> of the gradient (momentum step) to update the parameters:</p>
<div class="math notranslate nohighlight">
\[
    v(\theta) = \alpha \, v(\theta) - (1 - \alpha)  \, \nabla_\theta  \, \mathcal{L}(\theta)
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta \theta = \eta \,  v(\theta)
\]</div>
<p><span class="math notranslate nohighlight">\(0 \leq \alpha &lt; 1\)</span> controls how much of the gradient we use for the parameter update (usually around 0.9). <span class="math notranslate nohighlight">\(\alpha=0\)</span> is the vanilla SGD.</p>
<p><img alt="" src="../_images/momentum-sgd.gif" /></p>
<p>When the gradient for a single parameter has always the same direction between successive examples, gradient descent accelerates (bigger steps). When its sign changes, the weight changes continue in the same direction for while, allowing to “jump” over small local minima if the speed is sufficient. If the gradient keeps being in the opposite direction, the weight changes will finally reverse their direction. SGD with momentum uses an <strong>adaptive learning rate</strong>: the learning is implictly higher when the gradient does not reverse its sign (the estimate “accelerates”).</p>
<div class="figure align-default" id="id10">
<a class="reference internal image-reference" href="../_images/momentum-goh.png"><img alt="../_images/momentum-goh.png" src="../_images/momentum-goh.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.39 </span><span class="caption-text">The momentum dampens oscillations around ravines. Source: <a class="reference external" href="https://distill.pub/2017/momentum/">https://distill.pub/2017/momentum/</a>.</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>With momentum, the flat parameters keep increasing their update speed, while the steep ones slow down. SGD with momentum gets rid of oscillations at higher learning rates. The momentum method benefits a lot from the variance of SGD: noisy gradients are used to escape local minima but are averaged around the global minimum.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Check the great visualization by Gabriel Goh on <a class="reference external" href="https://distill.pub/2017/momentum/">https://distill.pub/2017/momentum/</a>.</p>
</div>
</div>
<div class="section" id="sgd-with-nesterov-momentum">
<h4><span class="section-number">2.2.1.3. </span>SGD with Nesterov momentum<a class="headerlink" href="#sgd-with-nesterov-momentum" title="Permalink to this headline">¶</a></h4>
<div class="figure align-default" id="id11">
<a class="reference internal image-reference" href="../_images/momentum-vs-nesterov-momentum.gif"><img alt="../_images/momentum-vs-nesterov-momentum.gif" src="../_images/momentum-vs-nesterov-momentum.gif" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.40 </span><span class="caption-text">The momentum method tends to oscillate around the global minimum. Source: <a class="reference external" href="https://ikocabiyik.com/blog/en/visualizing-ml-optimizers/">https://ikocabiyik.com/blog/en/visualizing-ml-optimizers/</a>.</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
<p>SGD with momentum tends to oscillate around the minimum. The Nesterov momentum <strong>corrects</strong> these oscillations by estimating the gradient <strong>after</strong> the momentum update:</p>
<div class="math notranslate nohighlight">
\[
    v(\theta) = \alpha \, v(\theta) - (1 - \alpha) \, \nabla_\theta  \, \mathcal{L}(\theta \color{red}{+ \alpha \, v(\theta)})
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta \theta = \eta \,  v(\theta)
\]</div>
<div class="figure align-default" id="id12">
<a class="reference internal image-reference" href="../_images/nesterov.jpeg"><img alt="../_images/nesterov.jpeg" src="../_images/nesterov.jpeg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.41 </span><span class="caption-text">Difference between the momentum and Nesterov momentum. Source: <a class="reference external" href="https://cs231n.github.io/neural-networks-3/">https://cs231n.github.io/neural-networks-3/</a></span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="rmsprop">
<h4><span class="section-number">2.2.1.4. </span>RMSprop<a class="headerlink" href="#rmsprop" title="Permalink to this headline">¶</a></h4>
<p>Instead of smoothing the gradient, what destroys information, one could adapt the learning rate to the <strong>curvature</strong> of the loss function:</p>
<ul class="simple">
<li><p>put the <strong>brakes</strong> on when the function is steep (high gradient).</p></li>
<li><p><strong>accelerate</strong> when the loss function is flat (plateau).</p></li>
</ul>
<p>RMSprop (Root Mean Square Propagation, proposed by Geoffrey Hinton in his lecture <a class="reference external" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a>) scales the learning rate by a running average of the squared gradient (second moment <span class="math notranslate nohighlight">\(\approx\)</span> variance).</p>
<div class="math notranslate nohighlight">
\[
    v(\theta) = \alpha \, v(\theta) + (1 - \alpha) \, (\nabla_\theta  \, \mathcal{L}(\theta))^2
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta \theta = - \frac{\eta}{\epsilon + \sqrt{v(\theta)}} \, \nabla_\theta  \, \mathcal{L}(\theta)
\]</div>
<p>If the gradients vary a lot between two minibatches, the learning rate is reduced. If the gradients do not vary much, the learning rate is increased.</p>
</div>
<div class="section" id="adam">
<h4><span class="section-number">2.2.1.5. </span>Adam<a class="headerlink" href="#adam" title="Permalink to this headline">¶</a></h4>
<p>Adam (Adaptive Moment Estimation, <a class="bibtex reference internal" href="../zreferences.html#kingma2014" id="id2">[KB14]</a>) builds on the idea of RMSprop, but uses also a moving average of the gradient.</p>
<div class="math notranslate nohighlight">
\[
    m(\theta) = \beta_1 m(\theta) + (1 - \beta_1) \, \nabla_\theta  \, \mathcal{L}(\theta)
\]</div>
<div class="math notranslate nohighlight">
\[
    v(\theta) = \beta_2 v(\theta) + (1 - \beta_2) \, \nabla_\theta  \, \mathcal{L}(\theta)^2
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta \theta = - \eta \, \frac{m(\theta)}{\epsilon + \sqrt{v(\theta)}}
\]</div>
<p>In short: Adam = RMSprop + momentum. Other possible optimizers: Adagrad, Adadelta, AdaMax, Nadam…</p>
</div>
<div class="section" id="comparison-of-modern-optimizers">
<h4><span class="section-number">2.2.1.6. </span>Comparison of modern optimizers<a class="headerlink" href="#comparison-of-modern-optimizers" title="Permalink to this headline">¶</a></h4>
<p>The different optimizers build on the idea of gradient descent and try to fix the main issues. They have different convergence properties, which can be seen in the figures below.</p>
<p>In practice, SGD with momentum allows to find better solutions (global minimum), but the meta-parameters are harder to find (need for cross-validation). Adam finds slightly poorer solutions, but the parameters <span class="math notranslate nohighlight">\(\beta_1\)</span>, <span class="math notranslate nohighlight">\(\beta_2\)</span> and <span class="math notranslate nohighlight">\(\epsilon\)</span> can usually be kept at default, so it is a good idea to start with it, find the NN architecture that solves the problem and then replace it with SGD+momentum to fine-tune the performance.</p>
<div class="figure align-default" id="id13">
<a class="reference internal image-reference" href="../_images/contours_evaluation_optimizers.gif"><img alt="../_images/contours_evaluation_optimizers.gif" src="../_images/contours_evaluation_optimizers.gif" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.42 </span><span class="caption-text">Source: Alec Radford <a class="reference external" href="https://imgur.com/a/Hqolp">https://imgur.com/a/Hqolp</a></span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id14">
<a class="reference internal image-reference" href="../_images/saddle_point_evaluation_optimizers.gif"><img alt="../_images/saddle_point_evaluation_optimizers.gif" src="../_images/saddle_point_evaluation_optimizers.gif" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.43 </span><span class="caption-text">Source: Alec Radford <a class="reference external" href="https://imgur.com/a/Hqolp">https://imgur.com/a/Hqolp</a>.</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The different optimizers are available in keras, see <a class="reference external" href="https://keras.io/api/optimizers">https://keras.io/api/optimizers</a>.</p>
<ul class="simple">
<li><p>SGD:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>SGD with Nesterov momentum:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
    <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>RMSprop:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-07</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Adam:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
    <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-07</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="hyperparameters-annealing">
<h3><span class="section-number">2.2.2. </span>Hyperparameters annealing<a class="headerlink" href="#hyperparameters-annealing" title="Permalink to this headline">¶</a></h3>
<p>Finding the optimal value for the hyperparameters (or metaparameters) of the network is not easy: learning rate <span class="math notranslate nohighlight">\(\eta\)</span>, momentum <span class="math notranslate nohighlight">\(\alpha\)</span>, etc.</p>
<div class="figure align-default" id="id15">
<a class="reference internal image-reference" href="../_images/learningrates.jpeg"><img alt="../_images/learningrates.jpeg" src="../_images/learningrates.jpeg" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.44 </span><span class="caption-text">Finding the optimal learning rate is difficult. Source: <a class="reference external" href="https://cs231n.github.io/neural-networks-3/">https://cs231n.github.io/neural-networks-3/</a></span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p>For example, choosing <span class="math notranslate nohighlight">\(\eta\)</span> too small leads to very slow learning. Choosing it too big can lead to oscillations and prevent convergence. A better strategy is to start with a big learning rate to “roughly” find the position of the global minimum and progressively decrease its value for a better convergence:</p>
<div class="math notranslate nohighlight">
\[
    \eta \leftarrow (1 - \beta) \, \eta
\]</div>
<p>The decrease can be applied after each epoch.  <span class="math notranslate nohighlight">\(\beta\)</span> is called the <strong>decay rate</strong>, usually very small (<span class="math notranslate nohighlight">\(10^{-6}\)</span>). The method is called <strong>annealing</strong> or <strong>scheduling</strong>.</p>
<div class="figure align-default" id="id16">
<a class="reference internal image-reference" href="../_images/learningratescheduler.jpeg"><img alt="../_images/learningratescheduler.jpeg" src="../_images/learningratescheduler.jpeg" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.45 </span><span class="caption-text">Exponentially-decaying learning rate. Source: <a class="reference external" href="https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1">https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1</a></span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
<p>A simple trick to find a good estimate of the learning rate (or its start/stop value) is to increase its value exponentially <strong>for each minibatch</strong> at the beginning of learning. The “good” region for the learning rate is the one where the validation loss decreases, but does not oscillate.</p>
<div class="figure align-default" id="id17">
<a class="reference internal image-reference" href="../_images/learningrate-selection.png"><img alt="../_images/learningrate-selection.png" src="../_images/learningrate-selection.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.46 </span><span class="caption-text">Estimating the optimal range for the learning rate by increasing its value at the beginning of learning. Source <a class="reference external" href="https://towardsdatascience.com/advanced-topics-in-neural-networks-f27fbcc638ae">https://towardsdatascience.com/advanced-topics-in-neural-networks-f27fbcc638ae</a>.</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="hyperparameter-search">
<h3><span class="section-number">2.2.3. </span>Hyperparameter search<a class="headerlink" href="#hyperparameter-search" title="Permalink to this headline">¶</a></h3>
<p>Even with annealing, it is tricky to find the optimal value of the hyperparameters. The only option is to perform <strong>cross-validation</strong>, by varying the hyperparameters systematically and initializing the weights randomly every time. There are two basic strategies:</p>
<ul class="simple">
<li><p><strong>Grid search</strong>: different values of each parameter are chosen linearly (<span class="math notranslate nohighlight">\([0.1, 0.2, \ldots, 0.9]\)</span>) or logarithmically (<span class="math notranslate nohighlight">\([10^{-6}, 10^{-5}, \ldots, 10^{-1}]\)</span>).</p></li>
<li><p><strong>Random search</strong>: the value are randomly chosen each time from some distribution (uniform, normal, lognormal).</p></li>
</ul>
<div class="figure align-default" id="id18">
<a class="reference internal image-reference" href="../_images/gridsearchbad.jpg"><img alt="../_images/gridsearchbad.jpg" src="../_images/gridsearchbad.jpg" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.47 </span><span class="caption-text">Grid-search vs. random search. Source: <a class="reference external" href="http://cs231n.github.io/neural-networks-3/">http://cs231n.github.io/neural-networks-3/</a></span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
<p>The advantage of random search is that you can stop it anytime if you can not wait any longer. Grid search is very time-consuming, but easy to perform in parallel if you have clusters of CPUs or GPUs (<strong>data-parallel</strong>).</p>
<p>A more advanced and efficient technique is <strong>Bayesian hyperparameter optimization</strong>, for example the <strong>Tree Parzen Estimator</strong> (TPE) algorithm. The idea is to build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. Roughly speaking, it focuses parameter sampling on the interesting regions.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">hyperopt</span></code> Python library <a class="reference external" href="https://github.com/hyperopt/hyperopt">https://github.com/hyperopt/hyperopt</a> is extremely simple to use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">hyperopt</span> <span class="kn">import</span> <span class="n">fmin</span><span class="p">,</span> <span class="n">tpe</span><span class="p">,</span> <span class="n">hp</span><span class="p">,</span> <span class="n">STATUS_OK</span>

<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">eta</span><span class="p">):</span>
    <span class="c1"># Train model with:</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">test_loss</span><span class="p">,</span> <span class="s1">&#39;status&#39;</span><span class="p">:</span> <span class="n">STATUS_OK</span> <span class="p">}</span>

<span class="n">best</span> <span class="o">=</span> <span class="n">fmin</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span>
    <span class="n">space</span><span class="o">=</span><span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;eta&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">algo</span><span class="o">=</span><span class="n">tpe</span><span class="o">.</span><span class="n">suggest</span><span class="p">,</span>
    <span class="n">max_evals</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="nb">print</span> <span class="n">best</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="long-training-times">
<h2><span class="section-number">2.3. </span>Long training times<a class="headerlink" href="#long-training-times" title="Permalink to this headline">¶</a></h2>
<div class="section" id="importance-of-normalization">
<h3><span class="section-number">2.3.1. </span>Importance of normalization<a class="headerlink" href="#importance-of-normalization" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/meanremoval.gif" /></p>
<p>If the data is not centered in the input space, the hyperplane (i.e. each neuron) may need a lot of iterations to “move” to the correct position using gradient descent. The initialization of the weights will matter a lot: if you start too far away from the solution, you will need many iterations.</p>
<p>If the data is normalized (zero mean, unit variance), the bias can be initialized to 0 and will converge much faster. Only the <strong>direction</strong> of the weight vector matters, not its norm, so it will be able to classify the data much faster.</p>
<div class="figure align-default" id="id19">
<a class="reference internal image-reference" href="../_images/datanormalization.jpg"><img alt="../_images/datanormalization.jpg" src="../_images/datanormalization.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.48 </span><span class="caption-text">Input data normalization. Source: <a class="reference external" href="http://cs231n.github.io/neural-networks-2/">http://cs231n.github.io/neural-networks-2/</a></span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<p>In practice, the input data <span class="math notranslate nohighlight">\(X\)</span> <strong>must</strong> be normalized before training, in order to improve the training time:</p>
<ul class="simple">
<li><p><strong>Mean removal</strong> or <strong>zero-centering</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    X' = X - \mathbb{E}(X)
\]</div>
<ul class="simple">
<li><p><strong>Normalization</strong> : mean removal + <strong>unit variance</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    X' = \frac{X - \mathbb{E}(X)}{\text{Std}(X)}
\]</div>
<p><strong>Whitening</strong> goes one step further by first decorrelating the input dimensions (using <strong>Principal Component Analysis</strong> - PCA) and then scaling them so that the data lies in the unit sphere. It is  better method than simple data normalization, but computationally expensive. When predicting on new data, do not forget to normalize/whiten them too!</p>
<div class="figure align-default" id="id20">
<a class="reference internal image-reference" href="../_images/datawhitening.jpg"><img alt="../_images/datawhitening.jpg" src="../_images/datawhitening.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.49 </span><span class="caption-text">Input data whitening. Source: <a class="reference external" href="http://cs231n.github.io/neural-networks-2/">http://cs231n.github.io/neural-networks-2/</a></span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="batch-normalization">
<h3><span class="section-number">2.3.2. </span>Batch normalization<a class="headerlink" href="#batch-normalization" title="Permalink to this headline">¶</a></h3>
<p>A single layer can learn very fast if its inputs are normalized with zero mean and unit variance. This is easy to do for the first layer, as one only need to preprocess the inputs <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, but not the others. The outputs of the first layer are not normalized anymore, so learning in the second layer will be slow.</p>
<p><img alt="" src="../_images/batchnorm.png" /></p>
<p><strong>Batch normalization</strong> <a class="bibtex reference internal" href="../zreferences.html#ioffe2015" id="id3">[IS15]</a> allows each layer to normalize its inputs on a <strong>single minibatch</strong>:</p>
<div class="math notranslate nohighlight">
\[
    X_\mathcal{B}' = \frac{X_\mathcal{B} - E(X_\mathcal{B})}{\text{Std}(X_\mathcal{B})}
\]</div>
<p>The mean and variance will vary from one minibatch to another, but it does not matter. At the end of learning, the mean and variance over the whole training set is computed and stored. BN allows to more easily initialize the weights relative to the input strength and to use higher learning rates.</p>
<p>The <strong>Batch Normalization</strong> layer is usually placed between the FC layer and the activation function.It is differentiable w.r.t the input layer and the parameters, so backpropagation still works.</p>
<div class="figure align-default" id="id21">
<a class="reference internal image-reference" href="../_images/batch_normalization.png"><img alt="../_images/batch_normalization.png" src="../_images/batch_normalization.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.50 </span><span class="caption-text">A batch normalization layer should be introduced between the linear net activation and the activation function. Source: <a class="reference external" href="http://heimingx.cn/2016/08/18/cs231n-neural-networks-part-2-setting-up-the-Data-and-the-loss/">http://heimingx.cn/2016/08/18/cs231n-neural-networks-part-2-setting-up-the-Data-and-the-loss/</a></span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="weight-initialization">
<h3><span class="section-number">2.3.3. </span>Weight initialization<a class="headerlink" href="#weight-initialization" title="Permalink to this headline">¶</a></h3>
<p>Weight matrices are initialized randomly, but how they are initialized impacts performance a lot There are empirical rules to initialize the weights between two layers with <span class="math notranslate nohighlight">\(N_{\text{in}}\)</span> and <span class="math notranslate nohighlight">\(N_{\text{out}}\)</span> neurons.</p>
<ul class="simple">
<li><p><strong>Xavier</strong>: Uniform initialization (when using logistic or tanh, <a class="bibtex reference internal" href="../zreferences.html#glorot2010" id="id4">[GB10]</a>):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    W \in \mathcal{U}( - \sqrt{\frac{6}{N_{\text{in}}+N_{\text{out}}}} , \sqrt{\frac{6}{N_{\text{in}}+N_{\text{out}}}}  )
\]</div>
<ul class="simple">
<li><p><strong>He</strong>: Gaussian initialization (when using ReLU or PReLU, <a class="bibtex reference internal" href="../zreferences.html#he2015a" id="id5">[HZRS15]</a>):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    W \in \mathcal{N}( 0 , \sqrt{\frac{2}{N_{\text{in}} }} )
\]</div>
<ul class="simple">
<li><p>When using BN, the bias <span class="math notranslate nohighlight">\(b\)</span> can be initialized to 0.</p></li>
<li><p>Most frameworks (tensorflow, pytorch) initialize the weights correctly for you, but you can also control it.</p></li>
</ul>
</div>
</div>
<div class="section" id="overfitting">
<h2><span class="section-number">2.4. </span>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this headline">¶</a></h2>
<p>The main problem with deep NN is <strong>overfitting</strong>. With increasing depth, the network has too many weights = free parameters, so its VC dimension is high.</p>
<div class="math notranslate nohighlight">
\[
    \epsilon = \frac{\text{VC}_\text{dim}}{N}
\]</div>
<p>The training error will be very small, but the generalization error high. <strong>The network learns the data, not the underlying function.</strong></p>
<p>We need to put constraints on the weights to reduce the VC dimension.</p>
<ul class="simple">
<li><p>If the weights move freely (i.e. can take any value), the VC dimension is equal to the number of free parameters.</p></li>
<li><p>If the weights cannot take any value they like, this implicitely reduces the VC dimension.</p></li>
</ul>
<p>In linear classification, the weights were unconstrained: the norm of the weight vector can take any value, as only its direction is important.</p>
<p><strong>Intuition:</strong> The norm of the weight vector influences the speed of learning in linear classification. A weight update on a strong weight has less influence than on a weak weight:</p>
<div class="math notranslate nohighlight">
\[
    W \leftarrow W + \Delta W = W - \eta \, \frac{\partial \mathcal{l}(\theta)}{\partial W}
\]</div>
<p>as the gradient <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{l}(\theta)}{\partial W}\)</span> does not depend on the norm of the weights, only the output error.</p>
<div class="section" id="l2-and-l1-regularization">
<h3><span class="section-number">2.4.1. </span>L2 and L1 Regularization<a class="headerlink" href="#l2-and-l1-regularization" title="Permalink to this headline">¶</a></h3>
<p><strong><span class="math notranslate nohighlight">\(\mathcal{L}_2\)</span> regularization</strong> keeps the <span class="math notranslate nohighlight">\(\mathcal{L}_2\)</span> norm of the free parameters <span class="math notranslate nohighlight">\(||\theta||\)</span> as small as possible during learning.</p>
<div class="math notranslate nohighlight">
\[
    ||\theta||^2 = w_1^2 + w_2^2 + \dots + w_M^2
\]</div>
<p>Each neuron will use all its inputs with small weights, instead on specializing on a small part with high weights. Two things have to be minimized at the same time: the training loss and a <strong>penalty term</strong> representing the norm of the weights:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta) = \mathbb{E}_\mathcal{D}( ||\mathbf{t} - \mathbf{y}||^2) + \lambda \, ||\theta||^2
\]</div>
<p>The <strong>regularization parameter</strong> <span class="math notranslate nohighlight">\(\lambda\)</span> controls the strength of regularization:</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(\lambda\)</span> is small, there is only a small regularization, the weights can increase.</p></li>
<li><p>if <span class="math notranslate nohighlight">\(\lambda\)</span> is high, the weights will be kept very small, but they may not minimize the training loss.</p></li>
</ul>
<p>Example of the mse loss with <span class="math notranslate nohighlight">\(\mathcal{L}_2\)</span> regularization penalty term:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta) = \mathbb{E}_\mathcal{D} [||\mathbf{t} - \mathbf{y}||^2] + \lambda \, ||\theta||^2
\]</div>
<p>The gradient of the new loss function is easy to find:</p>
<div class="math notranslate nohighlight">
\[
    \nabla_\theta \mathcal{L}(\theta) = - 2 \, (\mathbf{t} - \mathbf{y}) \nabla_\theta \mathbf{y} + 2 \, \lambda \, \theta
\]</div>
<p>The parameter updates become:</p>
<div class="math notranslate nohighlight">
\[
    \Delta \theta = \eta \, (\mathbf{t} - \mathbf{y}) \nabla_\theta \mathbf{y} - \eta \, \lambda \, \theta
\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{L}_2\)</span> regularization leads to <strong>weight decay</strong>: even if there is no output error, the weight will converge to 0. This forces the weight to constantly learn: it can not specialize on a particular example anymore (overfitting) and is forced to generalize.</p>
<p><strong><span class="math notranslate nohighlight">\(\mathcal{L}_1\)</span> regularization</strong> penalizes the absolute value of the weights instead of their Euclidian norm:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta) = \mathbb{E}_\mathcal{D} [||\mathbf{t} - \mathbf{y}||^2] + \lambda \, |\theta|\]</div>
<p>It leads to very sparse representations: a lot of neurons will be inactive, and only a few will represent the input.</p>
</div>
<div class="section" id="dropout">
<h3><span class="section-number">2.4.2. </span>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<p>Randomly dropping (inactivating) some neurons with a <strong>probability</strong> <span class="math notranslate nohighlight">\(p\)</span> between two input presentations reduces the number of free parameters available for each learning phase. Multiple smaller networks (smaller VC dimension) are in fact learned in parallel on different data, but they share some parameters. This <strong>dropout</strong> method forces the network to generalize <a class="bibtex reference internal" href="../zreferences.html#srivastava2014" id="id6">[SHK+14]</a>. It is a form of regularization (mathematically equivalent to L2), now preferred in deep networks. <em>p</em> is usually around 0.5.</p>
<div class="figure align-default" id="id22">
<a class="reference internal image-reference" href="../_images/dropout.gif"><img alt="../_images/dropout.gif" src="../_images/dropout.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.51 </span><span class="caption-text">Each new input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> (or minibatch of inputs) is learned by a different neural network, but <strong>on average</strong>, the big neural network has learned the whole dataset without overfitting. Source: <a class="reference external" href="https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a">https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a</a></span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="data-augmentation">
<h3><span class="section-number">2.4.3. </span>Data augmentation<a class="headerlink" href="#data-augmentation" title="Permalink to this headline">¶</a></h3>
<p>The best way to avoid overfitting is to use more data (with variability), but this is not always possible. A simple trick to have more data is <strong>data augmentation</strong>, i.e. modifying the inputs while keeping the output constant. For object recognition, it consists of applying various affine transformations (translation, rotation, scaling) on each input image, while keeping the label constant. This allows virtually infinite training sets.</p>
<div class="figure align-default" id="id23">
<a class="reference internal image-reference" href="../_images/data_augmentation2.png"><img alt="../_images/data_augmentation2.png" src="../_images/data_augmentation2.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.52 </span><span class="caption-text">Data augmentation. Source : <a class="reference external" href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html">https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html</a></span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="early-stopping">
<h3><span class="section-number">2.4.4. </span>Early-Stopping<a class="headerlink" href="#early-stopping" title="Permalink to this headline">¶</a></h3>
<p>Early-stopping fights overfitting by monitoring the model’s performance on a validation set. A <strong>validation set</strong> is a set of examples that we never use for gradient descent, but which is also not a part of the test set. If the model’s performance ceases to improve sufficiently on the validation set, or even degrades with further optimization, we can either stop learning or modify some meta-parameters (learning rate, momentum, regularization…). The validation loss is usually lower than the training loss at the beginning of learning (underfitting), but becomes higher when the network overfits.</p>
<div class="figure align-default" id="id24">
<a class="reference internal image-reference" href="../_images/earlystopping.png"><img alt="../_images/earlystopping.png" src="../_images/earlystopping.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.53 </span><span class="caption-text">Early-stopping by checking the validation loss during training.</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="vanishing-gradient">
<h2><span class="section-number">2.5. </span>Vanishing gradient<a class="headerlink" href="#vanishing-gradient" title="Permalink to this headline">¶</a></h2>
<div class="section" id="principle">
<h3><span class="section-number">2.5.1. </span>Principle<a class="headerlink" href="#principle" title="Permalink to this headline">¶</a></h3>
<p>Contrary to what we could think, adding more layers to a DNN does not necessarily lead to a better performance, both on the training and test set. Here is the performance of neural networks with 20 or 56 layers on <strong>CIFAR-10</strong>:</p>
<div class="figure align-default" id="id25">
<a class="reference internal image-reference" href="../_images/vanishinggradient-cifar.png"><img alt="../_images/vanishinggradient-cifar.png" src="../_images/vanishinggradient-cifar.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.54 </span><span class="caption-text">Adding more layers does not necessarily increase the training or test accuracy on CIFAR-10. Source: <a class="reference external" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8</a></span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
<p>The main reason behind this is the <strong>vanishing gradient problem</strong>. The gradient of the loss function is repeatedly multiplied by a weight matrix <span class="math notranslate nohighlight">\(W\)</span> as it travels backwards in a deep network.</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathbf{h}_{k}}{\partial \mathbf{h}_{k-1}} = f' (W^k \, \mathbf{h}_{k-1} + \mathbf{b}^k) \, W^k
\]</div>
<p>When it arrives in the first FC layer, the contribution of the weight matrices is comprised between:</p>
<div class="math notranslate nohighlight">
\[
    (W_\text{min})^d \quad \text{and} \quad (W_\text{max})^d
\]</div>
<p>where <span class="math notranslate nohighlight">\(W_\text{max}\)</span> (resp. <span class="math notranslate nohighlight">\(W_\text{min}\)</span>) is the weight matrix with the highest (resp. lowest) norm, and <span class="math notranslate nohighlight">\(d\)</span> is the depth of the network.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(|W_\text{max}| &lt; 1\)</span>, then <span class="math notranslate nohighlight">\((W_\text{max})^d\)</span> is very small for high values of <span class="math notranslate nohighlight">\(d\)</span> : <strong>the gradient vanishes</strong>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(|W_\text{min}| &gt; 1\)</span>, then <span class="math notranslate nohighlight">\((W_\text{min})^d\)</span> is very high for high values of <span class="math notranslate nohighlight">\(d\)</span> : <strong>the gradient explodes</strong>.</p></li>
</ul>
<p><strong>Exploding gradients</strong> can be solved by <strong>gradient clipping</strong>, i.e. normalizing the backpropagated gradient if its norm exceeds a threshold.</p>
<div class="math notranslate nohighlight">
\[
    || \frac{\partial \mathcal{L}(\theta)}{\partial W^k}|| \gets \min(||\frac{\partial \mathcal{L}(\theta)}{\partial W^k}||, \text{MAX_GRAD})
\]</div>
<p><strong>Vanishing gradients</strong> are still the current limitation of deep networks. The solutions include: ReLU activation functions, unsupervised pre-training, batch normalization, <strong>residual networks</strong>…</p>
<div class="figure align-default" id="id26">
<a class="reference internal image-reference" href="../_images/vanishinggradient.png"><img alt="../_images/vanishinggradient.png" src="../_images/vanishinggradient.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.55 </span><span class="caption-text">Vanishing gradient problem. Source: <a class="reference external" href="https://smartstuartkim.wordpress.com/2019/02/09/vanishing-gradient-problem/">https://smartstuartkim.wordpress.com/2019/02/09/vanishing-gradient-problem/</a></span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="derivative-of-the-activation-function">
<h3><span class="section-number">2.5.2. </span>Derivative of the activation function<a class="headerlink" href="#derivative-of-the-activation-function" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id27">
<a class="reference internal image-reference" href="../_images/derivative_sigmoid.png"><img alt="../_images/derivative_sigmoid.png" src="../_images/derivative_sigmoid.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.56 </span><span class="caption-text">The derivative of the logistic function is 0 for extreme values, what blocks the backpropagation of the gradient.</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
<p>Old-school MLP used logistic or tanh transfer functions for the hidden neurons, but their gradient is zero for very high or low net activations. If a neuron is saturated, it won’t transmit the gradient backwards, so the vanishing gradient is even worse. Deep networks now typically use the ReLU <a class="bibtex reference internal" href="../zreferences.html#maas2013" id="id7">[MHN13]</a> or PReLU activation functions to improve convergence.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    f'(x) = \begin{cases} 1 \qquad \text{if } x &gt; 0 \\
                          \alpha \qquad \text{if } x \leq 0 \\
            \end{cases}
\end{split}\]</div>
<p>PReLU always backpropagates the gradient, so it helps fighting against vanishing gradient.</p>
</div>
</div>
<div class="section" id="deep-neural-networks-in-practice">
<h2><span class="section-number">2.6. </span>Deep neural networks in practice<a class="headerlink" href="#deep-neural-networks-in-practice" title="Permalink to this headline">¶</a></h2>
<p>The definition of a deep NN in keras can be as simple as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tf.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tf.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">tf.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Input</span><span class="p">(</span><span class="mi">784</span><span class="p">,))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">200</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span> 
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span> 
<span class="p">)</span>
</pre></div>
</div>
<p>If you want to successfully train a deep neural network, you should:</p>
<ul class="simple">
<li><p>Use as much data as possible, with <strong>data augmentation</strong> if needed.</p></li>
<li><p><strong>Normalize</strong> the inputs.</p></li>
<li><p>Use <strong>batch normalization</strong> in every layer and at least <strong>ReLU</strong>.</p></li>
<li><p>Use a good <strong>optimizer</strong> (SGD with momentum, Adam).</p></li>
<li><p><strong>Regularize</strong> learning (L2, L1, dropout).</p></li>
<li><p>Track overfitting on the validation set and use <strong>early-stopping</strong>.</p></li>
<li><p>Search for the best <strong>hyperparameters</strong> using grid search or hyperopt:</p>
<ul>
<li><p>Learning rate, schedule, momentum, dropout level, number of layers/neurons, transfer functions, etc.</p></li>
</ul>
</li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3-deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="1-NN.html" title="previous page"><span class="section-number">1. </span>Artificial neural networks</a>
    <a class='right-next' id="next-link" href="../5-exercises/ex1-Python.html" title="next page"><span class="section-number">1. </span>Introduction to Python</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>