
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2. Linear Regression &#8212; Neurocomputing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/2-linear/2-LinearRegression.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Regularization" href="3-Regularization.html" />
    <link rel="prev" title="1. Optimization" href="1-Optimization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tuc.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neurocomputing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/10-Attention.html">
   10. Attentional neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/2-linear/2-LinearRegression.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   2.1. Linear regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#least-mean-squares">
     2.1.1. Least Mean Squares
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#delta-learning-rule">
     2.1.2. Delta learning rule
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-linear-regression">
   2.2. Multiple linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#logistic-regression">
   2.3. Logistic regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#polynomial-regression">
   2.4. Polynomial regression
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1><span class="section-number">2. </span>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/2.2-LinearRegression.pdf">pdf</a></p>
<section id="id1">
<h2><span class="section-number">2.1. </span>Linear regression<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/lEILkDvT0gI' frameborder='0' allowfullscreen></iframe></div>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="../_images/regression-animation2.png"><img alt="../_images/regression-animation2.png" src="../_images/regression-animation2.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.23 </span><span class="caption-text">Simple linear regression. <span class="math notranslate nohighlight">\(x\)</span> is the input, <span class="math notranslate nohighlight">\(y\)</span> the output. The data is represented by blue dots, the model by the black line.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Let’s consider a training set of N examples <span class="math notranslate nohighlight">\(\mathcal{D} = (x_i, t_i)_{i=1..N}\)</span>. In <strong>linear regression</strong>, we want to learn a linear model (hypothesis) <span class="math notranslate nohighlight">\(y\)</span> that is linearly dependent on the input <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    y = f_{w, b}(x) = w \, x + b
\]</div>
<p>The <strong>free parameters</strong> of the model are the slope <span class="math notranslate nohighlight">\(w\)</span> and the intercept <span class="math notranslate nohighlight">\(b\)</span>. This model corresponds to a single <strong>artificial neuron</strong> with output <span class="math notranslate nohighlight">\(y\)</span>, having one input <span class="math notranslate nohighlight">\(x\)</span>, one weight <span class="math notranslate nohighlight">\(w\)</span>, one bias <span class="math notranslate nohighlight">\(b\)</span> and a <strong>linear</strong> activation function <span class="math notranslate nohighlight">\(f(x) = x\)</span>.</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../_images/artificialneuron.svg"><img alt="../_images/artificialneuron.svg" src="../_images/artificialneuron.svg" width="60%" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.24 </span><span class="caption-text">Artificial neuron with multiple inputs.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The goal of the linear regression (or least mean squares - LMS) is to minimize the <strong>mean square error</strong> (mse) between the targets and the predictions. This loss function is defined as the mathematical expectation of the quadratic error over the training set:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(w, b) =  \mathbb{E}_{x_i, t_i \in \mathcal{D}} [ (t_i - y_i )^2 ]
\]</div>
<p>As the training set is finite and the samples i.i.d, we can simply replace the expectation by an average over the training set:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(w, b) = \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2
\]</div>
<p>The minimum of the mse is achieved when the <strong>prediction</strong> <span class="math notranslate nohighlight">\(y_i = f_{w, b}(x_i)\)</span> is equal to the <strong>true value</strong> <span class="math notranslate nohighlight">\(t_i\)</span> for all training examples. In other words, we want to minimize the <strong>residual error</strong> of the model on the data. It is not always possible to obtain the global minimum (0) as the data may be noisy, but the closer, the better.</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="../_images/regression-animation-mse-dual.png"><img alt="../_images/regression-animation-mse-dual.png" src="../_images/regression-animation-mse-dual.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.25 </span><span class="caption-text">A good fit to the data is when the prediction <span class="math notranslate nohighlight">\(y_i\)</span> (on the line) is close to the data <span class="math notranslate nohighlight">\(t_i\)</span> for all training examples.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="least-mean-squares">
<h3><span class="section-number">2.1.1. </span>Least Mean Squares<a class="headerlink" href="#least-mean-squares" title="Permalink to this headline">¶</a></h3>
<p>We search for <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> which minimize the mean square error:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(w, b) = \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2
\]</div>
<p>We will apply <strong>gradient descent</strong> to iteratively modify estimates of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \Delta w = - \eta \, \frac{\partial \mathcal{L}(w, b)}{\partial w}
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta b = - \eta \, \frac{\partial \mathcal{L}(w, b)}{\partial b}
\]</div>
<p>Let’s search for the partial derivative of the mean square error with respect to <span class="math notranslate nohighlight">\(w\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{\partial}{\partial w} [\frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2]
\]</div>
<p>Partial derivatives are linear, so the derivative of a sum is the sum of the derivatives:</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{1}{N} \, \sum_{i=1}^{N} \frac{\partial}{\partial w} (t_i - y_i )^2
\]</div>
<p>This means we can compute a gradient for each training example instead of for the whole training set (see later the distinction batch/online):</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{1}{N} \, \sum_{i=1}^{N} \frac{\partial}{\partial w} \mathcal{l}_i(w, b)
    \qquad \text{with} \qquad \mathcal{l}_i(w, b) = (t_i - y_i )^2
\]</div>
<p>The individual loss <span class="math notranslate nohighlight">\(\mathcal{l}_i(w, b) = (t_i - y_i )^2\)</span> is the composition of two functions:</p>
<ul class="simple">
<li><p>a square error function <span class="math notranslate nohighlight">\(g_i(y_i) = (t_i - y_i)^2\)</span>.</p></li>
<li><p>the prediction <span class="math notranslate nohighlight">\(y_i = f_{w, b}(x_i) = w \, x_i + b\)</span>.</p></li>
</ul>
<p>The <strong>chain rule</strong> tells us how to derive such composite functions:</p>
<div class="math notranslate nohighlight">
\[
    \frac{ d f(g(x))}{dx} = \frac{ d f(g(x))}{d g(x)} \times \frac{ d g(x)}{dx} = \frac{ d f(y)}{dy} \times \frac{ d g(x)}{dx}
\]</div>
<p>The first derivative considers <span class="math notranslate nohighlight">\(g(x)\)</span> to be a single variable. Applied to our problem, this gives:</p>
<div class="math notranslate nohighlight">
\[
     \frac{\partial}{\partial w} \mathcal{l}_i(w, b) =  \frac{\partial g_i(y_i)}{\partial y_i} \times  \frac{\partial y_i}{\partial w}
\]</div>
<p>The square error function <span class="math notranslate nohighlight">\(g_i(y) = (t_i - y)^2\)</span> is easy to differentiate w.r.t <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial g_i(y_i)}{\partial y_i} = - 2 \, (t_i - y_i)
\]</div>
<p>The prediction <span class="math notranslate nohighlight">\(y_i = w \, x_i + b\)</span> also w.r.t <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[
   \frac{\partial  y_i}{\partial w} = x_i
\]</div>
<div class="math notranslate nohighlight">
\[
   \frac{\partial  y_i}{\partial b} = 1
\]</div>
<p>The partial derivative of the individual loss is:</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{l}_i(w, b)}{\partial w} = - 2 \, (t_i - y_i) \, x_i
\]</div>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{l}_i(w, b)}{\partial b} = - 2 \, (t_i - y_i)
\]</div>
<p>This gives us:</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = - \frac{2}{N} \sum_{i=1}^{N} (t_i - y_i) \, x_i
\]</div>
<div class="math notranslate nohighlight">
\[
    \frac{\partial \mathcal{L}(w, b)}{\partial b} = - \frac{2}{N} \sum_{i=1}^{N} (t_i - y_i)
\]</div>
<p>Gradient descent is then defined by the learning rules (absorbing the 2 in <span class="math notranslate nohighlight">\(\eta\)</span>):</p>
<div class="math notranslate nohighlight">
\[
    \Delta w = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i) \, x_i
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta b = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i)
\]</div>
<p><strong>Least Mean Squares</strong> (LMS) or Ordinary Least Squares (OLS) is a <strong>batch</strong> algorithm: the parameter changes are computed over the whole dataset.</p>
<div class="math notranslate nohighlight">
\[
    \Delta w = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i) \, x_i
\]</div>
<div class="math notranslate nohighlight">
\[
    \Delta b = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i)
\]</div>
<p>The parameter changes have to be applied multiple times (<strong>epochs</strong>) in order for the parameters to converge. One can stop when the parameters do not change much, or after a fixed number of epochs.</p>
<div class="admonition-lms-algorithm admonition">
<p class="admonition-title">LMS algorithm</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w=0 \quad;\quad b=0\)</span></p></li>
<li><p><strong>for</strong> M epochs:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(dw=0 \quad;\quad db=0\)</span></p></li>
<li><p><strong>for</strong> each sample <span class="math notranslate nohighlight">\((x_i, t_i)\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(y_i = w \, x_i + b\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(dw = dw + (t_i - y_i) \, x_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(db = db + (t_i - y_i)\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\Delta w = \eta \, \frac{1}{N} dw\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta b = \eta \, \frac{1}{N} db\)</span></p></li>
</ul>
</li>
</ul>
</div>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="../_images/regression-animation.gif"><img alt="../_images/regression-animation.gif" src="../_images/regression-animation.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.26 </span><span class="caption-text">Visualization of least mean squares applied to a simple regression problem with <span class="math notranslate nohighlight">\(\eta=0.1\)</span>. Each step of the animation corresponds to one epoch (iteration over the training set).</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>During learning, the <strong>mean square error</strong> (mse) decreases with the number of epochs but does not reach zero because of the noise in the data.</p>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="../_images/regression-animation-loss.png"><img alt="../_images/regression-animation-loss.png" src="../_images/regression-animation-loss.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.27 </span><span class="caption-text">Evolution of the loss function during training.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="delta-learning-rule">
<h3><span class="section-number">2.1.2. </span>Delta learning rule<a class="headerlink" href="#delta-learning-rule" title="Permalink to this headline">¶</a></h3>
<p>LMS is very slow, because it changes the weights only after the whole training set has been evaluated. It is also possible to update the weights immediately after each example using the <strong>delta learning rule</strong>, which is the <strong>online</strong> version of LMS:</p>
<div class="math notranslate nohighlight">
\[\Delta w = \eta \, (t_i - y_i) \, x_i\]</div>
<div class="math notranslate nohighlight">
\[\Delta b = \eta \, (t_i - y_i)\]</div>
<div class="admonition-delta-learning-rule admonition">
<p class="admonition-title">Delta learning rule</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w=0 \quad;\quad b=0\)</span></p></li>
<li><p><strong>for</strong> M epochs:</p>
<ul>
<li><p><strong>for</strong> each sample <span class="math notranslate nohighlight">\((x_i, t_i)\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(y_i = w \, x_i + b\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta w = \eta \, (t_i - y_i ) \, x_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta b = \eta \, (t_i - y_i)\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<p>The batch version is more stable, but the online version is faster: the weights have already learned something when arriving at the end of the first epoch. Note that the loss function is slightly higher at the end of learning (see Exercise 3 for a deeper discussion).</p>
<figure class="align-default" id="id7">
<a class="reference internal image-reference" href="../_images/regression-animation-online.gif"><img alt="../_images/regression-animation-online.gif" src="../_images/regression-animation-online.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.28 </span><span class="caption-text">Visualization of the delta learning rule applied to a simple regression problem with <span class="math notranslate nohighlight">\(\eta = 0.1\)</span>. Each step of the animation corresponds to one epoch (iteration over the training set).</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id8">
<a class="reference internal image-reference" href="../_images/regression-animation-online-loss.png"><img alt="../_images/regression-animation-online-loss.png" src="../_images/regression-animation-online-loss.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.29 </span><span class="caption-text">Evolution of the loss function during training. With the same learning rate, the delta learning rule converges much faster but reaches a poorer minimum. Lowering the learning rate slows down learning but reaches a better minimum.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="multiple-linear-regression">
<h2><span class="section-number">2.2. </span>Multiple linear regression<a class="headerlink" href="#multiple-linear-regression" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/BOJFFy0nA6I' frameborder='0' allowfullscreen></iframe></div>
<p>The key idea of linear regression (one input <span class="math notranslate nohighlight">\(x\)</span>, one output <span class="math notranslate nohighlight">\(y\)</span>) can be generalized to multiple inputs and outputs.</p>
<p><strong>Multiple Linear Regression</strong> (MLR) predicts several output variables based on several explanatory variables:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
y_1 = w_1 \, x_1 + w_2 \, x_2 + b_1\\
\\
y_2 = w_3 \, x_1 + w_3 \, x_2 + b_2\\
\end{cases}
\end{split}\]</div>
<div class="admonition-example-fuel-consumption-and-co2-emissions admonition">
<p class="admonition-title">Example: fuel consumption and CO2 emissions</p>
<p>Let’s suppose you have 13971 measurements in some Excel file, linking engine size, number of cylinders, fuel consumption and CO2 emissions of various cars. You want to predict fuel consumption and CO2 emissions when you know the engine size and the number of cylinders.</p>
<table class="table" id="example-table">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Engine size</p></th>
<th class="head"><p>Cylinders</p></th>
<th class="head"><p>Fuel consumption</p></th>
<th class="head"><p>CO2 emissions</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2</p></td>
<td><p>4</p></td>
<td><p>8.5</p></td>
<td><p>196</p></td>
</tr>
<tr class="row-odd"><td><p>2.4</p></td>
<td><p>4</p></td>
<td><p>9.6</p></td>
<td><p>221</p></td>
</tr>
<tr class="row-even"><td><p>1.5</p></td>
<td><p>4</p></td>
<td><p>5.9</p></td>
<td><p>136</p></td>
</tr>
<tr class="row-odd"><td><p>3.5</p></td>
<td><p>6</p></td>
<td><p>11</p></td>
<td><p>255</p></td>
</tr>
<tr class="row-even"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
</tbody>
</table>
<figure class="align-default" id="id9">
<a class="reference internal image-reference" href="../_images/MLR-example-data.png"><img alt="../_images/MLR-example-data.png" src="../_images/MLR-example-data.png" style="width: 90%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.30 </span><span class="caption-text">CO2 emissions and fuel consumption depend almost linearly on the engine size and number of cylinders.</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id10">
<a class="reference internal image-reference" href="../_images/MLR-example-data-3d.png"><img alt="../_images/MLR-example-data-3d.png" src="../_images/MLR-example-data-3d.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.31 </span><span class="caption-text">CO2 emissions and fuel consumption depend almost linearly on the engine size and number of cylinders.</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We can notice that the output variables seem to linearly depend on the inputs. Noting the input variables <span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span> and the output ones <span class="math notranslate nohighlight">\(y_1\)</span>, <span class="math notranslate nohighlight">\(y_2\)</span>, we can define our problem as a multiple linear regression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
y_1 = w_1 \, x_1 + w_2 \, x_2 + b_1\\
\\
y_2 = w_3 \, x_1 + w_3 \, x_2 + b_2\\
\end{cases}
\end{split}\]</div>
<p>and solve it using the least mean squares method by minimizing the mse between the model and the data.</p>
<figure class="align-default" id="id11">
<a class="reference internal image-reference" href="../_images/MLR-example-fit-3d.png"><img alt="../_images/MLR-example-fit-3d.png" src="../_images/MLR-example-fit-3d.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.32 </span><span class="caption-text">The result of MLR is a plane in the input space.</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using the Python library <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> (<a class="reference external" href="https://scikit-learn.org">https://scikit-learn.org</a>), this is done in two lines of code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The system of equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
y_1 = w_1 \, x_1 + w_2 \, x_2 + b_1\\
\\
y_2 = w_3 \, x_1 + w_4 \, x_2 + b_2\\
\end{cases}
\end{split}\]</div>
<p>can be put in a matrix-vector form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{bmatrix} y_1 \\ y_2 \\\end{bmatrix} = \begin{bmatrix} w_1 &amp; w_2 \\ w_3 &amp; w_4 \\\end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\\end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\\end{bmatrix}
\end{split}\]</div>
<p>We simply create the corresponding vectors and matrices:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\\end{bmatrix} \qquad \mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\\end{bmatrix} \qquad \mathbf{t} = \begin{bmatrix} t_1 \\ t_2 \\\end{bmatrix} \qquad \mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \\\end{bmatrix} \qquad W = \begin{bmatrix} w_1 &amp; w_2 \\ w_3 &amp; w_4 \\\end{bmatrix}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is the input vector, <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is the output vector, <span class="math notranslate nohighlight">\(\mathbf{t}\)</span> is the target vector. <span class="math notranslate nohighlight">\(W\)</span> is called the <strong>weight matrix</strong> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> the <strong>bias vector</strong>.</p>
<p>The model is now defined by:</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{y} = f_{W, \mathbf{b}}(\mathbf{x}) = W \times \mathbf{x} + \mathbf{b}
\]</div>
<p>The problem is exactly the same as before, except that we use vectors and matrices instead of scalars: <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> can have any number of dimensions, the same procedure will apply. This corresponds to a <strong>linear neural network</strong> (or linear perceptron), with one <strong>output neuron</strong> per predicted value <span class="math notranslate nohighlight">\(y_i\)</span> using the linear activation function.</p>
<figure class="align-default" id="id12">
<a class="reference internal image-reference" href="../_images/linearperceptron.svg"><img alt="../_images/linearperceptron.svg" src="../_images/linearperceptron.svg" width="60%" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.33 </span><span class="caption-text">A linear perceptron is a single layer of artificial neurons. The output vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is compared to the ground truth vector <span class="math notranslate nohighlight">\(\mathbf{t}\)</span> using the mse loss.</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The mean square error still needs to be a scalar in order to be minimized. We can define it as the squared norm of the error <strong>vector</strong>:</p>
<div class="math notranslate nohighlight">
\[
    \min_{W, \mathbf{b}} \, \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_\mathcal{D} [ ||\mathbf{t} - \mathbf{y}||^2 ] = \mathbb{E}_\mathcal{D} [ ((t_1 - y_1)^2 + (t_2 - y_2)^2) ]
\]</div>
<p>In order to apply gradient descent, one needs to calculate partial derivatives w.r.t the weight matrix <span class="math notranslate nohighlight">\(W\)</span> and the bias vector <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>, i.e. <strong>gradients</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{cases}
    \Delta W = - \eta \, \nabla_W \, \mathcal{L}(W, \mathbf{b}) \\
    \\
    \Delta \mathbf{b} = - \eta \, \nabla_\mathbf{b}  \mathcal{L}(W, \mathbf{b}) \\
    \end{cases}
\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some more advanced linear algebra becomes important to know how to compute these gradients:</p>
<p><a class="reference external" href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf">https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf</a></p>
</div>
<p>We search the minimum of the mse loss function:</p>
<div class="math notranslate nohighlight">
\[
    \min_{W, \mathbf{b}} \, \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_\mathcal{D} [ ||\mathbf{t} - \mathbf{y}||^2 ] \approx \frac{1}{N} \, \sum_{i=1}^N ||\mathbf{t}_i - \mathbf{y}_i||^2 = \frac{1}{N} \, \sum_{i=1}^N \mathcal{l}_i(W, \mathbf{b})
\]</div>
<p>The individual loss function <span class="math notranslate nohighlight">\(\mathcal{l}_i(W, \mathbf{b})\)</span> is the squared <span class="math notranslate nohighlight">\(\mathcal{L}^2\)</span>-norm of the error vector, what can be expressed as a dot product or a vector multiplication:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{l}_i(W, \mathbf{b}) = ||\mathbf{t}_i - \mathbf{y}_i||^2 = \langle \mathbf{t}_i - \mathbf{y}_i \cdot \mathbf{t}_i - \mathbf{y}_i \rangle = (\mathbf{t}_i - \mathbf{y}_i)^T \times (\mathbf{t}_i - \mathbf{y}_i)
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x}^T \times \mathbf{x} = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = x_1 \, x_1 + x_2 \, x_2 + \ldots + x_n \, x_n = \langle \mathbf{x} \cdot \mathbf{x} \rangle = ||\mathbf{x}||^2_2\end{split}\]</div>
</div>
<p>The chain rule tells us in principle that:</p>
<div class="math notranslate nohighlight">
\[\nabla_{W} \, \mathcal{l}_i(W, \mathbf{b}) = \nabla_{\mathbf{y}_i} \, \mathcal{l}_i(W, \mathbf{b}) \times \nabla_{W} \, \mathbf{y}_i\]</div>
<p>The gradient w.r.t the output vector <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> is quite easy to obtain, as it a quadratic function of <span class="math notranslate nohighlight">\(\mathbf{t}_i - \mathbf{y}_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\mathbf{y}_i} \, \mathcal{l}_i(W, \mathbf{b}) = \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i)^T \times (\mathbf{t}_i - \mathbf{y}_i)\]</div>
<p>The proof relies on product differentiation <span class="math notranslate nohighlight">\((f\times g)' = f' \, g + f \, g'\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i)^T \times (\mathbf{t}_i - \mathbf{y}_i) &amp; = ( \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i) ) \times (\mathbf{t}_i - \mathbf{y}_i) + (\mathbf{t}_i - \mathbf{y}_i) \times \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i)  \\
    &amp;\\
    &amp;= - (\mathbf{t}_i - \mathbf{y}_i) - (\mathbf{t}_i - \mathbf{y}_i) \\
    &amp;\\
    &amp;= - 2 \, (\mathbf{t}_i - \mathbf{y}_i) \\
\end{aligned}
\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We use the properties <span class="math notranslate nohighlight">\(\nabla_{\mathbf{x}}\,  \mathbf{x}^T \times \mathbf{z} = \mathbf{z}\)</span> and <span class="math notranslate nohighlight">\(\nabla_{\mathbf{z}} \, \mathbf{x}^T \times \mathbf{z} = \mathbf{x}\)</span> to get rid of the transpose.</p>
</div>
<p>The “problem” is when computing <span class="math notranslate nohighlight">\(\nabla_{W} \, \mathbf{y}_i = \nabla_{W} \, (W \times \mathbf{x}_i + \mathbf{b})\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> is a vector and <span class="math notranslate nohighlight">\(W\)</span> a matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_{W} \, \mathbf{y}_i\)</span> is then a Jacobian (matrix), not a gradient (vector).</p></li>
</ul>
<p>Intuitively, differentiating <span class="math notranslate nohighlight">\(W \times \mathbf{x}_i + \mathbf{b}\)</span> w.r.t <span class="math notranslate nohighlight">\(W\)</span> should return <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>, but it is a vector, not a matrix…</p>
<p>Actually, only the gradient (or Jacobian) of <span class="math notranslate nohighlight">\(\mathcal{l}_i(W, \mathbf{b})\)</span> w.r.t <span class="math notranslate nohighlight">\(W\)</span> should be a matrix of the same size as <span class="math notranslate nohighlight">\(W\)</span> so that we can apply gradient descent:</p>
<div class="math notranslate nohighlight">
\[\Delta W = - \eta \, \nabla_W \, \mathcal{L}(W, \mathbf{b})\]</div>
<p>We already know that:</p>
<div class="math notranslate nohighlight">
\[\nabla_{W} \, \mathcal{l}_i(W, \mathbf{b}) = - 2\, (\mathbf{t}_i - \mathbf{y}_i) \times \nabla_{W} \, \mathbf{y}_i\]</div>
<p>If <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> has <span class="math notranslate nohighlight">\(n\)</span> elements and <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> <span class="math notranslate nohighlight">\(m\)</span> elements, <span class="math notranslate nohighlight">\(W\)</span> is a <span class="math notranslate nohighlight">\(m \times n\)</span> matrix.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember the outer product between two vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{u} \times \mathbf{v}^\textsf{T} =
  \begin{bmatrix}u_1 \\ u_2 \\ u_3 \\ u_4\end{bmatrix}
    \begin{bmatrix}v_1 &amp; v_2 &amp; v_3\end{bmatrix} =
  \begin{bmatrix}
    u_1v_1 &amp; u_1v_2 &amp; u_1v_3 \\
    u_2v_1 &amp; u_2v_2 &amp; u_2v_3 \\
    u_3v_1 &amp; u_3v_2 &amp; u_3v_3 \\
    u_4v_1 &amp; u_4v_2 &amp; u_4v_3
  \end{bmatrix}.
\end{split}\]</div>
</div>
<p>It is easy to see that the outer product between <span class="math notranslate nohighlight">\((\mathbf{t}_i - \mathbf{y}_i)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> gives a <span class="math notranslate nohighlight">\(m \times n\)</span> matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \nabla_W \, \mathcal{l}_i(W, \mathbf{b}) = - 2 \, (\mathbf{t}_i - \mathbf{y}_i) \times \mathbf{x}_i^T\\
\end{split}\]</div>
<p>Let’s prove it element per element on a small matrix:</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{y} = W \times \mathbf{x} + \mathbf{b}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{bmatrix} y_1 \\ y_2 \\\end{bmatrix} = \begin{bmatrix} w_1 &amp; w_2 \\ w_3 &amp; w_4 \\\end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\\end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\\end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{l}(W, \mathbf{b}) = (\mathbf{t} - \mathbf{y})^T \times (\mathbf{t} - \mathbf{y}) = \begin{bmatrix} t_1 - y_1 &amp; t_2 - y_2 \\\end{bmatrix} \times \begin{bmatrix} t_1 - y_1 \\ t_2 - y_2 \\\end{bmatrix} = (t_1 - y_1)^2 + (t_2 - y_2)^2
\end{split}\]</div>
<p>The Jacobian w.r.t <span class="math notranslate nohighlight">\(W\)</span> can be explicitly formed using partial derivatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla_W \, \mathcal{l}(W, \mathbf{b}) = \begin{bmatrix} 
\dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_1} &amp; \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_2} \\ \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_3} &amp; \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_4} \\
\end{bmatrix}
= \begin{bmatrix} 
-2 \, (t_1 - y_1) \, x_1 &amp; -2 \, (t_1 - y_1) \, x_2 \\ -2 \, (t_2 - y_2) \, x_1 &amp; -2 \, (t_2 - y_2) \, x_2 \\
\end{bmatrix}
\end{split}\]</div>
<p>We can rearrange this matrix as an outer product:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla_W \, \mathcal{l}(W, \mathbf{b}) = -2 \, \begin{bmatrix} 
t_1 - y_1  \\  t_2 - y_2 \\
\end{bmatrix} \times \begin{bmatrix} 
x_1 &amp; x_2 \\
\end{bmatrix}
= - 2 \, (\mathbf{t} - \mathbf{y}) \times \mathbf{x}^T
\end{split}\]</div>
<p><strong>Multiple linear regression</strong></p>
<ul class="simple">
<li><p>Batch version:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}
    \Delta W = \eta \, \dfrac{1}{N} \sum_{i=1}^N \, (\mathbf{t}_i - \mathbf{y}_i ) \times \mathbf{x}_i^T \\
    \\
    \Delta \mathbf{b} = \eta \, \dfrac{1}{N} \sum_{i=1}^N \, (\mathbf{t}_i - \mathbf{y}_i) \\
\end{cases}\end{split}\]</div>
<ul class="simple">
<li><p>Online version (<strong>delta learning rule</strong>):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}
    \Delta W = \eta \, (\mathbf{t}_i - \mathbf{y}_i ) \times \mathbf{x}_i^T \\
    \\
    \Delta \mathbf{b} = \eta \, (\mathbf{t}_i - \mathbf{y}_i) \\
\end{cases}\end{split}\]</div>
<p>The matrix-vector notation is completely equivalent to having one learning rule per parameter:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    \Delta w_1 = \eta \, (t_1 - y_1) \, x_1 \\
    \Delta w_2 = \eta \, (t_1 - y_1) \, x_2 \\
    \Delta w_3 = \eta \, (t_2 - y_2) \, x_1 \\
    \Delta w_4 = \eta \, (t_2 - y_2) \, x_2 \\
\end{cases}
\qquad
\begin{cases}
    \Delta b_1 = \eta \, (t_1 - y_1) \\
    \Delta b_2 = \eta \, (t_2 - y_2) \\
\end{cases}
\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The delta learning rule is always of the form: <span class="math notranslate nohighlight">\(\Delta w\)</span> = eta * error * input. Biases have an input of 1.</p>
</div>
</section>
<section id="logistic-regression">
<h2><span class="section-number">2.3. </span>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/fRuzoyV036Y' frameborder='0' allowfullscreen></iframe></div>
<p>Let’s suppose we want to perform a regression, but where the outputs <span class="math notranslate nohighlight">\(t_i\)</span> are bounded between 0 and 1. We could use a logistic (or sigmoid) function instead of a linear function in order to transform the input into an output:</p>
<div class="math notranslate nohighlight">
\[
    y = \sigma(w \, x + b )  = \displaystyle\frac{1}{1+\exp(-w \, x - b )}
\]</div>
<figure class="align-default" id="id13">
<a class="reference internal image-reference" href="../_images/sigmoid.png"><img alt="../_images/sigmoid.png" src="../_images/sigmoid.png" style="width: 60%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.34 </span><span class="caption-text">Logistic or sigmoid function <span class="math notranslate nohighlight">\(\sigma(x)=\displaystyle\frac{1}{1+\exp(-x)}\)</span>.</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>By definition of the logistic function, the prediction <span class="math notranslate nohighlight">\(y\)</span> will be bounded between 0 and 1, what matches the targets <span class="math notranslate nohighlight">\(t\)</span>. Let’s now apply gradient descent on the <strong>mse</strong> loss using this new model. The individual loss will be:</p>
<div class="math notranslate nohighlight">
\[l_i(w, b) = (t_i - \sigma(w \, x_i + b) )^2 \]</div>
<p>The partial derivative of the individual loss is easy to find using the chain rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \displaystyle\frac{\partial l_i(w, b)}{\partial w}
        &amp;= 2 \, (t_i - y_i)  \, \frac{\partial}{\partial w}  (t_i - \sigma(w \, x_i + b ))\\
        &amp;\\
        &amp;= - 2 \, (t_i - y_i) \, \sigma'(w \, x_i + b ) \,  x_i \\
\end{aligned}
\end{split}\]</div>
<p>The non-linear transfer function <span class="math notranslate nohighlight">\(\sigma(x)\)</span> therefore adds its derivative into the gradient:</p>
<div class="math notranslate nohighlight">
\[
    \Delta w = \eta \, (t_i - y_i) \, \sigma'(w \, x_i + b ) \, x_i
\]</div>
<p>The logistic function <span class="math notranslate nohighlight">\(\sigma(x)=\frac{1}{1+\exp(-x)}\)</span> has the nice property that its derivative can be expressed easily:</p>
<div class="math notranslate nohighlight">
\[
    \sigma'(x) = \sigma(x) \, (1 - \sigma(x) )
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Here is the proof using the fact that the derivative of <span class="math notranslate nohighlight">\(\displaystyle\frac{1}{f(x)}\)</span> is <span class="math notranslate nohighlight">\(\displaystyle\frac{- f'(x)}{f^2(x)}\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \sigma'(x) &amp; = \displaystyle\frac{-1}{(1+\exp(-x))^2} \, (- \exp(-x)) \\
    &amp;\\
    &amp;= \frac{1}{1+\exp(-x)} \times \frac{\exp(-x)}{1+\exp(-x)}\\
    &amp;\\
    &amp;= \frac{1}{1+\exp(-x)} \times \frac{1 + \exp(-x) - 1}{1+\exp(-x)}\\
    &amp;\\
    &amp;= \frac{1}{1+\exp(-x)} \times (1 - \frac{1}{1+\exp(-x)})\\
    &amp;\\
    &amp;= \sigma(x) \, (1 - \sigma(x) )\\
\end{aligned}
\end{split}\]</div>
</div>
<p>The delta learning rule for the logistic regression model is therefore easy to obtain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    \Delta w = \eta \, (t_i - y_i) \, y_i \, ( 1 - y_i ) \, x_i \\
\\
    \Delta b = \eta \, (t_i - y_i) \, y_i \, ( 1 - y_i ) \\
\end{cases}
\end{split}\]</div>
<p><strong>Generalized form of the delta learning rule</strong></p>
<figure class="align-default" id="id14">
<a class="reference internal image-reference" href="../_images/artificialneuron.svg"><img alt="../_images/artificialneuron.svg" src="../_images/artificialneuron.svg" width="60%" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.35 </span><span class="caption-text">Artificial neuron with multiple inputs.</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>For a linear perceptron with parameters <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> and any activation function <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{y} = f(W \times \mathbf{x} + \mathbf{b} )  
\]</div>
<p>and the <strong>mse</strong> loss function:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_{\mathcal{D}}[||\mathbf{t} - \mathbf{y}||^2]
\]</div>
<p>the <strong>delta learning rule</strong> has the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    \Delta W = \eta \, (\mathbf{t} - \mathbf{y}) \times f'(W \times \mathbf{x} + \mathbf{b}) \times \mathbf{x}^T \\
\\
    \Delta \mathbf{b} = \eta \, (\mathbf{t} - \mathbf{y}) \times f'(W \times \mathbf{x} + \mathbf{b}) \\
\end{cases}
\end{split}\]</div>
<p>In the linear case, <span class="math notranslate nohighlight">\(f'(x) = 1\)</span>. One can use any non-linear function, e.g hyperbolic tangent tanh(), ReLU, etc. Transfer functions are chosen for neural networks so that we can compute their derivative easily.</p>
</section>
<section id="polynomial-regression">
<h2><span class="section-number">2.4. </span>Polynomial regression<a class="headerlink" href="#polynomial-regression" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/a6sQgJovhzU' frameborder='0' allowfullscreen></iframe></div>
<figure class="align-default" id="id15">
<a class="reference internal image-reference" href="../_images/polynomialregression.png"><img alt="../_images/polynomialregression.png" src="../_images/polynomialregression.png" style="width: 60%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.36 </span><span class="caption-text">Polynomial regression.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The functions underlying real data are rarely linear plus some noise around the ideal value. In the figure above, the input/output function is better modeled by a second-order polynomial:</p>
<div class="math notranslate nohighlight">
\[y = f_{\mathbf{w}, b}(x) = w_1 \, x + w_2 \, x^2 +b\]</div>
<p>We can transform the input into a vector of coordinates:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x} = \begin{bmatrix} x \\ x^2 \\ \end{bmatrix} \qquad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \end{bmatrix}\end{split}\]</div>
<p>The problem becomes:</p>
<div class="math notranslate nohighlight">
\[y = \langle \mathbf{w} . \mathbf{x} \rangle + b = \sum_j w_j \, x_j + b\]</div>
<p>We can simply apply multiple linear regression (MLR) to find <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and b:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}
\Delta \mathbf{w} =  \eta \, (t - y) \, \mathbf{x}\\
\\
\Delta b =  \eta \, (t - y)\\
\end{cases}\end{split}\]</div>
<p>This generalizes to polynomials of any order <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="math notranslate nohighlight">
\[y = f_{\mathbf{w}, b}(x) = w_1 \, x + w_2 \, x^2 + \ldots + w_p \, x^p + b\]</div>
<p>We create a vector of powers of <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x} = \begin{bmatrix} x \\ x^2 \\ \ldots \\ x^p \end{bmatrix} \qquad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \ldots \\ w_p \end{bmatrix}\end{split}\]</div>
<p>ad apply multiple linear regression (MLR) to find <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and b:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}
\Delta \mathbf{w} =  \eta \, (t - y) \, \mathbf{x}\\
\\
\Delta b =  \eta \, (t - y)\\
\end{cases}\end{split}\]</div>
<p>Non-linear problem solved! The only unknown is which order for the polynomial matches best the data. One can perform regression with any kind of parameterized function using gradient descent.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./2-linear"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="1-Optimization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">1. </span>Optimization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3-Regularization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Regularization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>