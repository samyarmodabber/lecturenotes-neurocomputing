
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>8. Generative adversarial networks &#8212; Neurocomputing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/8-GAN.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. Recurrent neural networks" href="9-RNN.html" />
    <link rel="prev" title="7. Restricted Boltzmann machines (optional)" href="7-RBM.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tuc.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neurocomputing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-Attention.html">
   10. Attentional neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3-deeplearning/8-GAN.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   8.1. Generative adversarial networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generative-models">
     8.1.1. Generative models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#architecture-of-a-gan">
     8.1.2. Architecture of a GAN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gan-loss">
     8.1.3. GAN loss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variants">
     8.1.4. Variants
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditional-gans">
   8.2. Conditional GANs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cgan">
     8.2.1. cGAN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pix2pix">
     8.2.2. pix2pix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cyclegan-neural-style-transfer">
     8.2.3. CycleGAN : Neural Style Transfer
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="generative-adversarial-networks">
<h1><span class="section-number">8. </span>Generative adversarial networks<a class="headerlink" href="#generative-adversarial-networks" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/3.8-GAN.pdf">pdf</a></p>
<section id="id1">
<h2><span class="section-number">8.1. </span>Generative adversarial networks<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/CWgvnu8Qtug' frameborder='0' allowfullscreen></iframe></div>
<section id="generative-models">
<h3><span class="section-number">8.1.1. </span>Generative models<a class="headerlink" href="#generative-models" title="Permalink to this headline">¶</a></h3>
<p>An autoencoder learns to first encode inputs in a <strong>latent space</strong> and then use a generative model to model the data distribution.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_\text{autoencoder}(\theta, \phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} [ - \log p_\theta(\mathbf{z})]\]</div>
<p>Couldn’t we learn a decoder using random noise as input but still learning the distribution of the data?</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_\text{GAN}(\theta, \phi) = \mathbb{E}_{\mathbf{z} \sim \mathcal{N}(0, 1)} [ - \log p_\theta(\mathbf{z}) ]\]</div>
<p>After all, this is how random numbers are generated: a uniform distribution of pseudo-random numbers is transformed into samples of another distribution using a mathematical formula.</p>
<figure class="align-default" id="id18">
<a class="reference internal image-reference" href="../_images/generation-distribution.jpeg"><img alt="../_images/generation-distribution.jpeg" src="../_images/generation-distribution.jpeg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">Random numbers are generated using a standard distribution as source of randomness. Source: <a class="reference external" href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29</a></span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The problem is how to estimate the discrepancy between the true distribution and the generated distribution when we only have samples. The Maximum Mean Discrepancy (MMD) approach allows to do that, but does not work very well in highly-dimensional spaces.</p>
<figure class="align-default" id="id19">
<a class="reference internal image-reference" href="../_images/gan-principle2.png"><img alt="../_images/gan-principle2.png" src="../_images/gan-principle2.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text">The generative sample should learn to minimize the statistical distance between the true distribution and the parameterized distribution using samples. Source: <a class="reference external" href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29</a></span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="architecture-of-a-gan">
<h3><span class="section-number">8.1.2. </span>Architecture of a GAN<a class="headerlink" href="#architecture-of-a-gan" title="Permalink to this headline">¶</a></h3>
<p>The <strong>Generative Adversarial Network</strong> (GAN, <span id="id2">[<a class="reference internal" href="../zreferences.html#id57">Goodfellow et al., 2014</a>]</span>) is a smart way of providing a loss function to the generative model. It is composed of two parts:</p>
<ul class="simple">
<li><p>The <strong>Generator</strong> (or decoder) produces an image based on latent variables sampled from some random distribution (e.g. uniform or normal).</p></li>
<li><p>The <strong>Discriminator</strong> has to recognize real images from generated ones.</p></li>
</ul>
<figure class="align-default" id="id20">
<a class="reference internal image-reference" href="../_images/gan-simple2.png"><img alt="../_images/gan-simple2.png" src="../_images/gan-simple2.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">Architecture of a GAN. The generator only sees noisy latent representations and outputs a reconstruction. The discriminator gets alternatively real or generated inputs and predicts whether it is real or fake. Source: <a class="reference external" href="https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml">https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml</a></span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The generator and the discriminator are in competition with each other. The discriminator uses pure <strong>supervised learning</strong>: we know if the input is real or generated (binary classification) and train the discriminator accordingly. The generator tries to fool the discriminator, without ever seeing the data!</p>
<figure class="align-default" id="id21">
<a class="reference internal image-reference" href="../_images/gan-principle.png"><img alt="../_images/gan-principle.png" src="../_images/gan-principle.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.4 </span><span class="caption-text">Principle of a GAN. Source: <a class="reference external" href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29</a></span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="gan-loss">
<h3><span class="section-number">8.1.3. </span>GAN loss<a class="headerlink" href="#gan-loss" title="Permalink to this headline">¶</a></h3>
<p>Let’s define <span class="math notranslate nohighlight">\(x \sim P_\text{data}(x)\)</span> as a real image from the dataset and <span class="math notranslate nohighlight">\(G(z)\)</span> as an image generated by the generator,  where <span class="math notranslate nohighlight">\(z \sim P_z(z)\)</span> is a random input. The output of the discriminator is a single sigmoid neuron:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D(x) = 1\)</span> for real images.</p></li>
<li><p><span class="math notranslate nohighlight">\(D(G(z)) = 0\)</span> for generated images</p></li>
</ul>
<p>The discriminator wants both <span class="math notranslate nohighlight">\(D(x)\)</span> and <span class="math notranslate nohighlight">\(1-D(G(z))\)</span> to be close from 1, so the goal of the discriminator is to <strong>minimize</strong> the negative log-likelihood (cross-entropy) of classifying correctly the data:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(D) = \mathbb{E}_{x \sim P_\text{data}(x)} [ - \log D(x)] + \mathbb{E}_{z \sim P_z(z)} [ - \log(1 - D(G(z)))]
\]</div>
<p>It is similar to logistic regression: <span class="math notranslate nohighlight">\(x\)</span> belongs to the positive class, <span class="math notranslate nohighlight">\(G(z)\)</span> to the negative class.</p>
<p>The goal of the generator is to <strong>maximize</strong> the negative log-likelihood of the discriminator being correct on the generated images, i.e. fool it:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{J}(G) = \mathbb{E}_{z \sim P_z(z)} [ - \log(1 - D(G(z)))]
\]</div>
<p>The generator tries to maximize what the discriminator tries to minimize.</p>
<p>Putting both objectives together, we obtain the following <strong>minimax</strong> problem:</p>
<div class="math notranslate nohighlight">
\[
    \min_G \max_D \, \mathcal{V}(D, G) = \mathbb{E}_{x \sim P_\text{data}(x)} [\log D(x)] + \mathbb{E}_{z \sim P_z(z)} [\log(1 - D(G(z)))]
\]</div>
<p><span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(G\)</span> compete on the same objective function: one tries to maximize it, the other to minimize it. Note that the generator <span class="math notranslate nohighlight">\(G\)</span> never sees the data <span class="math notranslate nohighlight">\(x\)</span>: all it gets is a <strong>backpropagated gradient</strong> through the discriminator:</p>
<div class="math notranslate nohighlight">
\[\nabla_{G(z)} \, \mathcal{V}(D, G) = \nabla_{D(G(z))} \, \mathcal{V}(D, G) \times \nabla_{G(z)} \, D(G(z))\]</div>
<p>It informs the generator which <strong>pixels</strong> are the most responsible for an eventual bad decision of the discriminator.</p>
<p>This objective function can be optimized when the generator uses gradient descent and the discriminator gradient ascent: just apply a minus sign on the weight updates!</p>
<div class="math notranslate nohighlight">
\[
    \min_G \max_D V(D, G) = \mathbb{E}_{x \sim P_\text{data}(x)} [\log D(x)] + \mathbb{E}_{z \sim P_z(z)} [\log(1 - D(G(z)))]
\]</div>
<p>Both can therefore use the usual <strong>backpropagation</strong> algorithm to adapt their parameters. The discriminator and the generator should reach a <strong>Nash equilibrium</strong>: they try to beat each other, but both become better over time.</p>
<figure class="align-default" id="id22">
<a class="reference internal image-reference" href="../_images/gan-loss.png"><img alt="../_images/gan-loss.png" src="../_images/gan-loss.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.5 </span><span class="caption-text">The generator and discriminator loss functions reach an equilibrium, it is quite hard to tell when the network has converged. Source: Research project - Vivek Bakul Maru - TU Chemnitz</span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="variants">
<h3><span class="section-number">8.1.4. </span>Variants<a class="headerlink" href="#variants" title="Permalink to this headline">¶</a></h3>
<p>DCGAN <span id="id3">[<a class="reference internal" href="../zreferences.html#id151">Radford et al., 2015</a>]</span> is the convolutional version of GAN, using transposed convolutions in the generator and concolutions with stride in the discriminator.</p>
<figure class="align-default" id="id23">
<a class="reference internal image-reference" href="../_images/dcgan-flat.png"><img alt="../_images/dcgan-flat.png" src="../_images/dcgan-flat.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.6 </span><span class="caption-text">DCGAN <span id="id4">[<a class="reference internal" href="../zreferences.html#id151">Radford et al., 2015</a>]</span>.</span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id24">
<a class="reference internal image-reference" href="../_images/dcgan.png"><img alt="../_images/dcgan.png" src="../_images/dcgan.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.7 </span><span class="caption-text">Results of DCGAN <span id="id5">[<a class="reference internal" href="../zreferences.html#id151">Radford et al., 2015</a>]</span>.</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>GAN are quite sensible to train: the discriminator should not become too good too early, otherwise there is no usable gradient for the generator. In practice, one updates the generator more often than the discriminator. There has been many improvements on GANs to stabilizes training (see <span id="id6">[<a class="reference internal" href="../zreferences.html#id170">Salimans et al., 2016</a>]</span>):</p>
<ul class="simple">
<li><p>Wasserstein GAN (relying on the Wasserstein distance instead of the log-likelihood) <span id="id7">[<a class="reference internal" href="../zreferences.html#id5">Arjovsky et al., 2017</a>]</span>.</p></li>
<li><p>f-GAN (relying on any f-divergence) <span id="id8">[<a class="reference internal" href="../zreferences.html#id143">Nowozin et al., 2016</a>]</span>.</p></li>
</ul>
<p>But the generator often <strong>collapses</strong>, i.e. outputs non-sense, or always the same image. Hyperparameter tuning is very difficult.</p>
<p>StyleGAN2 from NVIDIA <span id="id9">[<a class="reference internal" href="../zreferences.html#id101">Karras et al., 2020</a>]</span> is one of the most realistic GAN variant. Check its generated faces at <a class="reference external" href="https://thispersondoesnotexist.com/">https://thispersondoesnotexist.com/</a>.</p>
</section>
</section>
<section id="conditional-gans">
<h2><span class="section-number">8.2. </span>Conditional GANs<a class="headerlink" href="#conditional-gans" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/WxVKJfwPjw4' frameborder='0' allowfullscreen></iframe></div>
<section id="cgan">
<h3><span class="section-number">8.2.1. </span>cGAN<a class="headerlink" href="#cgan" title="Permalink to this headline">¶</a></h3>
<p>The generator can also get additional <strong>deterministic</strong> information to the latent space, not only the random vector <span class="math notranslate nohighlight">\(z\)</span>. One can for example provide the <strong>label</strong> (class) in the context of supervised learning, allowing to generate many <strong>new</strong> examples of each class: data augmentation using a conditional GAN <span id="id10">[<a class="reference internal" href="../zreferences.html#id137">Mirza &amp; Osindero, 2014</a>]</span>. One could also provide the output of a pre-trained CNN (ResNet) to condition on images.</p>
<figure class="align-default" id="id25">
<a class="reference internal image-reference" href="../_images/cgan.png"><img alt="../_images/cgan.png" src="../_images/cgan.png" style="width: 60%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.8 </span><span class="caption-text">cGAN <span id="id11">[<a class="reference internal" href="../zreferences.html#id137">Mirza &amp; Osindero, 2014</a>]</span>.</span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id26">
<a class="reference internal image-reference" href="../_images/dcgan_network.jpg"><img alt="../_images/dcgan_network.jpg" src="../_images/dcgan_network.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.9 </span><span class="caption-text">cGAN conditioned on text <span id="id12">[<a class="reference internal" href="../zreferences.html#id162">Reed et al., 2016</a>]</span>.</span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id27">
<a class="reference internal image-reference" href="../_images/dcgan-textimage.jpg"><img alt="../_images/dcgan-textimage.jpg" src="../_images/dcgan-textimage.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.10 </span><span class="caption-text">cGAN conditioned on text <span id="id13">[<a class="reference internal" href="../zreferences.html#id162">Reed et al., 2016</a>]</span>.</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="pix2pix">
<h3><span class="section-number">8.2.2. </span>pix2pix<a class="headerlink" href="#pix2pix" title="Permalink to this headline">¶</a></h3>
<p>cGAN can be extended to have an autoencoder-like architecture, allowing to generate images from images. <strong>pix2pix</strong> <span id="id14">[<a class="reference internal" href="../zreferences.html#id93">Isola et al., 2018</a>]</span> is trained on pairs of similar images in different domains. The conversion from one domain to another is easy in one direction, but we want to learn the opposite.</p>
<figure class="align-default" id="id28">
<a class="reference internal image-reference" href="../_images/dcgan-imageimage.jpg"><img alt="../_images/dcgan-imageimage.jpg" src="../_images/dcgan-imageimage.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.11 </span><span class="caption-text">pix2pix <span id="id15">[<a class="reference internal" href="../zreferences.html#id93">Isola et al., 2018</a>]</span>.</span><a class="headerlink" href="#id28" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The goal of the generator is to convert for example a black-and-white image into a colorized one. It is a deep convolutional autoencoder, with convolutions with strides and transposed convolutions (SegNet-like).</p>
<figure class="align-default" id="id29">
<a class="reference internal image-reference" href="../_images/pix2pix-generator1.png"><img alt="../_images/pix2pix-generator1.png" src="../_images/pix2pix-generator1.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.12 </span><span class="caption-text">pix2pix generator. Source: <a class="reference external" href="https://affinelayer.com/pix2pix/">https://affinelayer.com/pix2pix/</a>.</span><a class="headerlink" href="#id29" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id30">
<a class="reference internal image-reference" href="../_images/pix2pix-generator2.png"><img alt="../_images/pix2pix-generator2.png" src="../_images/pix2pix-generator2.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.13 </span><span class="caption-text">Blocks of the pix2pix generator. Source: <a class="reference external" href="https://affinelayer.com/pix2pix/">https://affinelayer.com/pix2pix/</a>.</span><a class="headerlink" href="#id30" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In practice, it has a <strong>U-Net</strong> architecture with skip connections to generate fine details.</p>
<figure class="align-default" id="id31">
<a class="reference internal image-reference" href="../_images/pix2pix-generator3.png"><img alt="../_images/pix2pix-generator3.png" src="../_images/pix2pix-generator3.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.14 </span><span class="caption-text">pix2pix generator with skip connections. Source: <a class="reference external" href="https://affinelayer.com/pix2pix/">https://affinelayer.com/pix2pix/</a>.</span><a class="headerlink" href="#id31" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The discriminator takes a <strong>pair</strong> of images as input: input/target or input/generated. It does not output a single value real/fake, but a 30x30 “image” telling how real or fake is the corresponding <strong>patch</strong> of the unknown image. Patches correspond to overlapping 70x70 regions of the 256x256 input image. This type of discriminator is called a <strong>PatchGAN</strong>.</p>
<figure class="align-default" id="id32">
<a class="reference internal image-reference" href="../_images/pix2pix-discriminator-principle.png"><img alt="../_images/pix2pix-discriminator-principle.png" src="../_images/pix2pix-discriminator-principle.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.15 </span><span class="caption-text">pix2pix discriminator. Source: <a class="reference external" href="https://affinelayer.com/pix2pix/">https://affinelayer.com/pix2pix/</a>.</span><a class="headerlink" href="#id32" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id33">
<a class="reference internal image-reference" href="../_images/pix2pix-discriminator.png"><img alt="../_images/pix2pix-discriminator.png" src="../_images/pix2pix-discriminator.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.16 </span><span class="caption-text">pix2pix discriminator. Source: <a class="reference external" href="https://affinelayer.com/pix2pix/">https://affinelayer.com/pix2pix/</a>.</span><a class="headerlink" href="#id33" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The discriminator is trained like in a regular GAN by alternating input/target or input/generated pairs.</p>
<figure class="align-default" id="id34">
<a class="reference internal image-reference" href="../_images/pix2pix-discriminator-training.png"><img alt="../_images/pix2pix-discriminator-training.png" src="../_images/pix2pix-discriminator-training.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.17 </span><span class="caption-text">pix2pix discriminator training. Source: <a class="reference external" href="https://affinelayer.com/pix2pix/">https://affinelayer.com/pix2pix/</a>.</span><a class="headerlink" href="#id34" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The generator is trained by maximizing the GAN loss (using gradients backpropagated through the discriminator) but also by minimizing the L1 distance between the generated image and the target (supervised learning).</p>
<div class="math notranslate nohighlight">
\[
    \min_G \max_D V(D, G) = V_\text{GAN}(D, G) + \lambda \, \mathbb{E}_\mathcal{D} [|T - G|]
\]</div>
<figure class="align-default" id="id35">
<a class="reference internal image-reference" href="../_images/pix2pix-generator-training.png"><img alt="../_images/pix2pix-generator-training.png" src="../_images/pix2pix-generator-training.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.18 </span><span class="caption-text">pix2pix generator training. Source: <a class="reference external" href="https://affinelayer.com/pix2pix/">https://affinelayer.com/pix2pix/</a>.</span><a class="headerlink" href="#id35" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="cyclegan-neural-style-transfer">
<h3><span class="section-number">8.2.3. </span>CycleGAN : Neural Style Transfer<a class="headerlink" href="#cyclegan-neural-style-transfer" title="Permalink to this headline">¶</a></h3>
<p>The drawback of pix2pix is that you need <strong>paired</strong> examples of each domain, which is sometimes difficult to obtain. In <strong>style transfer</strong>, we are interested in converting images using unpaired datasets, for example realistic photographies and paintings. <strong>CycleGAN</strong> <span id="id16">[<a class="reference internal" href="../zreferences.html#id215">Zhu et al., 2020</a>]</span> is a GAN architecture for neural style transfer.</p>
<figure class="align-default" id="id36">
<a class="reference internal image-reference" href="../_images/img_translation.jpeg"><img alt="../_images/img_translation.jpeg" src="../_images/img_translation.jpeg" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.19 </span><span class="caption-text">Neural style transfer requires unpaired domains <span id="id17">[<a class="reference internal" href="../zreferences.html#id215">Zhu et al., 2020</a>]</span>.</span><a class="headerlink" href="#id36" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id37">
<a class="reference internal image-reference" href="../_images/doge_starrynight.jpg"><img alt="../_images/doge_starrynight.jpg" src="../_images/doge_starrynight.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.20 </span><span class="caption-text">Neural style transfer. Source: <a class="reference external" href="https://hardikbansal.github.io/CycleGANBlog/">https://hardikbansal.github.io/CycleGANBlog/</a></span><a class="headerlink" href="#id37" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Let’s suppose that we want to transform <strong>domain A</strong> (horses) into <strong>domain B</strong> (zebras) or the other way around. The problem is that the two datasets are not paired, so we cannot provide targets to pix2pix (supervised learning). If we just select any zebra target for a horse input, pix2pix would learn to generate zebras that do not correspond to the input horse (the shape may be lost).
How about we train a second GAN to generate the target?</p>
<figure class="align-default" id="id38">
<a class="reference internal image-reference" href="../_images/cycle-gan-zebra-horse-images.jpg"><img alt="../_images/cycle-gan-zebra-horse-images.jpg" src="../_images/cycle-gan-zebra-horse-images.jpg" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.21 </span><span class="caption-text">Neural style transfer between horses and zebras. Source: <a class="reference external" href="https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff">https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</a></span><a class="headerlink" href="#id38" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>Cycle A2B2A</strong></p>
<ul class="simple">
<li><p>The A2B generator generates a sample of B from an image of A.</p></li>
<li><p>The B discriminator allows to train A2B using real images of B.</p></li>
<li><p>The B2A generator generates a sample of A from the output of A2B, which can be used to minimize the L1-reconstruction loss (shape-preserving).</p></li>
</ul>
<figure class="align-default" id="id39">
<a class="reference internal image-reference" href="../_images/cyclegan-AB.jpeg"><img alt="../_images/cyclegan-AB.jpeg" src="../_images/cyclegan-AB.jpeg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.22 </span><span class="caption-text">Cycle A2B2A. Source: <a class="reference external" href="https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff">https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</a></span><a class="headerlink" href="#id39" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>Cycle B2A2B</strong></p>
<p>In the B2A2B cycle, the domains are reversed, what allows to train the A discriminator.</p>
<figure class="align-default" id="id40">
<a class="reference internal image-reference" href="../_images/cyclegan-BA.jpeg"><img alt="../_images/cyclegan-BA.jpeg" src="../_images/cyclegan-BA.jpeg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.23 </span><span class="caption-text">Cycle B2A2B. Source: <a class="reference external" href="https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff">https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</a></span><a class="headerlink" href="#id40" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>This cycle is repeated throughout training, allowing to train both GANS concurrently.</p>
<figure class="align-default" id="id41">
<a class="reference internal image-reference" href="../_images/cycleGAN2.jpg"><img alt="../_images/cycleGAN2.jpg" src="../_images/cycleGAN2.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.24 </span><span class="caption-text">CycleGAN. Source: <a class="reference external" href="https://github.com/junyanz/CycleGAN">https://github.com/junyanz/CycleGAN</a>.</span><a class="headerlink" href="#id41" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id42">
<a class="reference internal image-reference" href="../_images/cycleGAN3.jpg"><img alt="../_images/cycleGAN3.jpg" src="../_images/cycleGAN3.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.25 </span><span class="caption-text">CycleGAN. Source: <a class="reference external" href="https://github.com/junyanz/CycleGAN">https://github.com/junyanz/CycleGAN</a>.</span><a class="headerlink" href="#id42" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id43">
<a class="reference internal image-reference" href="../_images/cycleGAN4.jpg"><img alt="../_images/cycleGAN4.jpg" src="../_images/cycleGAN4.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.26 </span><span class="caption-text">CycleGAN. Source: <a class="reference external" href="https://github.com/junyanz/CycleGAN">https://github.com/junyanz/CycleGAN</a>.</span><a class="headerlink" href="#id43" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/fu2fzx4w3mI' frameborder='0' allowfullscreen></iframe></div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3-deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="7-RBM.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7. </span>Restricted Boltzmann machines (optional)</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="9-RNN.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Recurrent neural networks</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>