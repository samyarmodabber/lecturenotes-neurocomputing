
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7. Restricted Boltzmann machines (optional) &#8212; Neurocomputing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/7-RBM.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8. Generative adversarial networks" href="8-GAN.html" />
    <link rel="prev" title="6. Autoencoders" href="6-Autoencoders.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tuc.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neurocomputing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-Attention.html">
   10. Attentional neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3-deeplearning/7-RBM.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#structure-of-a-rbm">
   7.1. Structure of a RBM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#minimizing-the-free-energy">
   7.2. Minimizing the free energy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-belief-networks-stacked-rbms">
     7.2.1. Deep Belief Networks = stacked RBMs
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="restricted-boltzmann-machines-optional">
<h1><span class="section-number">7. </span>Restricted Boltzmann machines (optional)<a class="headerlink" href="#restricted-boltzmann-machines-optional" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/3.7-RBM.pdf">pdf</a></p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/LzarsZlz7As' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="structure-of-a-rbm">
<h2><span class="section-number">7.1. </span>Structure of a RBM<a class="headerlink" href="#structure-of-a-rbm" title="Permalink to this headline">¶</a></h2>
<p>Auto-encoders are not the only feature extractors that can be stacked.</p>
<p><strong>Restricted Boltzmann Machines</strong> (RBM, <span id="id1">[<a class="reference internal" href="../zreferences.html#id76">Hinton et al., 2006</a>]</span>) are generative stochastic artificial neural networks that can learn a probability distribution of their inputs. Their neurons form a bipartite graph with two groups of reciprocally connected units:</p>
<ul class="simple">
<li><p>the <strong>visible units</strong> <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> (the inputs)</p></li>
<li><p>the <strong>hidden units</strong> <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> (the features or latent space).</p></li>
</ul>
<p>Connections are bidirectional between <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>, but the neurons inside the two groups are independent from each other (<em>restricted</em>). The goal of learning is to find the weights allowing the network to <strong>explain</strong> best the input data.</p>
<div class="figure align-default" id="id8">
<a class="reference internal image-reference" href="img/rbm.jpg"><img alt="img/rbm.jpg" src="img/rbm.jpg" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 7.1 </span><span class="caption-text">Restricted Boltzmann Machine  <span id="id2">[<a class="reference internal" href="../zreferences.html#id76">Hinton et al., 2006</a>]</span>.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
<p>RBMs are a form of autoencoder where the input <span class="math notranslate nohighlight">\(\rightarrow\)</span> feature weight matrix is the same as the feature <span class="math notranslate nohighlight">\(\rightarrow\)</span> output matrix. There are two steps:</p>
<ul class="simple">
<li><p>The <strong>forward pass</strong> <span class="math notranslate nohighlight">\(P(\mathbf{h} | \mathbf{x})\)</span> propagates the visible units activation to the hidden units.</p></li>
<li><p>The <strong>backward pass</strong> <span class="math notranslate nohighlight">\(P(\mathbf{x} | \mathbf{h})\)</span> reconstructs the visible units from the the hidden units.</p></li>
</ul>
<p>If the weight matrix is correctly chosen, the reconstructed input should “match” the original input: the data is explained.</p>
<div class="figure align-default" id="id9">
<a class="reference internal image-reference" href="img/rbm-autoencoder.png"><img alt="img/rbm-autoencoder.png" src="img/rbm-autoencoder.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 7.2 </span><span class="caption-text">RBMs are autoencoders where the decoder uses the same weights as the encoder. Source: <a class="reference external" href="https://www.edureka.co/blog/restricted-boltzmann-machine-tutorial/">https://www.edureka.co/blog/restricted-boltzmann-machine-tutorial/</a></span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<p>The visible and units are generally <strong>binary units</strong> (0 or 1), with a probability defined by the weights and biases and the logistic function:</p>
<div class="math notranslate nohighlight">
\[
    P(h_j = 1 | \mathbf{v}) = \sigma( \sum_i W_{ij} \, v_i + c_j)
\]</div>
<div class="math notranslate nohighlight">
\[
    P(v_i = 1 | \mathbf{h}) = \sigma(\sum_i W_{ji} \, h_j + b_i )
\]</div>
<p>The weight matrix <span class="math notranslate nohighlight">\(W\)</span> and the biases <span class="math notranslate nohighlight">\(\mathbf{b}, \mathbf{c}\)</span> are the parameters <span class="math notranslate nohighlight">\(\theta\)</span> of a <strong>probability distribution</strong> over the activation of the visible and hidden units.</p>
</div>
<div class="section" id="minimizing-the-free-energy">
<h2><span class="section-number">7.2. </span>Minimizing the free energy<a class="headerlink" href="#minimizing-the-free-energy" title="Permalink to this headline">¶</a></h2>
<p>The goal is to find the parameters which explain best the data (visible units), i.e. the ones maximizing the <strong>log-likelihood</strong> of the model for the data <span class="math notranslate nohighlight">\((\mathbf{v}_1, \ldots, \mathbf{v}_N)\)</span>. We use <strong>maximum likelihood estimation</strong> (MLE) to maximize the log-likelihood of the model:</p>
<div class="math notranslate nohighlight">
\[
    \max_{\theta} \,  \mathcal{L}(\theta) = \mathbb{E}_{\mathbf{v} \sim \mathcal{D}} [ \log \, P_\theta(\mathbf{v}) ]
\]</div>
<p>In practice, MLE is not tractable in a RBM, as we cannot estimate the joint probability <span class="math notranslate nohighlight">\(P(\mathbf{v}, \mathbf{h})\)</span> of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> (too many combinations are possible).</p>
<div class="math notranslate nohighlight">
\[
    P(\mathbf{v}) = \sum_\mathbf{h} P(\mathbf{v}, \mathbf{h})
\]</div>
<p>The main trick in <strong>energy-based models</strong> is to rewrite the probabilities using an energy function <span class="math notranslate nohighlight">\(E(\mathbf{v}, \mathbf{h})\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    P(\mathbf{v}) = \sum_\mathbf{h} P(\mathbf{v}, \mathbf{h}) = \dfrac{\sum_\mathbf{h} \exp^{-E(\mathbf{v}, \mathbf{h})}}{\sum_{\mathbf{v}, \mathbf{h}} \exp^{-E(\mathbf{v}, \mathbf{h})}} = \dfrac{1}{Z} \sum_\mathbf{h} \exp^{-E(\mathbf{v}, \mathbf{h})}
\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[
Z = \sum_{\mathbf{v}, \mathbf{h}} \exp^{-E(\mathbf{v}, \mathbf{h})} = \sum_{\mathbf{v}} P(\mathbf{v}) \, \sum_{\mathbf{h}} \exp^{-E(\mathbf{v}, \mathbf{h})}
\]</div>
<p>is the <strong>partition function</strong> (a normalizing term).</p>
<p>The probabilities come from a <strong>Gibbs distribution</strong> (or Boltzmann distribution) parameterized by the energy of the system. This is equivalent to a simple <strong>softmax</strong> over the energy…</p>
<p>Having reformulated the probabilities in terms of energy:</p>
<div class="math notranslate nohighlight">
\[
    P(\mathbf{v}) = \frac{1}{Z} \sum_\mathbf{h} \exp^{-E(\mathbf{v}, \mathbf{h})}
\]</div>
<p>we can introduce the <strong>free energy</strong> of the model for a sample <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> (how surprising is the input <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> for the model):</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{F}(\mathbf{v}) = - \log \, \sum_\mathbf{h} \exp^{-E(\mathbf{v}, \mathbf{h})}
\]</div>
<p>The log-likelihood of the model for a sample <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> of the training data <span class="math notranslate nohighlight">\((\mathbf{v}_1, \ldots, \mathbf{v}_N)\)</span> becomes:</p>
<div class="math notranslate nohighlight">
\[\log \, P(\mathbf{v}) = \log \, \frac{1}{Z} \sum_\mathbf{h} \exp^{-E(\mathbf{v}, \mathbf{h})} = - \mathcal{F}(\mathbf{v}) + \log Z = - \mathcal{F}(\mathbf{v}) + \sum_{\mathbf{v}} P(\mathbf{v}) \, \mathcal{F}(\mathbf{v})\]</div>
<p>Note that the second term sums over all possible inputs <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>. Maximizing the log-likelihood of the model on the training data can be done using gradient ascent by following this gradient:</p>
<div class="math notranslate nohighlight">
\[
    \nabla_\theta \mathcal{L}(\theta) = \mathbb{E}_{\mathbf{v}} [\nabla_\theta \log \, P(\mathbf{v}_i)] = \mathbb{E}_{\mathbf{v}} [ - \nabla_\theta \mathcal{F}(\mathbf{v}) + \sum_{\mathbf{v}} P(\mathbf{v}) \nabla_\theta  \mathcal{F}(\mathbf{v})]
\]</div>
<p>The free energy for a RBM with binary neurons is fortunately known analytically:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{F}(\mathbf{v}) = - \sum_i b_i \, v_i - \sum_j \log (1 + \exp^{\sum_i W_{ij} \, v_i + c_j})
\]</div>
<p>so finding the gradient w.r.t <span class="math notranslate nohighlight">\(\theta = (W, \mathbf{b}, \mathbf{c})\)</span> of the first term on the r.h.s (the free energy of the sample) is easy:</p>
<div class="math notranslate nohighlight">
\[
    \nabla_\theta \log \, P(\mathbf{v}) = - \nabla_\theta \mathcal{F}(\mathbf{v}) + \sum_{\mathbf{v}} P(\mathbf{v}) \nabla_\theta  \mathcal{F}(\mathbf{v})
\]</div>
<p>In particular, the gradient w.r.t the matrix <span class="math notranslate nohighlight">\(W\)</span> is the outer product between <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> and <span class="math notranslate nohighlight">\(P(\mathbf{h} | \mathbf{v})\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \nabla_W \mathcal{F}(\mathbf{v}) = - \mathbf{v} \times P(\mathbf{h} | \mathbf{v})
\]</div>
<p>The problem is the second term: we would need to integrate over all possible values of the inputs <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, what is not tractable. We will therefore make an approximation using <strong>Gibbs sampling</strong> (a variant of <strong>Monte-Carlo Markov Chain</strong> sampling - MCMC) to estimate that second term.</p>
<div class="figure align-default" id="id10">
<a class="reference internal image-reference" href="img/gibbssampling.png"><img alt="img/gibbssampling.png" src="img/gibbssampling.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 7.3 </span><span class="caption-text">Gibbs sampling. Source : <a class="reference external" href="https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15">https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15</a></span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>Gibbs sampling consists of repeatedly applying the encoder <span class="math notranslate nohighlight">\(P(\mathbf{h} | \mathbf{v})\)</span> and the decoder <span class="math notranslate nohighlight">\(P(\mathbf{v} | \mathbf{h})\)</span> on the input.</p>
<ul class="simple">
<li><p>We start by setting <span class="math notranslate nohighlight">\(\mathbf{v}_0 = \mathbf{v}\)</span> using a training sample.</p></li>
<li><p>We obtain <span class="math notranslate nohighlight">\(\mathbf{h}_0\)</span> by computing <span class="math notranslate nohighlight">\(P(\mathbf{h} | \mathbf{v}_0)\)</span> and sampling it.</p></li>
<li><p>We obtain <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> y computing <span class="math notranslate nohighlight">\(P(\mathbf{v} | \mathbf{h}_0)\)</span> and sampling it.</p></li>
<li><p>…</p></li>
<li><p>We obtain <span class="math notranslate nohighlight">\(\mathbf{v}_k\)</span> y computing <span class="math notranslate nohighlight">\(P(\mathbf{v} | \mathbf{h}_{k-1})\)</span> and sampling it.</p></li>
</ul>
<p>After enough iterations <span class="math notranslate nohighlight">\(k\)</span>, we should have a good estimate of <span class="math notranslate nohighlight">\(P(\mathbf{v}, \mathbf{h})\)</span>. The <span class="math notranslate nohighlight">\(k\)</span> iterations have generated enough <strong>reconstructions</strong> of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> to cover the distribution of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>.</p>
<p>We set <span class="math notranslate nohighlight">\(\mathbf{v}_0 = \mathbf{v}\)</span> on a training sample and let Gibbs sampling iterate for <span class="math notranslate nohighlight">\(k\)</span> iterations until we obtain <span class="math notranslate nohighlight">\(\mathbf{v}_k = \mathbf{v}^*\)</span>. <strong>Contrastive divergence</strong> (CD-<span class="math notranslate nohighlight">\(k\)</span>, <span id="id3">[<a class="reference internal" href="../zreferences.html#id77">Hinton &amp; Salakhutdinov, 2006</a>]</span>) shows that the gradient of the log-likelihood can be approximated by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\nabla_W \log \, P(\mathbf{v}) &amp; = - \nabla_W \mathcal{F}(\mathbf{v}) + \sum_{\mathbf{v}} P(\mathbf{v}) \nabla_W  \mathcal{F}(\mathbf{v}) \\
    &amp; \approx \mathbf{v} \times P(\mathbf{h} | \mathbf{v}) - \mathbf{v}^* \times P(\mathbf{h} | \mathbf{v}^*) \\
\end{align}
\end{split}\]</div>
<p>The gradient of the log-likelihood is the difference between the initial explanation of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> by the model, and its explanation after <span class="math notranslate nohighlight">\(k\)</span> iterations (relaxation). If the model is good, the reconstruction <span class="math notranslate nohighlight">\(\mathbf{v}^*\)</span> is the same as the input <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, so the gradient is zero. An input <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> is likely under the RBM model if it is able to reconstruct it, i.e. when it is not surprising (the free energy is low). In practice, <span class="math notranslate nohighlight">\(k=1\)</span> gives surprisingly good results, but RBMs are very painful to train (hyperparameters)…</p>
<div class="section" id="deep-belief-networks-stacked-rbms">
<h3><span class="section-number">7.2.1. </span>Deep Belief Networks = stacked RBMs<a class="headerlink" href="#deep-belief-networks-stacked-rbms" title="Permalink to this headline">¶</a></h3>
<p>A <strong>Deep Belief Network</strong> (DBN) is a simple stack of RBMS, trained using greedy layer-wise learning. The “bottom” parts of the DBM become unidirectional when learning the top part.</p>
<div class="figure align-default" id="id11">
<a class="reference internal image-reference" href="img/deep_belief_network_example.png"><img alt="img/deep_belief_network_example.png" src="img/deep_belief_network_example.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 7.4 </span><span class="caption-text">Deep belief network <span id="id4">[<a class="reference internal" href="../zreferences.html#id77">Hinton &amp; Salakhutdinov, 2006</a>]</span>.</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
<p>Andrew Ng and colleagues (Google, Stanford, <span id="id5">[<a class="reference internal" href="../zreferences.html#id117">Le, 2013</a>]</span>) trained a deep belief network on color images (200x200) taken from 10 million random unlabeled Youtube videos. Each layer was trained greedily. They also used a couple of other tricks (receptive fields, contrast normalization). Training was distributed over 1000 machines (16.000 cores) and lasted for three days. There was absolutely no task: the network just had to watch youtube videos. After learning, they visualized what the neurons had learned.</p>
<div class="figure align-default" id="id12">
<a class="reference internal image-reference" href="img/catnetwork.png"><img alt="img/catnetwork.png" src="img/catnetwork.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 7.5 </span><span class="caption-text">Deep Belief Network <span id="id6">[<a class="reference internal" href="../zreferences.html#id117">Le, 2013</a>]</span>.</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p>After training, some neurons had learned to respond uniquely to faces, or to cats, without ever having been instructed to. The network can then be fine-tuned for classification tasks, improving the pre-AlexNet state-of-the-art on ImageNet by 70%.</p>
<div class="figure align-default" id="id13">
<a class="reference internal image-reference" href="img/catfinder.png"><img alt="img/catfinder.png" src="img/catfinder.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 7.6 </span><span class="caption-text">Deep Belief Network <span id="id7">[<a class="reference internal" href="../zreferences.html#id117">Le, 2013</a>]</span>.</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3-deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="6-Autoencoders.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">6. </span>Autoencoders</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="8-GAN.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Generative adversarial networks</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>