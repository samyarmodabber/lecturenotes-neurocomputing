
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Convolutional neural networks &#8212; Neurocomputing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/3-CNN.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Object detection" href="4-ObjectDetection.html" />
    <link rel="prev" title="2. Deep neural networks" href="2-DNN.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tuc.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neurocomputing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-Attention.html">
   10. Attentional neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3-deeplearning/3-CNN.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   3.1. Convolutional neural networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rationale">
     3.1.1. Rationale
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-convolutional-layer">
     3.1.2. The convolutional layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#max-pooling-layer">
     3.1.3. Max-pooling layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-with-strides">
     3.1.4. Convolution with strides
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dilated-convolutions">
     3.1.5. Dilated convolutions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backpropagation-through-a-convolutional-layer">
     3.1.6. Backpropagation through a convolutional layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backpropagation-through-a-max-pooling-layer">
     3.1.7. Backpropagation through a max-pooling layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     3.1.8. Convolutional Neural Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-famous-convolutional-networks">
   3.2. Some famous convolutional networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neocognitron">
     3.2.1. Neocognitron
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lenet">
     3.2.2. LeNet
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alexnet">
     3.2.3. AlexNet
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vgg-16">
     3.2.4. VGG-16
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#googlenet-inception-v1">
     3.2.5. GoogLeNet - Inception v1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#resnet">
     3.2.6. ResNet
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#highnets-highway-networks">
     3.2.7. HighNets: Highway networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#densenets-dense-networks">
     3.2.8. DenseNets: Dense networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-zoos">
     3.2.9. Model zoos
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications-of-cnn">
   3.3. Applications of CNN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#object-recognition">
     3.3.1. Object recognition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#facial-recognition">
     3.3.2. Facial recognition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pose-estimation">
     3.3.3. Pose estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#speech-recognition">
     3.3.4. Speech recognition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sentiment-analysis">
     3.3.5. Sentiment analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wavenet-text-to-speech-synthesis">
     3.3.6. Wavenet : text-to-speech synthesis
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transfer-learning">
   3.4. Transfer learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ensemble-learning">
   3.5. Ensemble learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bagging">
     3.5.1. Bagging
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#boosting">
     3.5.2. Boosting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stacking">
     3.5.3. Stacking
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="convolutional-neural-networks">
<h1><span class="section-number">3. </span>Convolutional neural networks<a class="headerlink" href="#convolutional-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/3.3-CNN.pdf">pdf</a></p>
<div class="section" id="id1">
<h2><span class="section-number">3.1. </span>Convolutional neural networks<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/2KASQi7avYA' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="rationale">
<h3><span class="section-number">3.1.1. </span>Rationale<a class="headerlink" href="#rationale" title="Permalink to this headline">¶</a></h3>
<p>The different layers of a deep network extract increasingly complex features.</p>
<blockquote>
<div><p>edges <span class="math notranslate nohighlight">\(\rightarrow\)</span> contours <span class="math notranslate nohighlight">\(\rightarrow\)</span> shapes <span class="math notranslate nohighlight">\(\rightarrow\)</span> objects</p>
</div></blockquote>
<p><img alt="" src="../_images/deeplearning.png" /></p>
<p>Using full images as inputs leads to an explosion of the number of weights to be learned: A moderately big 800 * 600 image has 480,000 pixels with RGB values. The number of dimensions of the input space is 800 * 600 * 3 = 1.44 million. Even if you take only 1000 neurons in the first hidden layer, you get 1.44 <strong>billion</strong> weights to learn, just for the first layer. To obtain a generalization error in the range of 10%, you would need at least 14 billion training examples…</p>
<div class="math notranslate nohighlight">
\[\epsilon \approx \frac{\text{VC}_\text{dim}}{N}\]</div>
<div class="figure align-default" id="id37">
<a class="reference internal image-reference" href="../_images/fullyconnected.png"><img alt="../_images/fullyconnected.png" src="../_images/fullyconnected.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.40 </span><span class="caption-text">Fully-connected layers require a lot of weights in images.</span><a class="headerlink" href="#id37" title="Permalink to this image">¶</a></p>
</div>
<p>Early features (edges) are usually local, there is no need to learn weights from the whole image. Natural images are stationary: the statistics of the pixel in a small patch are the same, regardless the position on the image. <strong>Idea:</strong> One only needs to extract features locally and <strong>share the weights</strong> between the different locations. This is a <strong>convolution operation</strong>: a filter/kernel is applied on small patches and slided over the whole image.</p>
<div class="figure align-default" id="id38">
<a class="reference internal image-reference" href="../_images/convolutional.png"><img alt="../_images/convolutional.png" src="../_images/convolutional.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.41 </span><span class="caption-text">Convolutional layers share weights along the image dimensions.</span><a class="headerlink" href="#id38" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="the-convolutional-layer">
<h3><span class="section-number">3.1.2. </span>The convolutional layer<a class="headerlink" href="#the-convolutional-layer" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id39">
<a class="reference internal image-reference" href="../_images/convolution-anim2.gif"><img alt="../_images/convolution-anim2.gif" src="../_images/convolution-anim2.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.42 </span><span class="caption-text">Principle of a convolution. Source: <a class="reference external" href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53</a></span><a class="headerlink" href="#id39" title="Permalink to this image">¶</a></p>
</div>
<p>In a <strong>convolutional layer</strong>, <span class="math notranslate nohighlight">\(d\)</span> filters are defined with very small sizes (3x3, 5x5…). Each filter is convoluted over the input image (or the previous layer) to create a <strong>feature map</strong>. The set of <span class="math notranslate nohighlight">\(d\)</span> feature maps becomes a new 3D structure: a <strong>tensor</strong>.</p>
<div class="math notranslate nohighlight">
\[\mathbf{h}_k = W_k \ast \mathbf{h}_{k-1} + \mathbf{b}_k\]</div>
<div class="figure align-default" id="id40">
<a class="reference internal image-reference" href="../_images/depthcol.jpeg"><img alt="../_images/depthcol.jpeg" src="../_images/depthcol.jpeg" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.43 </span><span class="caption-text">Convolutional layer. Source: <a class="reference external" href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></span><a class="headerlink" href="#id40" title="Permalink to this image">¶</a></p>
</div>
<p>If the input image is 32x32x3, the resulting tensor will be 32x32xd. The convolutional layer has only very few parameters: each feature map has 3x3x3 values in the filter plus a bias, i.e. 28 parameters. As in image processing, a padding method must be chosen (what to do when a pixel is outside the image).</p>
<div class="figure align-default" id="id41">
<a class="reference internal image-reference" href="../_images/same_padding_no_strides.gif"><img alt="../_images/same_padding_no_strides.gif" src="../_images/same_padding_no_strides.gif" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.44 </span><span class="caption-text">Convolution with stride 1. Source: <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a></span><a class="headerlink" href="#id41" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="max-pooling-layer">
<h3><span class="section-number">3.1.3. </span>Max-pooling layer<a class="headerlink" href="#max-pooling-layer" title="Permalink to this headline">¶</a></h3>
<p>The number of elements in a convolutional layer is still too high. We need to reduce the spatial dimension of a convolutional layer by <strong>downsampling</strong> it. For each feature, a <strong>max-pooling</strong> layer takes the maximum value of a feature for each subregion of the image (generally 2x2).Mean-pooling layers are also possible, but they are not used anymore. Pooling allows translation invariance: the same input pattern will be detected whatever its position in the input image.</p>
<div class="figure align-default" id="id42">
<a class="reference internal image-reference" href="../_images/pooling.jpg"><img alt="../_images/pooling.jpg" src="../_images/pooling.jpg" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.45 </span><span class="caption-text">Pooling layer. Source: <a class="reference external" href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></span><a class="headerlink" href="#id42" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id43">
<a class="reference internal image-reference" href="../_images/maxpooling.png"><img alt="../_images/maxpooling.png" src="../_images/maxpooling.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.46 </span><span class="caption-text">Pooling layer. Source: <a class="reference external" href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></span><a class="headerlink" href="#id43" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="convolution-with-strides">
<h3><span class="section-number">3.1.4. </span>Convolution with strides<a class="headerlink" href="#convolution-with-strides" title="Permalink to this headline">¶</a></h3>
<p>Convolution with strides <span id="id2">[<a class="reference internal" href="../zreferences.html#id179">Springenberg et al., 2015</a>]</span> is an alternative to max-pooling layers. The convolution simply “jumps” one pixel when sliding over the image (stride 2). This results in a smaller feature map, using much less operations than a convolution with stride 1 followed by max-pooling, for the same performance.
They are particularly useful for generative models (VAE, GAN, etc).</p>
<div class="figure align-default" id="id44">
<a class="reference internal image-reference" href="../_images/padding_strides.gif"><img alt="../_images/padding_strides.gif" src="../_images/padding_strides.gif" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.47 </span><span class="caption-text">Convolution with stride 2. Source: <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a></span><a class="headerlink" href="#id44" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="dilated-convolutions">
<h3><span class="section-number">3.1.5. </span>Dilated convolutions<a class="headerlink" href="#dilated-convolutions" title="Permalink to this headline">¶</a></h3>
<p>A <strong>dilated convolution</strong> is a convolution with holes (à trous). The filter has a bigger spatial extent than its number of values.</p>
<div class="figure align-default" id="id45">
<a class="reference internal image-reference" href="../_images/dilation.gif"><img alt="../_images/dilation.gif" src="../_images/dilation.gif" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.48 </span><span class="caption-text">Convolution à trous. Source: <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a></span><a class="headerlink" href="#id45" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="backpropagation-through-a-convolutional-layer">
<h3><span class="section-number">3.1.6. </span>Backpropagation through a convolutional layer<a class="headerlink" href="#backpropagation-through-a-convolutional-layer" title="Permalink to this headline">¶</a></h3>
<p>But how can we do backpropagation through a convolutional layer?</p>
<div class="figure align-default" id="id46">
<a class="reference internal image-reference" href="../_images/convolutional-forward.gif"><img alt="../_images/convolutional-forward.gif" src="../_images/convolutional-forward.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.49 </span><span class="caption-text">Forward convolution. Source: <a class="reference external" href="https://medium.com/&#64;mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710">https://medium.com/&#64;mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710</a></span><a class="headerlink" href="#id46" title="Permalink to this image">¶</a></p>
</div>
<p>In the example above, the four neurons of the feature map will receive a gradient from the upper layers. How can we use it to learn the filter values and pass the gradient to the lower layers?</p>
<p>The answer is simply by convolving the output gradients with the flipped filter!</p>
<div class="figure align-default" id="id47">
<a class="reference internal image-reference" href="../_images/convolutional-backward.gif"><img alt="../_images/convolutional-backward.gif" src="../_images/convolutional-backward.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.50 </span><span class="caption-text">Backward convolution. Source: <a class="reference external" href="https://medium.com/&#64;mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710">https://medium.com/&#64;mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710</a></span><a class="headerlink" href="#id47" title="Permalink to this image">¶</a></p>
</div>
<p>The filter just has to be flipped (<span class="math notranslate nohighlight">\(180^o\)</span> symmetry) before the convolution.</p>
<div class="figure align-default" id="id48">
<a class="reference internal image-reference" href="../_images/convolution-flipped.png"><img alt="../_images/convolution-flipped.png" src="../_images/convolution-flipped.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.51 </span><span class="caption-text">Flipping the filter. Source: <a class="reference external" href="https://medium.com/&#64;mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710">https://medium.com/&#64;mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710</a></span><a class="headerlink" href="#id48" title="Permalink to this image">¶</a></p>
</div>
<p>The convolution operation is differentiable, so we can apply backpropagation and learn the filters.</p>
<div class="math notranslate nohighlight">
\[\mathbf{h}_k = W_k \ast \mathbf{h}_{k-1} + \mathbf{b}_k\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k-1}} = W_k^F \ast \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{h}_{k}}\]</div>
</div>
<div class="section" id="backpropagation-through-a-max-pooling-layer">
<h3><span class="section-number">3.1.7. </span>Backpropagation through a max-pooling layer<a class="headerlink" href="#backpropagation-through-a-max-pooling-layer" title="Permalink to this headline">¶</a></h3>
<p>We can also use backpropagation through a max-pooling layer. We need to remember which location was the winning location in order to backpropagate the gradient. A max-pooling layer has no parameter, we do not need to learn anything, just to pass the gradient backwards.</p>
<div class="figure align-default" id="id49">
<a class="reference internal image-reference" href="../_images/max-pooling-backprop.png"><img alt="../_images/max-pooling-backprop.png" src="../_images/max-pooling-backprop.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.52 </span><span class="caption-text">Backpropagation through a max-pooling layer. Source: <a class="reference external" href="https://mukulrathi.com/demystifying-deep-learning/conv-net-backpropagation-maths-intuition-derivation/">https://mukulrathi.com/demystifying-deep-learning/conv-net-backpropagation-maths-intuition-derivation/</a></span><a class="headerlink" href="#id49" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="id3">
<h3><span class="section-number">3.1.8. </span>Convolutional Neural Networks<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>A <strong>convolutional neural network</strong> (CNN) is a cascade of convolution and pooling operations, extracting layer by layer increasingly  complex features. The spatial dimensions decrease after each pooling operation, but the number of extracted features increases after each convolution. One usually stops when the spatial dimensions are around 7x7. The last layers are fully connected (classical MLP). Training a CNN uses backpropagation all along: the convolution and pooling operations are differentiable.</p>
<div class="figure align-default" id="id50">
<a class="reference internal image-reference" href="../_images/lenet.png"><img alt="../_images/lenet.png" src="../_images/lenet.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.53 </span><span class="caption-text">Convolutional Neural Network. <span id="id4">[<a class="reference internal" href="../zreferences.html#id119">LeCun et al., 1998</a>]</span>.</span><a class="headerlink" href="#id50" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition-implementing-a-cnn-in-keras admonition">
<p class="admonition-title">Implementing a CNN in keras</p>
<p>Convolutional and max-pooling layers are regular objects in keras/tensorflow/pytorch/etc. You do not need to care about their implementation, they are designed to run fast on GPUs. You have to apply to the CNN all the usual tricks: optimizers, dropout, batch normalization, etc.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Input</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span><span class="n">L</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">RMSprop</span><span class="p">(</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
    <span class="n">decay</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="some-famous-convolutional-networks">
<h2><span class="section-number">3.2. </span>Some famous convolutional networks<a class="headerlink" href="#some-famous-convolutional-networks" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/pw-sFY3UqG4' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="neocognitron">
<h3><span class="section-number">3.2.1. </span>Neocognitron<a class="headerlink" href="#neocognitron" title="Permalink to this headline">¶</a></h3>
<p>The <strong>Neocognitron</strong> (Fukushima, 1980 <span id="id5">[<a class="reference internal" href="../zreferences.html#id44">Fukushima, 1980</a>]</span>) was actually the first CNN able to recognize handwritten digits. Training is not based on backpropagation, but a set of biologically realistic learning rules (Add-if-silent, margined WTA). Inspired by the human visual system.</p>
<div class="figure align-default" id="id51">
<a class="reference internal image-reference" href="../_images/Neocognitron.jpg"><img alt="../_images/Neocognitron.jpg" src="../_images/Neocognitron.jpg" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.54 </span><span class="caption-text">Neocognitron. <span id="id6">[<a class="reference internal" href="../zreferences.html#id44">Fukushima, 1980</a>]</span>. Source: <a class="reference external" href="https://uplLoad.wikimedia.org/wikipedia/uk/4/42/Neocognitron.jpg">https://uplLoad.wikimedia.org/wikipedia/uk/4/42/Neocognitron.jpg</a></span><a class="headerlink" href="#id51" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="lenet">
<h3><span class="section-number">3.2.2. </span>LeNet<a class="headerlink" href="#lenet" title="Permalink to this headline">¶</a></h3>
<p><strong>LeNet</strong> (1998, Yann LeCun at AT&amp;T labs <span id="id7">[<a class="reference internal" href="../zreferences.html#id119">LeCun et al., 1998</a>]</span>) was one of the first CNN able to learn from raw data using backpropagation. It has two convolutional layers, two <strong>mean</strong>-pooling layers, two fully-connected layers and an output layer. It uses tanh as the activation function and works on CPU only. Used for handwriting recognition (for example ZIP codes).</p>
<div class="figure align-default" id="id52">
<a class="reference internal image-reference" href="../_images/lenet.png"><img alt="../_images/lenet.png" src="../_images/lenet.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.55 </span><span class="caption-text">LeNet5. <span id="id8">[<a class="reference internal" href="../zreferences.html#id119">LeCun et al., 1998</a>]</span>.</span><a class="headerlink" href="#id52" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="alexnet">
<h3><span class="section-number">3.2.3. </span>AlexNet<a class="headerlink" href="#alexnet" title="Permalink to this headline">¶</a></h3>
<p><strong>AlexNet</strong> (2012, Toronto University <span id="id9">[<a class="reference internal" href="../zreferences.html#id111">Krizhevsky et al., 2012</a>]</span>) started the DL revolution by winning ImageNet 2012. I has a similar architecture to LeNet, but is trained on two GPUs using augmented data. It uses ReLU,  max-pooling, dropout, SGD with momentum, L2 regularization.</p>
<div class="figure align-default" id="id53">
<a class="reference internal image-reference" href="../_images/alexnet.png"><img alt="../_images/alexnet.png" src="../_images/alexnet.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.56 </span><span class="caption-text">Alexnet <span id="id10">[<a class="reference internal" href="../zreferences.html#id111">Krizhevsky et al., 2012</a>]</span>.</span><a class="headerlink" href="#id53" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="vgg-16">
<h3><span class="section-number">3.2.4. </span>VGG-16<a class="headerlink" href="#vgg-16" title="Permalink to this headline">¶</a></h3>
<p><strong>VGG-16</strong> (2014, Visual Geometry Group, Oxford <span id="id11">[<a class="reference internal" href="../zreferences.html#id176">Simonyan &amp; Zisserman, 2015</a>]</span>) placed second at ImageNet 2014. It went much deeper than AlexNet with 16 parameterized layers (a VGG-19 version is also available with 19 layers). Its main novelty is that two convolutions are made successively before the max-pooling, implicitly increasing the receptive field (2 consecutive 3x3 filters cover 5x5 pixels). Drawback: 140M parameters (mostly from the last convolutional layer to the first fully connected) quickly fill up the memory of the GPU.</p>
<div class="figure align-default" id="id54">
<a class="reference internal image-reference" href="../_images/vgg16.png"><img alt="../_images/vgg16.png" src="../_images/vgg16.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.57 </span><span class="caption-text">VGG-16 <span id="id12">[<a class="reference internal" href="../zreferences.html#id176">Simonyan &amp; Zisserman, 2015</a>]</span>.</span><a class="headerlink" href="#id54" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="googlenet-inception-v1">
<h3><span class="section-number">3.2.5. </span>GoogLeNet - Inception v1<a class="headerlink" href="#googlenet-inception-v1" title="Permalink to this headline">¶</a></h3>
<p><strong>GoogLeNet</strong> (2014, Google Brain <span id="id13">[<a class="reference internal" href="../zreferences.html#id186">Szegedy et al., 2015</a>]</span>) used Inception modules (Network-in-Network) to further complexify each stage It won ImageNet 2014 with 22 layers. Dropout, SGD with Nesterov momentum.</p>
<div class="figure align-default" id="id55">
<a class="reference internal image-reference" href="../_images/googlenet-arch.jpg"><img alt="../_images/googlenet-arch.jpg" src="../_images/googlenet-arch.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.58 </span><span class="caption-text">GoogLeNet <span id="id14">[<a class="reference internal" href="../zreferences.html#id186">Szegedy et al., 2015</a>]</span>.</span><a class="headerlink" href="#id55" title="Permalink to this image">¶</a></p>
</div>
<p>Inside GoogleNet, each <strong>Inception</strong> module learns features at different resolutions using convolutions and max poolings of different sizes. 1x1 convolutions are <strong>shared MLPS</strong>: they transform a <span class="math notranslate nohighlight">\((w, h, d_1)\)</span> tensor into <span class="math notranslate nohighlight">\((w, h, d_2)\)</span> pixel per pixel. The resulting feature maps are concatenated along the feature dimension and passed to the next module.</p>
<div class="figure align-default" id="id56">
<a class="reference internal image-reference" href="../_images/inception.png"><img alt="../_images/inception.png" src="../_images/inception.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.59 </span><span class="caption-text">Inception module <span id="id15">[<a class="reference internal" href="../zreferences.html#id186">Szegedy et al., 2015</a>]</span>.</span><a class="headerlink" href="#id56" title="Permalink to this image">¶</a></p>
</div>
<p>Three softmax layers predict the classes at different levels of the network. The combined loss is:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta) = \mathbb{E}_\mathcal{D} [- \mathbf{t} \, \log \mathbf{y}_1 - \mathbf{t} \, \log \mathbf{y}_2 - \mathbf{t} \, \log \mathbf{y}_3]\]</div>
<p>Only the deeper softmax layer matters for the prediction. The additional losses improve convergence by fight vanishing gradients: the early layers get useful gradients from the lower softmax layers.</p>
<p>Several variants of GoogleNet have been later proposed: Inception v2, v3, InceptionResNet, Xception… Xception <span id="id16">[<a class="reference internal" href="../zreferences.html#id29">Chollet, 2017b</a>]</span> has currently the best top-1 accuracy on ImageNet: 126 layers, 22M parameters (88 MB). Pretrained weights are available in <code class="docutils literal notranslate"><span class="pre">keras</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">Xception</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;imagenet&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default" id="id57">
<a class="reference internal image-reference" href="../_images/inceptionv3.png"><img alt="../_images/inceptionv3.png" src="../_images/inceptionv3.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.60 </span><span class="caption-text">Inception v3 <span id="id17">[<a class="reference internal" href="../zreferences.html#id29">Chollet, 2017b</a>]</span>. Source: <a class="reference external" href="https://cloud.google.com/tpu/docs/inception-v3-advanced">https://cloud.google.com/tpu/docs/inception-v3-advanced</a></span><a class="headerlink" href="#id57" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="resnet">
<h3><span class="section-number">3.2.6. </span>ResNet<a class="headerlink" href="#resnet" title="Permalink to this headline">¶</a></h3>
<p><strong>ResNet</strong> (2015, Microsoft <span id="id18">[<a class="reference internal" href="../zreferences.html#id71">He et al., 2015a</a>]</span>) won ImageNet 2015. Instead of learning to transform an input directly with <span class="math notranslate nohighlight">\(\mathbf{h}_n = f_W(\mathbf{h}_{n-1})\)</span>, a <strong>residual layer</strong> learns to represent the residual between the output and the input:</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{h}_n = f_W(\mathbf{h}_{n-1}) + \mathbf{h}_{n-1}  \quad \rightarrow \quad f_W(\mathbf{h}_{n-1}) = \mathbf{h}_n - \mathbf{h}_{n-1}
\]</div>
<div class="figure align-default" id="id58">
<a class="reference internal image-reference" href="../_images/resnetlayer.jpg"><img alt="../_images/resnetlayer.jpg" src="../_images/resnetlayer.jpg" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.61 </span><span class="caption-text">Residual layer with skip connections  <span id="id19">[<a class="reference internal" href="../zreferences.html#id71">He et al., 2015a</a>]</span>.</span><a class="headerlink" href="#id58" title="Permalink to this image">¶</a></p>
</div>
<p>These <strong>skip connections</strong> allow the network to decide how deep it has to be. If the layer is not needed, the residual layer learns to output 0.</p>
<div class="figure align-default" id="id59">
<a class="reference internal image-reference" href="../_images/resnet2.png"><img alt="../_images/resnet2.png" src="../_images/resnet2.png" style="width: 20%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.62 </span><span class="caption-text">ResNet  <span id="id20">[<a class="reference internal" href="../zreferences.html#id71">He et al., 2015a</a>]</span>.</span><a class="headerlink" href="#id59" title="Permalink to this image">¶</a></p>
</div>
<p>Skip connections help overcome the <strong>vanishing gradients</strong> problem, as the contribution of bypassed layers to the backpropagated gradient is 1.</p>
<div class="math notranslate nohighlight">
\[\mathbf{h}_n = f_W(\mathbf{h}_{n-1}) + \mathbf{h}_{n-1}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathbf{h}_n}{\partial \mathbf{h}_{n-1}} = \frac{\partial f_W(\mathbf{h}_{n-1})}{\partial \mathbf{h}_{n-1}} + 1\]</div>
<p>The norm of the gradient stays roughly around one, limiting vanishing. Skip connections even can bypass whole blocks of layers. ResNet can have many layers without vanishing gradients. The most popular variants are:</p>
<ul class="simple">
<li><p>ResNet-50.</p></li>
<li><p>ResNet-101.</p></li>
<li><p>ResNet-152.</p></li>
</ul>
<p>It was the first network to make an heavy use of <strong>batch normalization</strong>.</p>
<div class="figure align-default" id="id60">
<a class="reference internal image-reference" href="../_images/resnet-block.png"><img alt="../_images/resnet-block.png" src="../_images/resnet-block.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.63 </span><span class="caption-text">Residual block  <span id="id21">[<a class="reference internal" href="../zreferences.html#id71">He et al., 2015a</a>]</span>.</span><a class="headerlink" href="#id60" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="highnets-highway-networks">
<h3><span class="section-number">3.2.7. </span>HighNets: Highway networks<a class="headerlink" href="#highnets-highway-networks" title="Permalink to this headline">¶</a></h3>
<p><strong>Highway networks</strong> (IDSIA <span id="id22">[<a class="reference internal" href="../zreferences.html#id181">Srivastava et al., 2015</a>]</span>) are residual networks which also learn to balance inputs with feature extraction:</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{h}_n = T_{W'} \, f_W(h_{n-1}) + (1 -  T_{W'}) \, h_{n-1}
\]</div>
<p>The balance between the <strong>primary</strong> pathway and the <strong>skip</strong> pathway adapts to the task. It has been used up to 1000 layers and improved state-of-the-art accuracy on MNIST and CIFAR-10.</p>
<div class="figure align-default" id="id61">
<a class="reference internal image-reference" href="../_images/highway.png"><img alt="../_images/highway.png" src="../_images/highway.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.64 </span><span class="caption-text">Highway network  <span id="id23">[<a class="reference internal" href="../zreferences.html#id181">Srivastava et al., 2015</a>]</span>.</span><a class="headerlink" href="#id61" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="densenets-dense-networks">
<h3><span class="section-number">3.2.8. </span>DenseNets: Dense networks<a class="headerlink" href="#densenets-dense-networks" title="Permalink to this headline">¶</a></h3>
<p><strong>Dense networks</strong> (Cornell University &amp; Facebook AI <span id="id24">[<a class="reference internal" href="../zreferences.html#id84">Huang et al., 2018</a>]</span>) are residual networks that can learn <strong>bypasses</strong> between any layer of the network (up to 5). It has 100 layers altogether and improved state-of-the-art accuracy on five major benchmarks.</p>
<div class="figure align-default" id="id62">
<a class="reference internal image-reference" href="../_images/densenetworks.png"><img alt="../_images/densenetworks.png" src="../_images/densenetworks.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.65 </span><span class="caption-text">Dense network  <span id="id25">[<a class="reference internal" href="../zreferences.html#id84">Huang et al., 2018</a>]</span>.</span><a class="headerlink" href="#id62" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="model-zoos">
<h3><span class="section-number">3.2.9. </span>Model zoos<a class="headerlink" href="#model-zoos" title="Permalink to this headline">¶</a></h3>
<p>These famous models are described in their respective papers, you could reimplement them and train them on ImageNet. Fortunately, their code is often released on Github by the authors or reimplemented by others. Most frameworks maintain <strong>model zoos</strong> of the most popular networks. Some models also have <strong>pretrained weights</strong> available, mostly on ImageNet. Very useful for <strong>transfer learning</strong> (see later).</p>
<ul class="simple">
<li><p>Overview website:</p></li>
</ul>
<p><a class="reference external" href="https://modelzoo.co">https://modelzoo.co</a></p>
<ul class="simple">
<li><p>Caffe:</p></li>
</ul>
<p><a class="reference external" href="https://github.com/BVLC/caffe/wiki/Model-Zoo">https://github.com/BVLC/caffe/wiki/Model-Zoo</a></p>
<ul class="simple">
<li><p>Tensorflow:</p></li>
</ul>
<p><a class="reference external" href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</a></p>
<ul class="simple">
<li><p>Pytorch:</p></li>
</ul>
<p><a class="reference external" href="https://pytorch.org/docs/stable/torchvision/models.html">https://pytorch.org/docs/stable/torchvision/models.html</a></p>
<ul class="simple">
<li><p>Papers with code:</p></li>
</ul>
<p><a class="reference external" href="https://paperswithcode.com/">https://paperswithcode.com/</a></p>
<p>Several criteria have to be considered when choosing an architecture:</p>
<ul class="simple">
<li><p>Accuracy on ImageNet.</p></li>
<li><p>Number of parameters (RAM consumption).</p></li>
<li><p>Speed (flops).</p></li>
</ul>
<div class="figure align-default" id="id63">
<a class="reference internal image-reference" href="../_images/deepnets-comparison.png"><img alt="../_images/deepnets-comparison.png" src="../_images/deepnets-comparison.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.66 </span><span class="caption-text">Speed-accuracy trade-off of state-of-the-art CNNs. Source: <a class="reference external" href="https://dataconomy.com/2017/04/history-neural-networks">https://dataconomy.com/2017/04/history-neural-networks</a></span><a class="headerlink" href="#id63" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="applications-of-cnn">
<h2><span class="section-number">3.3. </span>Applications of CNN<a class="headerlink" href="#applications-of-cnn" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/n7U9pywhQYM' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="object-recognition">
<h3><span class="section-number">3.3.1. </span>Object recognition<a class="headerlink" href="#object-recognition" title="Permalink to this headline">¶</a></h3>
<p><strong>Object recognition</strong> has become very easy thanks to CNNs. In object recognition, each image is associated to a label. With huge datasets like <strong>ImageNet</strong> (14 millions images), a CNN can learn to recognize 1000 classes of objects with a better accuracy than humans. Just get enough examples of an object and it can be recognized.</p>
<div class="figure align-default" id="id64">
<a class="reference internal image-reference" href="../_images/objrecog.png"><img alt="../_images/objrecog.png" src="../_images/objrecog.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.67 </span><span class="caption-text">Object recognition on ImageNet. Source <span id="id26">[<a class="reference internal" href="../zreferences.html#id111">Krizhevsky et al., 2012</a>]</span>.</span><a class="headerlink" href="#id64" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="facial-recognition">
<h3><span class="section-number">3.3.2. </span>Facial recognition<a class="headerlink" href="#facial-recognition" title="Permalink to this headline">¶</a></h3>
<p>Facebook used 4.4 million annotated faces from 4030 users to train <strong>DeepFace</strong> <span id="id27">[<a class="reference internal" href="../zreferences.html#id187">Taigman et al., 2014</a>]</span>. Accuracy of 97.35% for recognizing faces, on par with humans. Used now to recognize new faces from single examples (transfer learning, one-shot learning).</p>
<div class="figure align-default" id="id65">
<a class="reference internal image-reference" href="../_images/deepface.png"><img alt="../_images/deepface.png" src="../_images/deepface.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.68 </span><span class="caption-text">DeepFace. Source <span id="id28">[<a class="reference internal" href="../zreferences.html#id187">Taigman et al., 2014</a>]</span>.</span><a class="headerlink" href="#id65" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="pose-estimation">
<h3><span class="section-number">3.3.3. </span>Pose estimation<a class="headerlink" href="#pose-estimation" title="Permalink to this headline">¶</a></h3>
<p><strong>PoseNet</strong> <span id="id29">[<a class="reference internal" href="../zreferences.html#id100">Kendall et al., 2016</a>]</span> is a Inception-based CNN able to predict 3D information from 2D images. It can be for example the calibration matrix of a camera, 3D coordinates of joints or facial features. There is a free tensorflow.js implementation that can be used in the browser.</p>
<div class="figure align-default" id="id66">
<a class="reference internal image-reference" href="../_images/posnet-face.gif"><img alt="../_images/posnet-face.gif" src="../_images/posnet-face.gif" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.69 </span><span class="caption-text">PoseNet <span id="id30">[<a class="reference internal" href="../zreferences.html#id100">Kendall et al., 2016</a>]</span>. Source:  <a class="reference external" href="https://blog.tensorflow.org/2019/01/tensorflow-lite-now-faster-with-mobile.html">https://blog.tensorflow.org/2019/01/tensorflow-lite-now-faster-with-mobile.html</a></span><a class="headerlink" href="#id66" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="speech-recognition">
<h3><span class="section-number">3.3.4. </span>Speech recognition<a class="headerlink" href="#speech-recognition" title="Permalink to this headline">¶</a></h3>
<p>To perform speech recognition, one could treat speech signals like images: one direction is time, the other are frequencies (e.g. mel spectrum). A CNN can learn to associate phonemes to the corresponding signal. <strong>DeepSpeech</strong> <span id="id31">[<a class="reference internal" href="../zreferences.html#id68">Hannun et al., 2014</a>]</span> from Baidu is one of the state-of-the-art approaches. Convolutional networks can be used on any signals where early features are local. It uses additionally recurrent networks, which we will see later.</p>
<div class="figure align-default" id="id67">
<a class="reference internal image-reference" href="../_images/deepspeech.png"><img alt="../_images/deepspeech.png" src="../_images/deepspeech.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.70 </span><span class="caption-text">DeepSpeech. Source <span id="id32">[<a class="reference internal" href="../zreferences.html#id68">Hannun et al., 2014</a>]</span>.</span><a class="headerlink" href="#id67" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="sentiment-analysis">
<h3><span class="section-number">3.3.5. </span>Sentiment analysis<a class="headerlink" href="#sentiment-analysis" title="Permalink to this headline">¶</a></h3>
<p>It is also possible to apply convolutions on text. <strong>Sentiment analysis</strong> assigns a positive or negative judgment to sentences. Each word is represented by a vector of values (word2vec). The convolutional layer can slide over all over words to find out the sentiment of the sentence.</p>
<div class="figure align-default" id="id68">
<a class="reference internal image-reference" href="../_images/sentimentanalysis.png"><img alt="../_images/sentimentanalysis.png" src="../_images/sentimentanalysis.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.71 </span><span class="caption-text">Sentiment analysis. Source <span id="id33">[<a class="reference internal" href="../zreferences.html#id102">Kim, 2014</a>]</span>.</span><a class="headerlink" href="#id68" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="wavenet-text-to-speech-synthesis">
<h3><span class="section-number">3.3.6. </span>Wavenet : text-to-speech synthesis<a class="headerlink" href="#wavenet-text-to-speech-synthesis" title="Permalink to this headline">¶</a></h3>
<p><strong>Text-To-Speech</strong> (TTS) is also possible using CNNs. Google Home relies on <strong>Wavenet</strong> <span id="id34">[<a class="reference internal" href="../zreferences.html#id144">Oord et al., 2016</a>]</span>, a complex CNN using <em>dilated convolutions</em> to grasp long-term dependencies.</p>
<div class="figure align-default" id="id69">
<a class="reference internal image-reference" href="../_images/wavenet.png"><img alt="../_images/wavenet.png" src="../_images/wavenet.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.72 </span><span class="caption-text">Wavenet <span id="id35">[<a class="reference internal" href="../zreferences.html#id144">Oord et al., 2016</a>]</span>.  Source: <a class="reference external" href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a></span><a class="headerlink" href="#id69" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id70">
<a class="reference internal image-reference" href="../_images/wavenet-dilated.gif"><img alt="../_images/wavenet-dilated.gif" src="../_images/wavenet-dilated.gif" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.73 </span><span class="caption-text">Dilated convolutions in Wavenet <span id="id36">[<a class="reference internal" href="../zreferences.html#id144">Oord et al., 2016</a>]</span>.  Source: <a class="reference external" href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a></span><a class="headerlink" href="#id70" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="transfer-learning">
<h2><span class="section-number">3.4. </span>Transfer learning<a class="headerlink" href="#transfer-learning" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/12ohNYgDHvY' frameborder='0' allowfullscreen></iframe></div>
<p><strong>Myth:</strong> ones needs at least one million labeled examples to use deep learning. This is true if you train the CNN <strong>end-to-end</strong> with randomly initialized weights. But there are alternatives:</p>
<ol class="simple">
<li><p><strong>Unsupervised learning</strong> (autoencoders) may help extract useful representations using only images.</p></li>
<li><p><strong>Transfer learning</strong> allows to re-use weights obtained from a related task/domain.</p></li>
</ol>
<div class="figure align-default" id="id71">
<a class="reference internal image-reference" href="../_images/transferlearning2.jpg"><img alt="../_images/transferlearning2.jpg" src="../_images/transferlearning2.jpg" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.74 </span><span class="caption-text">Transfer learning.  Source: <a class="reference external" href="http://imatge-upc.github.io/telecombcn-2016-dlcv">http://imatge-upc.github.io/telecombcn-2016-dlcv</a></span><a class="headerlink" href="#id71" title="Permalink to this image">¶</a></p>
</div>
<p>Take a classical network (VGG-16, Inception, ResNet, etc.) trained on ImageNet (if your task is object recognition).</p>
<p><strong>Off-the-shelf</strong></p>
<ul class="simple">
<li><p>Cut the network before the last layer and use directly the high-level feature representation.</p></li>
<li><p>Use a shallow classifier directly on these representations (not obligatorily NN).</p></li>
</ul>
<div class="figure align-default" id="id72">
<a class="reference internal image-reference" href="../_images/transferlearning3.jpg"><img alt="../_images/transferlearning3.jpg" src="../_images/transferlearning3.jpg" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.75 </span><span class="caption-text">Off-the-shelf transfer learning.  Source: <a class="reference external" href="http://imatge-upc.github.io/telecombcn-2016-dlcv">http://imatge-upc.github.io/telecombcn-2016-dlcv</a></span><a class="headerlink" href="#id72" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Fine-tuning</strong></p>
<ul class="simple">
<li><p>Use the trained weights as initial weight values and re-train the network on your data (often only the last layers, the early ones are frozen).</p></li>
</ul>
<div class="figure align-default" id="id73">
<a class="reference internal image-reference" href="../_images/transferlearning4.jpg"><img alt="../_images/transferlearning4.jpg" src="../_images/transferlearning4.jpg" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.76 </span><span class="caption-text">Fine-tuned transfer learning.  Source: <a class="reference external" href="http://imatge-upc.github.io/telecombcn-2016-dlcv">http://imatge-upc.github.io/telecombcn-2016-dlcv</a></span><a class="headerlink" href="#id73" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition-example-of-transfer-learning admonition">
<p class="admonition-title">Example of transfer learning</p>
<p><img alt="" src="../_images/snowleopard.png" /></p>
<p>Microsoft wanted a system to automatically detect <strong>snow leopards</strong> into the wild, but there were not enough labelled images to train a deep network <strong>end-to-end</strong>. They used a pretrained <strong>ResNet50</strong> as a feature extractor for a simple <strong>logistic regression</strong> classifier.</p>
<p>Source: <a class="reference external" href="https://blogs.technet.microsoft.com/machinelearning/2017/06/27/saving-snow-leopards-with-deep-learning-and-computer-vision-on-spark/">https://blogs.technet.microsoft.com/machinelearning/2017/06/27/saving-snow-leopards-with-deep-learning-and-computer-vision-on-spark/</a></p>
</div>
<div class="admonition-transfer-learning-in-keras admonition">
<p class="admonition-title">Transfer learning in keras</p>
<p>Keras provides pre-trained CNNs that can be used as feature extractors:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tf.keras.applications.vgg16</span> <span class="kn">import</span> <span class="n">VGG16</span>

<span class="c1"># Download VGG without the FC layers</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">VGG16</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
              <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Freeze learning in VGG16</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
	<span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Add a fresh MLP on top</span>
<span class="n">flat1</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
<span class="n">class1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">flat1</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">class1</span><span class="p">)</span>

<span class="c1"># New model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://keras.io/api/applications/">https://keras.io/api/applications/</a> for the full list of pretrained networks.</p>
</div>
</div>
<div class="section" id="ensemble-learning">
<h2><span class="section-number">3.5. </span>Ensemble learning<a class="headerlink" href="#ensemble-learning" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/WVNqiUnHRPA' frameborder='0' allowfullscreen></iframe></div>
<p>Since 2016, only ensembles of existing networks win the competitions.</p>
<div class="figure align-default" id="id74">
<a class="reference internal image-reference" href="../_images/ensemble-scores.png"><img alt="../_images/ensemble-scores.png" src="../_images/ensemble-scores.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.77 </span><span class="caption-text">Top networks on the ImageNet 2016 competition. Source <a class="reference external" href="http://image-net.org/challenges/LSVRC/2016/results">http://image-net.org/challenges/LSVRC/2016/results</a></span><a class="headerlink" href="#id74" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Ensemble learning</strong> is the process of combining multiple independent classifiers together, in order to obtain a better performance. As long the individual classifiers do not make mistakes for the same examples, a simple majority vote might be enough to get better approximations.</p>
<div class="figure align-default" id="id75">
<a class="reference internal image-reference" href="../_images/ensemble-example.png"><img alt="../_images/ensemble-example.png" src="../_images/ensemble-example.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.78 </span><span class="caption-text">Ensemble of pre-trained CNNs. Source <a class="reference external" href="https://flyyufelix.github.io/2017/04/16/kaggle-nature-conservancy.html">https://flyyufelix.github.io/2017/04/16/kaggle-nature-conservancy.html</a></span><a class="headerlink" href="#id75" title="Permalink to this image">¶</a></p>
</div>
<p>Let’s consider we have three <strong>independent</strong> binary classifiers, each with an accuracy of 70% (P = 0.7 of being correct). When using a majority vote, we get the following cases:</p>
<ol>
<li><p>all three models are correct:</p>
<p>P = 0.7 * 0.7 * 0.7 = 0.3492</p>
</li>
<li><p>two models are correct</p>
<p>P = (0.7 * 0.7 * 0.3) + (0.7 * 0.3 * 0.7) + (0.3 * 0.7 * 0.7) = 0.4409</p>
</li>
<li><p>two models are wrong</p>
<p>P = (0.3 * 0.3 * 0.7) + (0.3 * 0.7 * 0.3) + (0.7 * 0.3 * 0.3) = 0.189</p>
</li>
<li><p>all three models are wrong</p>
<p>P = 0.3 * 0.3 * 0.3 = 0.027</p>
</li>
</ol>
<p>The majority vote is correct with a probability of P = 0.3492 + 0.4409 = <strong>0.78 !</strong> The individual learners only have to be slightly better than chance, but they <strong>must</strong> be as independent as possible.</p>
<div class="section" id="bagging">
<h3><span class="section-number">3.5.1. </span>Bagging<a class="headerlink" href="#bagging" title="Permalink to this headline">¶</a></h3>
<p>Bagging methods (bootstrap aggregation) trains multiple classifiers on randomly sampled subsets of the data.
A <strong>random forest</strong> is for example a bagging method for decision trees, where the data and features are sampled.. One can use majority vote, unweighted average, weighted average or even a meta-learner to form the final decision.</p>
<div class="figure align-default" id="id76">
<a class="reference internal image-reference" href="../_images/bagging.jpg"><img alt="../_images/bagging.jpg" src="../_images/bagging.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.79 </span><span class="caption-text">Bagging. Source: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0957417409008781">http://www.sciencedirect.com/science/article/pii/S0957417409008781</a></span><a class="headerlink" href="#id76" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="boosting">
<h3><span class="section-number">3.5.2. </span>Boosting<a class="headerlink" href="#boosting" title="Permalink to this headline">¶</a></h3>
<p>Bagging algorithms aim to reduce the complexity of models that overfit the training data. <strong>Boosting</strong> is an approach to increase the complexity of models that suffer from high bias, that is, models that underfit the training data. Algorithms: Adaboost, XGBoost (gradient boosting)…</p>
<div class="figure align-default" id="id77">
<a class="reference internal image-reference" href="../_images/boosting.png"><img alt="../_images/boosting.png" src="../_images/boosting.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.80 </span><span class="caption-text">Boosting. Source:  <a class="reference external" href="https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/">https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/</a></span><a class="headerlink" href="#id77" title="Permalink to this image">¶</a></p>
</div>
<p>Boosting is not very useful with deep networks (overfitting), but there are some approaches like SelfieBoost (<a class="reference external" href="https://arxiv.org/pdf/1411.3436.pdf">https://arxiv.org/pdf/1411.3436.pdf</a>).</p>
</div>
<div class="section" id="stacking">
<h3><span class="section-number">3.5.3. </span>Stacking<a class="headerlink" href="#stacking" title="Permalink to this headline">¶</a></h3>
<p><strong>Stacking</strong> is an ensemble learning technique that combines multiple models via a meta-classifier. The meta-model is trained on the outputs of the basic models as features. Winning approach of ImageNet 2016 and 2017. See <a class="reference external" href="https://blog.statsbot.co/ensemble-learning-d1dcd548e936">https://blog.statsbot.co/ensemble-learning-d1dcd548e936</a></p>
<div class="figure align-default" id="id78">
<a class="reference internal image-reference" href="../_images/stacking.png"><img alt="../_images/stacking.png" src="../_images/stacking.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.81 </span><span class="caption-text">Stacking. Source: <a class="reference external" href="doi:10.1371/journal.pone.0024386.g005">doi:10.1371/journal.pone.0024386.g005</a></span><a class="headerlink" href="#id78" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3-deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="2-DNN.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2. </span>Deep neural networks</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="4-ObjectDetection.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Object detection</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>