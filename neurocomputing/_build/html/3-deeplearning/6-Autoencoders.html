
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6. Autoencoders &#8212; Neurocomputing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/6-Autoencoders.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Restricted Boltzmann machines (optional)" href="7-RBM.html" />
    <link rel="prev" title="5. Semantic segmentation" href="5-SemanticSegmentation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tuc.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neurocomputing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-Attention.html">
   10. Attentional neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3-deeplearning/6-Autoencoders.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   6.1. Autoencoders
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacked-autoencoders">
   6.2. Stacked autoencoders
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-autoencoders">
   6.3. Deep autoencoders
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#semi-supervised-learning">
     6.3.1. Semi-supervised learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#denoising-autoencoders">
     6.3.2. Denoising autoencoders
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-clustering">
     6.3.3. Deep clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variational-autoencoders-vae">
   6.4. Variational autoencoders (VAE)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#motivation">
     6.4.1. Motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#architecture">
     6.4.2. Architecture
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-function-of-a-vae">
     6.4.3. Loss function of a VAE
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reparameterization-trick">
     6.4.4. Reparameterization trick
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     6.4.5. Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#advanced-vae">
     6.4.6. Advanced VAE
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#deepfake">
       6.4.6.1. DeepFake
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#beta-vae">
       6.4.6.2.
       <span class="math notranslate nohighlight">
        \(\beta\)
       </span>
       -VAE
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vq-vae">
       6.4.6.3. VQ-VAE
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#conditional-variational-autoencoder-cvae">
       6.4.6.4. Conditional variational autoencoder (CVAE)
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variational-inference-optional">
   6.5. Variational inference (optional)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-probability-distributions-from-samples">
     6.5.1. Learning probability distributions from samples
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#latent-space">
     6.5.2. Latent space
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variational-inference">
     6.5.3. Variational inference
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="autoencoders">
<h1><span class="section-number">6. </span>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/3.6-Autoencoders.pdf">pdf</a></p>
<div class="section" id="id1">
<h2><span class="section-number">6.1. </span>Autoencoders<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/bW2jYMdaPZk' frameborder='0' allowfullscreen></iframe></div>
<p><strong>Supervised learning</strong> algorithms need a lot of labeled data (with <span class="math notranslate nohighlight">\(\mathbf{t}\)</span>) in order to learn classification/regression tasks, but labeled data is very expensive to obtain (experts, crowd sourcing). A “bad” algorithm trained with a lot of data will perform better than a “good” algorithm trained with few data.</p>
<blockquote>
<div><p>“It is not who has the best algorithm who wins, it is who has the most data.”</p>
</div></blockquote>
<p>Unlabeled data is only useful for <strong>unsupervised learning</strong>, but very cheap to obtain (camera, microphone, search engines). Can we combine efficiently both approaches? <strong>Self-taught learning</strong> or <strong>semi-supervised learning</strong>.</p>
<p>An <strong>autoencoder</strong> is a NN trying to learn the identity function <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{x}\)</span> using a different number of neurons in the hidden layer than in the input layer.</p>
<div class="figure align-default" id="id14">
<a class="reference internal image-reference" href="../_images/autoencoder3.png"><img alt="../_images/autoencoder3.png" src="../_images/autoencoder3.png" style="width: 35%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.15 </span><span class="caption-text">Architecture of a shallow autoencoder.</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p>An autoencoder minimizes the <strong>reconstruction loss</strong> between the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and the reconstruction <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span>, for example the mse between the two vectors:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}_\text{reconstruction}(\theta) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}} [ ||\mathbf{x'} - \mathbf{x}||^2 ]
\]</div>
<p>An autoencoder uses <strong>unsupervised learning</strong>: the output data used for learning is the same as the input data: No need for labels!</p>
<p>By forcing the projection of the input data on a feature space with less dimensions (<strong>latent space</strong>), the network has to extract relevant <strong>features</strong> from the training data: Dimensionality reduction, compression.</p>
<p>If the latent space has more dimensions than the input space, we need to <strong>constrain</strong> the autoencoder so that it does not simply learn the identity mapping. Below is an example of a sparse autoencoder trained on natural images <span id="id2">[<a class="reference internal" href="../zreferences.html#id143">Olshausen &amp; Field, 1997</a>]</span>.</p>
<div class="figure align-default" id="id15">
<a class="reference internal image-reference" href="../_images/result-autoencoder.png"><img alt="../_images/result-autoencoder.png" src="../_images/result-autoencoder.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.16 </span><span class="caption-text">Sparse autoencoder trained on natural images <span id="id3">[<a class="reference internal" href="../zreferences.html#id143">Olshausen &amp; Field, 1997</a>]</span>.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p>Inputs are taken from random natural images and cut in 10*10 patches. 100 features are extracted in the hidden layer. The autoencoder is said <strong>sparse</strong> because it uses <strong>L1-regularization</strong> to make sure that only a few neurons are active in the hidden layer for a particular image. The learned features look like what the first layer of a CNN would learn, except that there was no labels at all! Can we take advantage of this to pre-train a supervised network?</p>
</div>
<div class="section" id="stacked-autoencoders">
<h2><span class="section-number">6.2. </span>Stacked autoencoders<a class="headerlink" href="#stacked-autoencoders" title="Permalink to this headline">¶</a></h2>
<p>In supervised learning, deep neural networks suffer from many problems: local minima, vanishing gradients, long training times… All these problems are due to the fact that the weights are randomly initialized at the beginning of training. <strong>Pretraining</strong> the weights using unsupervised learning allows to start already close to a good solution: the network will need less steps to converge, the gradients will vanish less and less data will be needed to learn a particular supervised task.</p>
<p>Let’s try to learn a <strong>stacked autoencoder</strong> by learning <em>progressively</em> each feature vector.</p>
<div class="figure align-default" id="id16">
<a class="reference internal image-reference" href="../_images/stacked-autoencoder.png"><img alt="../_images/stacked-autoencoder.png" src="../_images/stacked-autoencoder.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.17 </span><span class="caption-text">Architecture of the stacked autoencoder. Source: <a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a>.</span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
<p>Using unlabeled data, train an autoencoder to extract first-order features, freeze the weights and remove the decoder.</p>
<div class="figure align-default" id="id17">
<a class="reference internal image-reference" href="../_images/stacked1.png"><img alt="../_images/stacked1.png" src="../_images/stacked1.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.18 </span><span class="caption-text">The first layer is trained using an autoencoder on the inputs. Source: <a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a>.</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<p>Train another autoencoder on the same unlabeled data, but using the previous latent space as input/output.</p>
<div class="figure align-default" id="id18">
<a class="reference internal image-reference" href="../_images/stacked2.png"><img alt="../_images/stacked2.png" src="../_images/stacked2.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.19 </span><span class="caption-text">The second layer is trained using an autoencoder on the first layer. Source: <a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a>.</span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
<p>Repeat the operation as often as needed, and finish with a simple classifier using the labeled data.</p>
<div class="figure align-default" id="id19">
<a class="reference internal image-reference" href="../_images/stacked3.png"><img alt="../_images/stacked3.png" src="../_images/stacked3.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.20 </span><span class="caption-text">The output layer is trained using supervised learning on the last hidden layer. Source: <a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a>.</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<p>This defines a <strong>stacked autoencoder</strong>, trained using <strong>Greedy layer-wise</strong> learning. Each layer progressively learns more and more complex features of the input data (edges - contour - forms - objects): <strong>feature extraction</strong>. This method allows to train a deep network on few labeled data: the network will not overfit, because the weights are already in the right region. It solves <strong>gradient vanishing</strong>, as the weights are already close to the optimal solution and will efficiently transmit the gradient backwards. One can keep the pre-trained weights fixed for the classification task or <strong>fine-tune</strong> all the weights as in a regular DNN.</p>
</div>
<div class="section" id="deep-autoencoders">
<h2><span class="section-number">6.3. </span>Deep autoencoders<a class="headerlink" href="#deep-autoencoders" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/daBrY1dmvn0' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="semi-supervised-learning">
<h3><span class="section-number">6.3.1. </span>Semi-supervised learning<a class="headerlink" href="#semi-supervised-learning" title="Permalink to this headline">¶</a></h3>
<p>Autoencoders are not restricted to a single hidden layer.</p>
<ul class="simple">
<li><p>The <strong>encoder</strong> goes from the input space <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to the latent space <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathbf{z} = g_\phi(\mathbf{x})
\]</div>
<ul class="simple">
<li><p>The <strong>decoder</strong> goes from the latent space <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> to the output space <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathbf{x'} = f_\theta(\mathbf{z})
\]</div>
<div class="figure align-default" id="id20">
<a class="reference internal image-reference" href="../_images/autoencoder-architecture.png"><img alt="../_images/autoencoder-architecture.png" src="../_images/autoencoder-architecture.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.21 </span><span class="caption-text">Deep autoencoder. Source: <a class="reference external" href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html</a>.</span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</div>
<p>The <strong>latent space</strong> is a <strong>bottleneck</strong> layer of lower dimensionality, learning a compressed representation of the input which has to contain enough information in order to <strong>reconstruct</strong> the input. Both the encoder with weights <span class="math notranslate nohighlight">\(\phi\)</span> and the decoder with weights <span class="math notranslate nohighlight">\(\theta\)</span> try to minimize the <strong>reconstruction loss</strong>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_\text{reconstruction}(\theta, \phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}} [ ||f_\theta(g_\phi(\mathbf{x})) - \mathbf{x}||^2 ]
\]</div>
<p>Learning is <strong>unsupervised</strong>: we only need input data.</p>
<p>The encoder and decoder can be anything: fully-connected, convolutional, recurrent, etc. When using convolutional layers, the decoder has to <strong>upsample</strong> the latent space: max-unpooling or transposed convolutions can be used as in segmentation networks.</p>
<div class="figure align-default" id="id21">
<a class="reference internal image-reference" href="../_images/convolutionalAE.png"><img alt="../_images/convolutionalAE.png" src="../_images/convolutionalAE.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.22 </span><span class="caption-text">Deep convolutional autoencoder. Source: <span id="id4">[<a class="reference internal" href="../zreferences.html#id62">Guo et al., 2017</a>]</span>.</span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</div>
<p>In <strong>semi-supervised</strong> or <strong>self-taught</strong> learning, we can first train an autoencoder on huge amounts of unlabeled data, and then use the latent representations as an input to a shallow classifier on a small supervised dataset.</p>
<div class="figure align-default" id="id22">
<a class="reference internal image-reference" href="../_images/semisupervised-autoencoder.png"><img alt="../_images/semisupervised-autoencoder.png" src="../_images/semisupervised-autoencoder.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.23 </span><span class="caption-text">The encoder of an unsupervised autoencoder can be used as a feature extractor for a classifier. Source: <a class="reference external" href="https://doi.org/10.1117/12.2303912">https://doi.org/10.1117/12.2303912</a>.</span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</div>
<p>A linear classifier might even be enough if the latent space is well trained. The weights of the encoder can be fine-tuned with backpropagation, or remain fixed.</p>
</div>
<div class="section" id="denoising-autoencoders">
<h3><span class="section-number">6.3.2. </span>Denoising autoencoders<a class="headerlink" href="#denoising-autoencoders" title="Permalink to this headline">¶</a></h3>
<p>A <strong>denoising autoencoder</strong> (DAE, <span id="id5">[<a class="reference internal" href="../zreferences.html#id191">Vincent et al., 2010</a>]</span>) is trained with noisy inputs (some pixels are dropped) but perfect desired outputs. It learns to suppress that noise.</p>
<div class="figure align-default" id="id23">
<a class="reference internal image-reference" href="../_images/denoisingautoencoder.png"><img alt="../_images/denoisingautoencoder.png" src="../_images/denoisingautoencoder.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.24 </span><span class="caption-text">Denoising autoencoder. Source:  <a class="reference external" href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html</a>.</span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id24">
<a class="reference internal image-reference" href="../_images/denoisingautoencoder-result.png"><img alt="../_images/denoisingautoencoder-result.png" src="../_images/denoisingautoencoder-result.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.25 </span><span class="caption-text">Denoising autoencoder. Source:  <a class="reference external" href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html</a>.</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="deep-clustering">
<h3><span class="section-number">6.3.3. </span>Deep clustering<a class="headerlink" href="#deep-clustering" title="Permalink to this headline">¶</a></h3>
<p><strong>Clustering</strong> algorithms (k-means, Gaussian Mixture Models, spectral clustering, etc) can be applied in the latent space to group data points into clusters. If you are lucky, the clusters may even correspond to classes.</p>
<div class="figure align-default" id="id25">
<a class="reference internal image-reference" href="../_images/deepclustering.jpg"><img alt="../_images/deepclustering.jpg" src="../_images/deepclustering.jpg" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.26 </span><span class="caption-text">Clustering can be applied on the latent representations. Source: <a class="reference external" href="doi:10.1007/978-3-030-32520-6_55">doi:10.1007/978-3-030-32520-6_55</a>.</span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="variational-autoencoders-vae">
<h2><span class="section-number">6.4. </span>Variational autoencoders (VAE)<a class="headerlink" href="#variational-autoencoders-vae" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/XhBI14qOzXg' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="motivation">
<h3><span class="section-number">6.4.1. </span>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h3>
<p>Autoencoders are <strong>deterministic</strong>: after learning, the same input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> will generate the same latent code <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> and the same reconstruction <span class="math notranslate nohighlight">\(\mathbf{\tilde{x}}\)</span>. Sampling the latent space generally generates non-sense reconstructions, because an autoencoder only learns data samples, it does not learn the underlying <strong>probability distribution</strong>.</p>
<div class="figure align-default" id="id26">
<a class="reference internal image-reference" href="../_images/autoencoder-limits.png"><img alt="../_images/autoencoder-limits.png" src="../_images/autoencoder-limits.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.27 </span><span class="caption-text">Deterministic autoencoders do not regularize their latent space. Source: <a class="reference external" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a>.</span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
<p>The main problem of supervised learning is to get enough annotated data. Being able to generate <strong>new</strong> images similar to the training examples would be extremely useful (data augmentation).</p>
<p>In order for this to work, we need to <strong>regularize</strong> the latent space: Close points in the latent space should correspond to close images.  “Classical” L1 or L2 regularization does not ensure the regularity of the latent space.</p>
<div class="figure align-default" id="id27">
<a class="reference internal image-reference" href="../_images/vae-latentspace.png"><img alt="../_images/vae-latentspace.png" src="../_images/vae-latentspace.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.28 </span><span class="caption-text">Variational autoencoders do regularize their latent space. Source: <a class="reference external" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a>.</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="architecture">
<h3><span class="section-number">6.4.2. </span>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h3>
<p>The <strong>variational autoencoder</strong> (VAE) <span id="id6">[<a class="reference internal" href="../zreferences.html#id105">Kingma &amp; Welling, 2013</a>]</span> solves this problem by having the encoder represent the <strong>probability distribution</strong> <span class="math notranslate nohighlight">\(q_\phi(\mathbf{z}|\mathbf{x})\)</span> instead of a point <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> in the latent space.</p>
<p>This probability distribution is then <strong>sampled</strong> to obtain a vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> that will be passed to the decoder <span class="math notranslate nohighlight">\(p_\theta(\mathbf{z})\)</span>. The strong hypothesis is that the latent space follows a <strong>normal distribution</strong> with mean <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> and variance <span class="math notranslate nohighlight">\(\mathbf{\sigma_x}^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    \mathbf{z} \sim \mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)
\]</div>
<p>The two vectors <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\sigma_x}^2\)</span> are the outputs of the encoder.</p>
<div class="admonition-sampling-from-a-normal-distribution admonition">
<p class="admonition-title">Sampling from a normal distribution</p>
<div class="figure align-default" id="id28">
<a class="reference internal image-reference" href="../_images/normaldistribution.svg"><img alt="../_images/normaldistribution.svg" src="../_images/normaldistribution.svg" width="70%" /></a>
<p class="caption"><span class="caption-number">Fig. 6.29 </span><span class="caption-text">Sampling from a normal distribution.</span><a class="headerlink" href="#id28" title="Permalink to this image">¶</a></p>
</div>
<p>The normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span> is fully defined by its two parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> is the mean of the distribution.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma^2\)</span> is its variance.</p></li>
</ul>
<p>The <strong>probability density function</strong> (pdf) of the normal distribution is defined by the Gaussian function:</p>
<div class="math notranslate nohighlight">
\[
    f(x; \mu, \sigma) = \frac{1}{\sqrt{2\,\pi\,\sigma^2}} \, e^{-\displaystyle\frac{(x - \mu)^2}{2\,\sigma^2}}
\]</div>
<p>A sample <span class="math notranslate nohighlight">\(x\)</span> will likely be close to <span class="math notranslate nohighlight">\(\mu\)</span>, with a deviation defined by <span class="math notranslate nohighlight">\(\sigma^2\)</span>. It can be obtained using a sample of the <strong>standard normal distribution</strong> <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[x = \mu + \sigma \, \xi \; \; \text{with} \; \xi \sim \mathcal{N}(0, 1)\]</div>
</div>
<p>Architecture of the VAE:</p>
<ol class="simple">
<li><p>The encoder <span class="math notranslate nohighlight">\(q_\phi(\mathbf{z}|\mathbf{x})\)</span> outputs the parameters <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\sigma_x}^2\)</span> of a normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)\)</span>.</p></li>
<li><p>We sample one vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> from this distribution: <span class="math notranslate nohighlight">\(\mathbf{z} \sim \mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)\)</span>.</p></li>
<li><p>The decoder <span class="math notranslate nohighlight">\(p_\theta(\mathbf{z})\)</span> reconstructs the input.</p></li>
</ol>
<div class="figure align-default" id="id29">
<a class="reference internal image-reference" href="../_images/vae-structure.png"><img alt="../_images/vae-structure.png" src="../_images/vae-structure.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.30 </span><span class="caption-text">Architecture of a variational autoencoder. Source:  <a class="reference external" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></span><a class="headerlink" href="#id29" title="Permalink to this image">¶</a></p>
</div>
<p>Open questions:</p>
<ol class="simple">
<li><p>Which loss should we use and how do we regularize?</p></li>
<li><p>Does backpropagation still work?</p></li>
</ol>
</div>
<div class="section" id="loss-function-of-a-vae">
<h3><span class="section-number">6.4.3. </span>Loss function of a VAE<a class="headerlink" href="#loss-function-of-a-vae" title="Permalink to this headline">¶</a></h3>
<p>The <strong>loss function</strong> used in a VAE is of the form:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta, \phi) = \mathcal{L}_\text{reconstruction}(\theta, \phi) + \mathcal{L}_\text{regularization}(\phi)
\]</div>
<p>The first term is the usual <strong>reconstruction loss</strong> of an autoencoder which depends on both the encoder and the decoder. One could simply compute the <strong>mse</strong> (summed over all pixels) between the input and the reconstruction:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}_\text{reconstruction}(\theta, \phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} [ ||p_\theta(\mathbf{z}) - \mathbf{x}||^2 ]\]</div>
<p>In the expectation, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is sampled from the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> while <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is sampled from the encoder <span class="math notranslate nohighlight">\(q_\phi(\mathbf{z}|\mathbf{x})\)</span>. In <span id="id7">[<a class="reference internal" href="../zreferences.html#id105">Kingma &amp; Welling, 2013</a>]</span>, pixels values are normalized between 0 and 1, the decoder uses the logistic activation function for its output layer and the binary cross-entropy loss function is used:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}_\text{reconstruction}(\theta, \phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} [ - \log p_\theta(\mathbf{z})]\]</div>
<p>The justification comes from variational inference and evidence lower-bound optimization (ELBO) but is out of the scope of this lecture.</p>
<p>The second term is the <strong>regularization term</strong> for the latent space, which only depends on the encoder with weights <span class="math notranslate nohighlight">\(\phi\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}_\text{regularization}(\phi) = \text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || \mathcal{N}(\mathbf{0}, \mathbf{1})) = \text{KL}(\mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2) || \mathcal{N}(\mathbf{0}, \mathbf{1}))
\]</div>
<p>It is defined as the <strong>Kullback-Leibler divergence</strong> between the output of the encoder and the standard normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{0}, \mathbf{1})\)</span>. Think of it as a statistical “distance” between the distribution <span class="math notranslate nohighlight">\(q_\phi(\mathbf{z}|\mathbf{x})\)</span> and the distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{0}, \mathbf{1})\)</span>. The principle is not very different from L2-regularization, where we want the weights to be as close as possible from 0. Here we want the encoder to be as close as possible from <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{0}, \mathbf{1})\)</span>.</p>
<p>Why do we want the latent distributions to be close from <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{0}, \mathbf{1})\)</span> for <strong>all</strong> inputs <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>?
$<span class="math notranslate nohighlight">\(
    \mathcal{L}(\theta, \phi) = \mathcal{L}_\text{reconstruction}(\theta, \phi) + \text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || \mathcal{N}(\mathbf{0}, \mathbf{1}))
\)</span>$</p>
<p>By forcing the distributions to be close, we avoid “holes” in the latent space: we can move smoothly from one distribution to another without generating <strong>non-sense</strong> reconstructions.</p>
<div class="figure align-default" id="id30">
<a class="reference internal image-reference" href="../_images/vae-regularization.png"><img alt="../_images/vae-regularization.png" src="../_images/vae-regularization.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.31 </span><span class="caption-text">Regularizing the latent space by minimizing the KL with <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> makes sure that we can smoothly travel in the latent space. Source:  <a class="reference external" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></span><a class="headerlink" href="#id30" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To make <span class="math notranslate nohighlight">\(q_\phi(\mathbf{z}|\mathbf{x})\)</span> close from <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{0}, \mathbf{1})\)</span>, one could minimize instead the Euclidian distance in the <strong>parameter space</strong>:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta, \phi) = \mathcal{L}_\text{reconstruction}(\theta, \phi) +  (||\mathbf{\mu_x}||^2 + ||\mathbf{\sigma_x} - 1||^2)
\]</div>
<p>However, this does not consider the <strong>overlap</strong> between the distributions. The two pairs of distributions below have the same distance between their means (0 and 1) and the same variance (1 and 10 respectively). The distributions on the left are very different from each other, but the distance in the parameter space is the same.</p>
<p><img alt="" src="../_images/naturalgradient.png" /></p>
</div>
<div class="admonition-kullback-leibler-divergence admonition">
<p class="admonition-title">Kullback-Leibler divergence</p>
<p>The <strong>KL divergence</strong> between two random distributions <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> measures the <strong>statistical distance</strong> between them. It describes, on average, how likely a sample from <span class="math notranslate nohighlight">\(X\)</span> could come from <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \text{KL}(X ||Y) = \mathbb{E}_{x \sim X}[- \log \frac{P(Y=x)}{P(X=x)}]
\]</div>
<p><img alt="" src="../_images/crossentropy.svg" /></p>
<p>When the two distributions are equal almost anywhere, the KL divergence is 0. Otherwise it is positive. <strong>Minimizing the KL divergence between two distributions makes them close in the statistical sense</strong>.</p>
</div>
<p>The advantage of minimizing the KL of <span class="math notranslate nohighlight">\(q_\phi(\mathbf{z}|\mathbf{x})\)</span> with <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> is that the KL takes a <strong>closed form</strong>, i.e. there is no need to compute the expectation over all possible latent representations <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}_\text{regularization}(\phi) = \text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || \mathcal{N}(\mathbf{0}, \mathbf{1})) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})}[- \log \frac{f_{0, 1}(\mathbf{z}|\mathbf{x})}{q_\phi(\mathbf{z}|\mathbf{x})}]
\]</div>
<p>If <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> and  <span class="math notranslate nohighlight">\(\mathbf{\sigma_x}\)</span> have <span class="math notranslate nohighlight">\(K\)</span> elements (dimension of the latent space), the KL can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}_\text{regularization}(\phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}}[\dfrac{1}{2} \, \sum_{k=1}^K (\mathbf{\sigma_x} + \mathbf{\mu_x}^2 -1 - \log \mathbf{\sigma_x})]
\]</div>
<p>The KL is very easy to differentiate w.r.t <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> and  <span class="math notranslate nohighlight">\(\mathbf{\sigma_x}\)</span>, i.e. w.r.t <span class="math notranslate nohighlight">\(\phi\)</span>! In practice, the encoder predicts the vectors <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> and <span class="math notranslate nohighlight">\(\Sigma_\mathbf{x} = \log \mathbf{\sigma_x}\)</span>, so the loss becomes:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}_\text{regularization}(\phi) = \dfrac{1}{2} \, \sum_{k=1}^K (\exp \Sigma_\mathbf{x} + \mathbf{\mu_x}^2 -1 - \Sigma_\mathbf{x})
\]</div>
<p>See <a class="reference external" href="https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/">https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/</a> for the proof.</p>
<p>Regularization tends to create a “gradient” over the information encoded in the latent space.  A point of the latent space sampled between the means of two encoded distributions should be decoded in an image in between the two training images.</p>
<div class="figure align-default" id="id31">
<a class="reference internal image-reference" href="../_images/vae-regularization2.png"><img alt="../_images/vae-regularization2.png" src="../_images/vae-regularization2.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.32 </span><span class="caption-text">A regularized latent space makes sure that reconstructions always make sense. Source:  <a class="reference external" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></span><a class="headerlink" href="#id31" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="reparameterization-trick">
<h3><span class="section-number">6.4.4. </span>Reparameterization trick<a class="headerlink" href="#reparameterization-trick" title="Permalink to this headline">¶</a></h3>
<p>The second problem is that backpropagation does not work through the sampling operation. It is easy to backpropagate the gradient of the loss function through the decoder until the sample <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. But how do you backpropagate to the outputs of the encoder: <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\sigma_x}\)</span>?</p>
<div class="figure align-default" id="id32">
<a class="reference internal image-reference" href="../_images/vae-structure.png"><img alt="../_images/vae-structure.png" src="../_images/vae-structure.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.33 </span><span class="caption-text">Architecture of a variational autoencoder. Source:  <a class="reference external" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></span><a class="headerlink" href="#id32" title="Permalink to this image">¶</a></p>
</div>
<p>Modifying slightly <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> or <span class="math notranslate nohighlight">\(\mathbf{\sigma_x}\)</span> may not change at all the sample <span class="math notranslate nohighlight">\(\mathbf{z} \sim \mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)\)</span>, so you cannot estimate any gradient.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathbf{z}}{\partial \mathbf{\mu_x}} = \; ?\]</div>
<p>Backpropagation does not work through a <strong>sampling</strong> operation, because it is not differentiable.</p>
<div class="math notranslate nohighlight">
\[\mathbf{z} \sim \mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)\]</div>
<p>The <strong>reparameterization trick</strong> consists in taking a sample <span class="math notranslate nohighlight">\(\xi\)</span> out of <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> and reconstruct <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> with:</p>
<div class="math notranslate nohighlight">
\[\mathbf{z} = \mathbf{\mu_x} + \mathbf{\sigma_x} \, \xi \qquad \text{with} \qquad \xi \sim \mathcal{N}(0, 1)\]</div>
<div class="figure align-default" id="id33">
<a class="reference internal image-reference" href="../_images/vae-reparameterization.png"><img alt="../_images/vae-reparameterization.png" src="../_images/vae-reparameterization.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.34 </span><span class="caption-text">Architecture of a variational autoencoder with the reparameterization trick. Source:  <a class="reference external" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></span><a class="headerlink" href="#id33" title="Permalink to this image">¶</a></p>
</div>
<p>The sampled value <span class="math notranslate nohighlight">\(\xi \sim \mathcal{N}(0, 1)\)</span> becomes just another input to the neural network.</p>
<div class="figure align-default" id="id34">
<a class="reference internal image-reference" href="../_images/vae-reparameterization2.png"><img alt="../_images/vae-reparameterization2.png" src="../_images/vae-reparameterization2.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.35 </span><span class="caption-text">Architecture of a variational autoencoder with the reparameterization trick. Source:  <a class="reference external" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></span><a class="headerlink" href="#id34" title="Permalink to this image">¶</a></p>
</div>
<p>It allows to transform <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\sigma_x}\)</span> into a sample <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> of <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{z} = \mathbf{\mu_x} + \mathbf{\sigma_x} \, \xi\]</div>
<p>We do not need to backpropagate through <span class="math notranslate nohighlight">\(\xi\)</span>, as there is no parameter to learn! The neural network becomes differentiable end-to-end, backpropagation will work.</p>
</div>
<div class="section" id="summary">
<h3><span class="section-number">6.4.5. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>A variational autoencoder is an autoencoder where the latent space represents a probability distribution <span class="math notranslate nohighlight">\(q_\phi(\mathbf{z} | \mathbf{x})\)</span> using the mean <span class="math notranslate nohighlight">\(\mathbf{\mu_x}\)</span> and standard deviation <span class="math notranslate nohighlight">\(\mathbf{\sigma_x}\)</span> of a normal distribution. The latent space can be sampled to generate new images using the decoder <span class="math notranslate nohighlight">\(p_\theta(\mathbf{z})\)</span>. KL regularization and the reparameterization trick are essential to VAE.</p>
<p>VAE loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mathcal{L}(\theta, \phi) &amp;= \mathcal{L}_\text{reconstruction}(\theta, \phi) + \mathcal{L}_\text{regularization}(\phi) \\
    &amp;= \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \xi \sim \mathcal{N}(0, 1)} [ - \log p_\theta(\mathbf{\mu_x} + \mathbf{\sigma_x} \, \xi) + \dfrac{1}{2} \, \sum_{k=1}^K (\mathbf{\sigma_x} + \mathbf{\mu_x}^2 -1 - \log \mathbf{\sigma_x})] \\
\end{aligned}\end{split}\]</div>
<div class="figure align-default" id="id35">
<a class="reference internal image-reference" href="../_images/vae-structure.svg"><img alt="../_images/vae-structure.svg" src="../_images/vae-structure.svg" width="70%" /></a>
<p class="caption"><span class="caption-number">Fig. 6.36 </span><span class="caption-text">Principle of a VAE. Source:  <a class="reference external" href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html">https://ijdykeman.github.io/ml/2016/12/21/cvae.html</a></span><a class="headerlink" href="#id35" title="Permalink to this image">¶</a></p>
</div>
<p>The two main applications of VAEs in <strong>unsupervised learning</strong> are:</p>
<ol class="simple">
<li><p><strong>Dimensionality reduction</strong>: projecting high dimensional data (images) onto a smaller space, for example a 2D space for visualization.</p></li>
<li><p><strong>Generative modeling</strong>: generating samples from the same distribution as the training data (data augmentation, deep fakes) by sampling on the manifold.</p></li>
</ol>
</div>
<div class="section" id="advanced-vae">
<h3><span class="section-number">6.4.6. </span>Advanced VAE<a class="headerlink" href="#advanced-vae" title="Permalink to this headline">¶</a></h3>
<div class="section" id="deepfake">
<h4><span class="section-number">6.4.6.1. </span>DeepFake<a class="headerlink" href="#deepfake" title="Permalink to this headline">¶</a></h4>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/JbzVhzNaTdI' frameborder='0' allowfullscreen></iframe></div>
<p>DeepFakes now became very easy, you can for example find DeepFace here: <a class="reference external" href="https://github.com/iperov/DeepFaceLab">https://github.com/iperov/DeepFaceLab</a>.</p>
<div class="figure align-default" id="id36">
<a class="reference internal image-reference" href="../_images/deepfakes_01.png"><img alt="../_images/deepfakes_01.png" src="../_images/deepfakes_01.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.37 </span><span class="caption-text">During training of a deepfake, <strong>one</strong> encoder and <strong>two</strong> decoders learns to reproduce the face of each person. Source:  <a class="reference external" href="https://www.alanzucconi.com/2018/03/14/understanding-the-technology-behind-deepfakes/">https://www.alanzucconi.com/2018/03/14/understanding-the-technology-behind-deepfakes/</a></span><a class="headerlink" href="#id36" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id37">
<a class="reference internal image-reference" href="../_images/deepfakes_02.png"><img alt="../_images/deepfakes_02.png" src="../_images/deepfakes_02.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.38 </span><span class="caption-text">When generating the deepfake, the decoder of person B is used on the latent representation of person A. Source:  <a class="reference external" href="https://www.alanzucconi.com/2018/03/14/understanding-the-technology-behind-deepfakes/">https://www.alanzucconi.com/2018/03/14/understanding-the-technology-behind-deepfakes/</a></span><a class="headerlink" href="#id37" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="beta-vae">
<h4><span class="section-number">6.4.6.2. </span><span class="math notranslate nohighlight">\(\beta\)</span>-VAE<a class="headerlink" href="#beta-vae" title="Permalink to this headline">¶</a></h4>
<p>VAE does not use a regularization parameter to balance the reconstruction and regularization losses. What happens if you do?</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mathcal{L}(\theta, \phi) &amp;= \mathcal{L}_\text{reconstruction}(\theta, \phi) + \beta \, \mathcal{L}_\text{regularization}(\phi) \\
    &amp;= \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \xi \sim \mathcal{N}(0, 1)} [ - \log p_\theta(\mathbf{\mu_x} + \mathbf{\sigma_x} \, \xi) + \dfrac{\beta}{2} \, \sum_{k=1}^K (\mathbf{\sigma_x} + \mathbf{\mu_x}^2 -1 - \log \mathbf{\sigma_x})] \\
\end{aligned}\end{split}\]</div>
<p>Using <span class="math notranslate nohighlight">\(\beta &gt; 1\)</span> puts emphasis on learning statistically independent latent factors.</p>
<p>The <span class="math notranslate nohighlight">\(\beta\)</span>-VAE <span id="id8">[<a class="reference internal" href="../zreferences.html#id75">Higgins et al., 2016</a>]</span> allows to <strong>disentangle</strong> the latent variables, i.e. manipulate them individually to vary only one aspect of the image (pose, color, gender, etc.).</p>
<div class="figure align-default" id="id38">
<a class="reference internal image-reference" href="../_images/betavae-results.png"><img alt="../_images/betavae-results.png" src="../_images/betavae-results.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.39 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\beta\)</span>-VAE trained on CelebA allows to disentangle skin color, age/gender or saturation by manipulating individual latent variables. <span id="id9">[<a class="reference internal" href="../zreferences.html#id75">Higgins et al., 2016</a>]</span>.</span><a class="headerlink" href="#id38" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://worldmodels.github.io/">https://worldmodels.github.io/</a> for a live demo in the RL context.</p>
</div>
</div>
<div class="section" id="vq-vae">
<h4><span class="section-number">6.4.6.3. </span>VQ-VAE<a class="headerlink" href="#vq-vae" title="Permalink to this headline">¶</a></h4>
<p>Deepmind researchers proposed VQ-VAE-2 <span id="id10">[<a class="reference internal" href="../zreferences.html#id155">Razavi et al., 2019</a>]</span>, a hierarchical VAE using vector-quantized priors able to generate high-resolution images.</p>
<div class="figure align-default" id="id39">
<a class="reference internal image-reference" href="../_images/vqvae.png"><img alt="../_images/vqvae.png" src="../_images/vqvae.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.40 </span><span class="caption-text">VQ-VAE-2 <span id="id11">[<a class="reference internal" href="../zreferences.html#id155">Razavi et al., 2019</a>]</span>.</span><a class="headerlink" href="#id39" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id40">
<a class="reference internal image-reference" href="../_images/vqvae-results.png"><img alt="../_images/vqvae-results.png" src="../_images/vqvae-results.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.41 </span><span class="caption-text">Faces generated by VQ-VAE-2 <span id="id12">[<a class="reference internal" href="../zreferences.html#id155">Razavi et al., 2019</a>]</span>.</span><a class="headerlink" href="#id40" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="conditional-variational-autoencoder-cvae">
<h4><span class="section-number">6.4.6.4. </span>Conditional variational autoencoder (CVAE)<a class="headerlink" href="#conditional-variational-autoencoder-cvae" title="Permalink to this headline">¶</a></h4>
<p>What if we provide the labels to the encoder and the decoder during training?</p>
<div class="figure align-default" id="id41">
<a class="reference internal image-reference" href="../_images/cvae-structure.svg"><img alt="../_images/cvae-structure.svg" src="../_images/cvae-structure.svg" width="100%" /></a>
<p class="caption"><span class="caption-number">Fig. 6.42 </span><span class="caption-text">Conditional VAE. Source: <a class="reference external" href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html">https://ijdykeman.github.io/ml/2016/12/21/cvae.html</a></span><a class="headerlink" href="#id41" title="Permalink to this image">¶</a></p>
</div>
<p>When trained with labels, the <strong>conditional variational autoencoder</strong> (CVAE <span id="id13">[<a class="reference internal" href="../zreferences.html#id178">Sohn et al., 2015</a>]</span>) becomes able to sample many images of the same class.</p>
<div class="figure align-default" id="id42">
<a class="reference internal image-reference" href="../_images/cvae-generation.svg"><img alt="../_images/cvae-generation.svg" src="../_images/cvae-generation.svg" width="100%" /></a>
<p class="caption"><span class="caption-number">Fig. 6.43 </span><span class="caption-text">Conditional VAE. Source: <a class="reference external" href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html">https://ijdykeman.github.io/ml/2016/12/21/cvae.html</a></span><a class="headerlink" href="#id42" title="Permalink to this image">¶</a></p>
</div>
<p>CVAE allows to sample as many samples of a given class as we want: <strong>data augmentation</strong>.</p>
<div class="figure align-default" id="id43">
<a class="reference internal image-reference" href="../_images/cvae-mnist.png"><img alt="../_images/cvae-mnist.png" src="../_images/cvae-mnist.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.44 </span><span class="caption-text">MNIST digits generated by a Conditional VAE. Source: <a class="reference external" href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html">https://ijdykeman.github.io/ml/2016/12/21/cvae.html</a></span><a class="headerlink" href="#id43" title="Permalink to this image">¶</a></p>
</div>
<p>The condition does not need to be a label, it can be a shape or another image (passed through another encoder).</p>
<div class="figure align-default" id="id44">
<a class="reference internal image-reference" href="../_images/cvae-shape.png"><img alt="../_images/cvae-shape.png" src="../_images/cvae-shape.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.45 </span><span class="caption-text">CVAE conditioned on shapes. Source:  <a class="reference external" href="https://hci.iwr.uni-heidelberg.de/content/variational-u-net-conditional-appearance-and-shape-generation">https://hci.iwr.uni-heidelberg.de/content/variational-u-net-conditional-appearance-and-shape-generation</a></span><a class="headerlink" href="#id44" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>
<div class="section" id="variational-inference-optional">
<h2><span class="section-number">6.5. </span>Variational inference (optional)<a class="headerlink" href="#variational-inference-optional" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/IP8YqRBBCP8' frameborder='0' allowfullscreen></iframe></div>
<div class="section" id="learning-probability-distributions-from-samples">
<h3><span class="section-number">6.5.1. </span>Learning probability distributions from samples<a class="headerlink" href="#learning-probability-distributions-from-samples" title="Permalink to this headline">¶</a></h3>
<p>The input data <span class="math notranslate nohighlight">\(X\)</span> comes from an unknown distribution <span class="math notranslate nohighlight">\(P(X)\)</span>. The training set <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is formed by <strong>samples</strong> of that distribution. Learning the distribution of the data means learning a <strong>parameterized distribution</strong> <span class="math notranslate nohighlight">\(p_\theta(X)\)</span> that is as close as possible from the true distribution <span class="math notranslate nohighlight">\(P(X)\)</span>. The parameterized distribution could be a family of known distributions (e.g. normal) or a neural network with a softmax output layer.</p>
<div class="figure align-default" id="id45">
<a class="reference internal image-reference" href="../_images/kerneldensityestimation.png"><img alt="../_images/kerneldensityestimation.png" src="../_images/kerneldensityestimation.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.46 </span><span class="caption-text">Density estimation. Source: <a class="reference external" href="https://machinelearningmastery.com/probability-density-estimation/">https://machinelearningmastery.com/probability-density-estimation/</a></span><a class="headerlink" href="#id45" title="Permalink to this image">¶</a></p>
</div>
<p>This means that we want to minimize the KL between the two distributions:</p>
<div class="math notranslate nohighlight">
\[\min_\theta \, \text{KL}(P(X) || p_\theta(X)) = \mathbb{E}_{x \sim P(X)} [- \log \dfrac{p_\theta(X=x)}{P(X=x)}]\]</div>
<p>The problem is that we do not know <span class="math notranslate nohighlight">\(P(X)\)</span> as it is what we want to learn, so we cannot estimate the KL directly.</p>
<p>In supervised learning, we are learning the <strong>conditional probability</strong> <span class="math notranslate nohighlight">\(P(T | X)\)</span> of the targets given the inputs, i.e. what is the probability of having the label <span class="math notranslate nohighlight">\(T=t\)</span> given the input <span class="math notranslate nohighlight">\(X=x\)</span>. A NN with a softmax output layer represents the parameterized distribution <span class="math notranslate nohighlight">\(p_\theta(T | X)\)</span>. The KL between the two distributions is:</p>
<div class="math notranslate nohighlight">
\[\text{KL}(P(T | X) || p_\theta(T | X)) = \mathbb{E}_{x, t \sim \mathcal{D}} [- \log \dfrac{p_\theta(T=t | X=x)}{P(T=t | X=x)}]\]</div>
<p>With the properties of the log, we know that the KL is the cross-entropy minus the entropy of the data:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{KL}(P(T | X) || p_\theta(T | X)) &amp;= \mathbb{E}_{x, t \sim \mathcal{D}} [- \log p_\theta(T=t | X=x)]  - \mathbb{E}_{x, t \sim \mathcal{D}} [- \log P(T=t | X=x)] \\
&amp;\\
    &amp; = H(P(T | X), p_\theta(T |X)) - H(P(T|X)) \\
\end{aligned}\end{split}\]</div>
<p>When we minimize the KL by applying gradient descent on the parameters <span class="math notranslate nohighlight">\(\theta\)</span>, only the cross-entropy will change, as the data does not depends on the model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nabla_\theta \, \text{KL}(P(T | X) || p_\theta(T | X))  &amp; = \nabla_\theta \, H(P(T | X), p_\theta(T |X)) - \nabla_\theta \,  H(P(T|X)) \\
    &amp;\\
     &amp; = \nabla_\theta \, H(P(T | X), p_\theta(T |X)) \\
     &amp; \\
     &amp; = \nabla_\theta \, \mathbb{E}_{x, t \sim \mathcal{D}} [-  \log p_\theta(T=t | X=x) ]\\
\end{aligned}\end{split}\]</div>
<p>Minimizing the cross-entropy (negative log likelihood) of the model on the data is the same as minimizing the KL between the two distributions in supervised learning! We were actually minimizing the KL all along.</p>
<p>When trying to learn the distribution <span class="math notranslate nohighlight">\(P(X)\)</span> of the data directly, we could use the same trick:</p>
<div class="math notranslate nohighlight">
\[\nabla_\theta \, \text{KL}(P(X) || p_\theta(X)) = \nabla_\theta \, H(P(X), p_\theta(X))  = \nabla_\theta \, \mathbb{E}_{x \sim X} [- \log p_\theta(X=x)]\]</div>
<p>i.e. maximize the log-likelihood of the model on the data <span class="math notranslate nohighlight">\(X\)</span>. If we use <span class="math notranslate nohighlight">\(N\)</span> data samples to estimate the expectation, we notice that:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{x \sim X} [\log p_\theta(X=x)] \approx \dfrac{1}{N} \, \sum_{i=1}^N \log p_\theta(X=x_i) = \dfrac{1}{N} \, \log \prod_{i=1}^N p_\theta(X=x_i) = \dfrac{1}{N} \, \log L(\theta)
\]</div>
<p>is indeed the log-likelihood of the model on the data that we maximized in <strong>maximum likelihood estimation</strong>.</p>
<p>The problem is that images are <strong>highly-dimensional</strong> (one dimension per pixel), so we would need astronomical numbers of samples to estimate the gradient (once): <strong>curse of dimensionality</strong>.</p>
<div class="figure align-default" id="id46">
<a class="reference internal image-reference" href="../_images/cursedimensionality.png"><img alt="../_images/cursedimensionality.png" src="../_images/cursedimensionality.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.47 </span><span class="caption-text">Curse of dimensionality. Source: <a class="reference external" href="https://dibyaghosh.com/blog/probability/highdimensionalgeometry.html">https://dibyaghosh.com/blog/probability/highdimensionalgeometry.html</a></span><a class="headerlink" href="#id46" title="Permalink to this image">¶</a></p>
</div>
<p>MLE does not work well in high-dimensional spaces. We need to work in a much lower-dimensional space.</p>
</div>
<div class="section" id="latent-space">
<h3><span class="section-number">6.5.2. </span>Latent space<a class="headerlink" href="#latent-space" title="Permalink to this headline">¶</a></h3>
<p>Images are not random samples of the pixel space: <strong>natural images</strong> are embedded in a much lower-dimensional space called a <strong>manifold</strong>. A manifold is a <strong>locally Euclidian</strong> topological space of lower dimension: The surface of the earth is locally flat and 2D, but globally spherical and 3D.</p>
<p>If we have a <strong>generative model</strong> telling us how a point on the manifold <span class="math notranslate nohighlight">\(z\)</span> maps to the image space (<span class="math notranslate nohighlight">\(P(X | z)\)</span>), we would only need to learn the distribution of the data in the lower-dimensional <strong>latent space</strong>.</p>
<div class="figure align-default" id="id47">
<a class="reference internal image-reference" href="../_images/manifold.png"><img alt="../_images/manifold.png" src="../_images/manifold.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.48 </span><span class="caption-text">Manifold. Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Manifold">https://en.wikipedia.org/wiki/Manifold</a></span><a class="headerlink" href="#id47" title="Permalink to this image">¶</a></p>
</div>
<p>The low-dimensional <strong>latent variables</strong> <span class="math notranslate nohighlight">\(z\)</span> are the actual cause for the observations <span class="math notranslate nohighlight">\(X\)</span>. Given a sample <span class="math notranslate nohighlight">\(z\)</span> on the manifold, we can train a <strong>generative model</strong> <span class="math notranslate nohighlight">\(p_\theta(X | z)\)</span> to recreate the input <span class="math notranslate nohighlight">\(X\)</span>. <span class="math notranslate nohighlight">\(p_\theta(X | z)\)</span> is the <strong>decoder</strong>: given a latent representation <span class="math notranslate nohighlight">\(z\)</span>, what is the corresponding observation <span class="math notranslate nohighlight">\(X\)</span>?</p>
<div class="figure align-default" id="id48">
<a class="reference internal image-reference" href="../_images/latentvariable.png"><img alt="../_images/latentvariable.png" src="../_images/latentvariable.png" style="width: 40%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.49 </span><span class="caption-text">Latent variable. Source: <a class="reference external" href="https://blog.evjang.com/2016/08/variational-bayes.html">https://blog.evjang.com/2016/08/variational-bayes.html</a></span><a class="headerlink" href="#id48" title="Permalink to this image">¶</a></p>
</div>
<p>If we learn the distribution <span class="math notranslate nohighlight">\(p_\theta(z)\)</span> of the manifold (latent space), we can infer the distribution of the data <span class="math notranslate nohighlight">\(p_\theta(X)\)</span> using that model:</p>
<div class="math notranslate nohighlight">
\[p_\theta(X) = \mathbb{E}_{z \sim p_\theta(z)} [p_\theta(X | z)] = \int_z p_\theta(X | z) \, p_\theta(z) \, dz\]</div>
<p>Problem: we do not know <span class="math notranslate nohighlight">\(p_\theta(z)\)</span>, as the only data we see is <span class="math notranslate nohighlight">\(X\)</span>: <span class="math notranslate nohighlight">\(z\)</span> is called a <strong>latent variable</strong> because it explains the data but is hidden.</p>
</div>
<div class="section" id="variational-inference">
<h3><span class="section-number">6.5.3. </span>Variational inference<a class="headerlink" href="#variational-inference" title="Permalink to this headline">¶</a></h3>
<p>To estimate <span class="math notranslate nohighlight">\(p_\theta(z)\)</span>, we could again marginalize over <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[p_\theta(z) = \mathbb{E}_{x \sim p_\theta(X)} [p_\theta(z | x)] = \int_x p_\theta(z | x) \, p_\theta(x) \, dx\]</div>
<p><span class="math notranslate nohighlight">\(p_\theta(z | x)\)</span> is the <strong>encoder</strong>: given an input <span class="math notranslate nohighlight">\(x \sim p_\theta(X)\)</span>, what is its latent representation <span class="math notranslate nohighlight">\(z\)</span>? The Bayes rule tells us:</p>
<div class="math notranslate nohighlight">
\[p_\theta(z | x) = p_\theta(x |z) \, \dfrac{p_\theta(z)}{p_\theta(x)}\]</div>
<p>The posterior probability (encoder) <span class="math notranslate nohighlight">\(p_\theta(z | X)\)</span> depends on the model (decoder) <span class="math notranslate nohighlight">\(p_\theta(X|z)\)</span>, the prior (assumption) <span class="math notranslate nohighlight">\(p_\theta(z)\)</span> and the evidence (data) <span class="math notranslate nohighlight">\(p_\theta(X)\)</span>.</p>
<p>We get:</p>
<div class="math notranslate nohighlight">
\[p_\theta(z) = \mathbb{E}_{x \sim p_\theta(X)} [p_\theta(x |z) \, \dfrac{p_\theta(z)}{p_\theta(x)}]\]</div>
<p>The posterior is <strong>untractable</strong> as it would require to integrate over all possible inputs <span class="math notranslate nohighlight">\(x \sim p_\theta(X)\)</span>:</p>
<div class="math notranslate nohighlight">
\[p_\theta(z) = \mathbb{E}_{x \sim p_\theta(X)} [p_\theta(x |z) \, \dfrac{p_\theta(z)}{p_\theta(x)}] = \int_x p_\theta(x |z) \, p_\theta(z) \, dx\]</div>
<p><strong>Variational inference</strong> proposes to approximate the true encoder <span class="math notranslate nohighlight">\(p_\theta(z | x)\)</span> by another parameterized distribution <span class="math notranslate nohighlight">\(q_\phi(z|x)\)</span>.</p>
<div class="figure align-default" id="id49">
<a class="reference internal image-reference" href="../_images/VAE-graphical-model.png"><img alt="../_images/VAE-graphical-model.png" src="../_images/VAE-graphical-model.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 6.50 </span><span class="caption-text">Variational inference approximates the encoder with another parameterized distribution. Source: <a class="reference external" href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html</a></span><a class="headerlink" href="#id49" title="Permalink to this image">¶</a></p>
</div>
<p>The decoder <span class="math notranslate nohighlight">\(p_\theta(x |z)\)</span> generates observations <span class="math notranslate nohighlight">\(x\)</span> from a latent representation <span class="math notranslate nohighlight">\(x\)</span> with parameters <span class="math notranslate nohighlight">\(\theta\)</span>. The encoder <span class="math notranslate nohighlight">\(q_\phi(z|x)\)</span> estimates the latent representation <span class="math notranslate nohighlight">\(z\)</span> of a generated observation <span class="math notranslate nohighlight">\(x\)</span>. It should approximate <span class="math notranslate nohighlight">\(p_\theta(z | x)\)</span> with parameters <span class="math notranslate nohighlight">\(\phi\)</span>.</p>
<p>To make <span class="math notranslate nohighlight">\(q_\phi(z| X)\)</span> close from <span class="math notranslate nohighlight">\(p_\theta(z | X)\)</span>, we minimize their KL divergence:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{KL}(q_\phi(z|X) || p_\theta(z | X) ) &amp;= \mathbb{E}_{z \sim q_\phi(z|X)} [- \log \dfrac{p_\theta(z | X)}{q_\phi(z|X)}]\\
\end{aligned}\end{split}\]</div>
<p>Note that we sample the latent representations from the learned encoder <span class="math notranslate nohighlight">\(q_\phi(z|X)\)</span> (imagination). As <span class="math notranslate nohighlight">\(p_\theta(z | X) = p_\theta(X |z) \, \dfrac{p_\theta(z)}{p_\theta(X)}\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{KL}(q_\phi(z|X) || p_\theta(z | X) ) &amp;= \mathbb{E}_{z \sim q_\phi(z|X)} [- \log \dfrac{p_\theta(X | z) \, p_\theta(z)}{q_\phi(z|X) \, p_\theta(X)}]\\
&amp;=\mathbb{E}_{z \sim q_\phi(z|X)} [- \log \dfrac{p_\theta(z)}{q_\phi(z|X)}] - \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X)]  \\
&amp;+ \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)]\\
\end{aligned}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(p_\theta(X)\)</span> does not depend on <span class="math notranslate nohighlight">\(z\)</span>, so its expectation w.r.t <span class="math notranslate nohighlight">\(z\)</span> is constant:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{KL}(q_\phi(z|X) || p_\theta(z | X) ) &amp;= \text{KL}(q_\phi(z|X) || p_\theta(z)) + \log p_\theta(X) + \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)]\\
\end{aligned}\end{split}\]</div>
<p>We rearrange the terms:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\log p_\theta(X) - \text{KL}(q_\phi(z|X) || p_\theta(z | X) ) &amp;=  - \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)] - \text{KL}(q_\phi(z|X) || p_\theta(z))\\
\end{aligned}\end{split}\]</div>
<ul class="simple">
<li><p>Training the <strong>encoder</strong> means that we <strong>minimize</strong> <span class="math notranslate nohighlight">\(\text{KL}(q_\phi(z|X) || p_\theta(z | X) )\)</span>.</p></li>
<li><p>Training the <strong>decoder</strong> means that we <strong>maximize</strong> <span class="math notranslate nohighlight">\(\log p_\theta(X)\)</span> (log-likelihood of the model).</p></li>
<li><p>Training the encoder and decoder together means that we <strong>maximize</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \text{ELBO}(\theta, \phi) = \log p_\theta(X) - \text{KL}(q_\phi(z|X) || p_\theta(z | X) )\]</div>
<p>The KL divergence is always positive or equal to 0, so we have:</p>
<div class="math notranslate nohighlight">
\[\text{ELBO}(\theta, \phi) \leq \log p_\theta(X)\]</div>
<p>This term is called  the <strong>evidence lower bound</strong> (ELBO): by maximizing it, we also maximize the untractable evidence <span class="math notranslate nohighlight">\(\log p_\theta(X)\)</span>, which is what we want to do. The trick is that the right-hand term of the equation gives us a tractable definition of the ELBO term:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{ELBO}(\theta, \phi) &amp;=  \log p_\theta(X) - \text{KL}(q_\phi(z|X) || p_\theta(z | X) ) \\
&amp;\\
&amp;= - \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)] - \text{KL}(q_\phi(z|X) || p_\theta(z))
\end{aligned}\end{split}\]</div>
<p>What happens when we <strong>minimize</strong> the negative ELBO?</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}(\theta, \phi) = - \text{ELBO}(\theta, \phi) = \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)] + \text{KL}(q_\phi(z|X) || p_\theta(z))\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)]\)</span> is the <strong>reconstruction loss</strong> of the decoder <span class="math notranslate nohighlight">\(p_\theta(X | z)\)</span>:</p>
<ul>
<li><p>Given a sample <span class="math notranslate nohighlight">\(z\)</span> of the encoder <span class="math notranslate nohighlight">\(q_\phi(z|X)\)</span>, minimize the negative log-likelihood of the reconstruction <span class="math notranslate nohighlight">\(p_\theta(X | z)\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\text{KL}(q_\phi(z|X) || p_\theta(z))\)</span> is the <strong>regularization loss</strong> for the encoder:</p>
<ul>
<li><p>The latent distribution <span class="math notranslate nohighlight">\(q_\phi(z|X)\)</span> should be too far from the <strong>prior</strong> <span class="math notranslate nohighlight">\(p_\theta(z)\)</span>.</p></li>
</ul>
</li>
</ul>
<p><strong>Variational autoencoders</strong> use <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> as a prior for the latent space, but any other prior could be used.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mathcal{L}(\theta, \phi) &amp;= \mathcal{L}_\text{reconstruction}(\theta, \phi) + \mathcal{L}_\text{regularization}(\phi) \\
    &amp;\\
    &amp;= \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} [ - \log p_\theta(\mathbf{z})] + \text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || \mathcal{N}(\mathbf{0}, \mathbf{1}))\\
\end{aligned}\end{split}\]</div>
<p>The reparameterization trick and the fact that the KL between normal distributions has a closed form allow us to use backpropagation end-to-end. The encoder <span class="math notranslate nohighlight">\(q_\phi(z|X)\)</span> and decoder <span class="math notranslate nohighlight">\(p_\theta(X | z)\)</span> are neural networks in a VAE, but other parametrized distributions can be used (e.g. in physics).</p>
<div class="admonition-sources admonition">
<p class="admonition-title">Sources</p>
<ul class="simple">
<li><p><a class="reference external" href="https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/">https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/</a></p></li>
<li><p><a class="reference external" href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html</a></p></li>
<li><p><a class="reference external" href="https://blog.evjang.com/2016/08/variational-bayes.html">https://blog.evjang.com/2016/08/variational-bayes.html</a></p></li>
<li><p><a class="reference external" href="https://jonathan-hui.medium.com/machine-learning-variational-inference-273d8e6480bb">https://jonathan-hui.medium.com/machine-learning-variational-inference-273d8e6480bb</a></p></li>
</ul>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3-deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="5-SemanticSegmentation.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">5. </span>Semantic segmentation</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="7-RBM.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">7. </span>Restricted Boltzmann machines (optional)</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>