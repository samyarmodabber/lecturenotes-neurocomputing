
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Object detection &#8212; Neurocomputing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/4-ObjectDetection.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Semantic segmentation" href="5-SemanticSegmentation.html" />
    <link rel="prev" title="3. Convolutional neural networks" href="3-CNN.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tuc.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neurocomputing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/3-deeplearning/4-ObjectDetection.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   4.1. Object detection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#r-cnn-regions-with-cnn-features">
   4.2. R-CNN : Regions with CNN features
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fast-r-cnn">
   4.3. Fast R-CNN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#faster-r-cnn">
   4.4. Faster R-CNN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#yolo">
   4.5. YOLO
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#architecture-of-the-cnn">
     4.5.1. Architecture of the CNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confidence-score">
     4.5.2. Confidence score
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intersection-over-union-iou">
     4.5.3. Intersection over Union (IoU)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-functions">
     4.5.4. Loss functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#yolo-trained-on-pascal-voc">
     4.5.5. YOLO  trained on PASCAL VOC
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ssd">
   4.6. SSD
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#d-object-detection">
   4.7. 3D object detection
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="object-detection">
<h1><span class="section-number">4. </span>Object detection<a class="headerlink" href="#object-detection" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/3.4-ObjectDetection.pdf">pdf</a></p>
<div class="section" id="id1">
<h2><span class="section-number">4.1. </span>Object detection<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/arzmgsxmpRw' frameborder='0' allowfullscreen></iframe></div>
<p>Contrary to object classification/recognition which assigns a single label to an image, <strong>object detection</strong> requires to both classify object and report their position and size on the image (bounding box).</p>
<div class="figure align-default" id="id23">
<a class="reference internal image-reference" href="../_images/dnn_classification_vs_detection.png"><img alt="../_images/dnn_classification_vs_detection.png" src="../_images/dnn_classification_vs_detection.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.10 </span><span class="caption-text">Object recognition vs. detection. Source: <a class="reference external" href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4">https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4</a></span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</div>
<p>A naive and very expensive method is to use a trained CNN as a high-level filter. The CNN is trained on small images and convolved on bigger images. The output is a <strong>heatmap</strong> of the probability that a particular object is present.</p>
<div class="figure align-default" id="id24">
<a class="reference internal image-reference" href="../_images/objectdetection.png"><img alt="../_images/objectdetection.png" src="../_images/objectdetection.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.11 </span><span class="caption-text">Using a pretrained CNN to generate heatmaps. Source: <a class="reference external" href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4">https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4</a></span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</div>
<p>Object detection is both a:</p>
<ul class="simple">
<li><p><strong>Classification</strong> problem, as one has to recognize an object.</p></li>
<li><p><strong>Regression</strong> problem, as one has to predict the coordinates <span class="math notranslate nohighlight">\((x, y, w, h)\)</span> of the bounding box.</p></li>
</ul>
<div class="figure align-default" id="id25">
<a class="reference internal image-reference" href="../_images/localization.png"><img alt="../_images/localization.png" src="../_images/localization.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.12 </span><span class="caption-text">Object detection is both a classification and regression task. Source: <a class="reference external" href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e">https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e</a></span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
<p>The main datasets for object detection are the <strong>PASCAL</strong> Visual Object Classes Challenge (20 classes, ~10K images, ~25K annotated objects, <a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2008/">http://host.robots.ox.ac.uk/pascal/VOC/voc2008/</a>) and the MS COCO dataset (Common Objects in COntext, 330k images, 80 labels, <a class="reference external" href="http://cocodataset.org">http://cocodataset.org</a>)</p>
</div>
<div class="section" id="r-cnn-regions-with-cnn-features">
<h2><span class="section-number">4.2. </span>R-CNN : Regions with CNN features<a class="headerlink" href="#r-cnn-regions-with-cnn-features" title="Permalink to this headline">¶</a></h2>
<p>R-CNN <span id="id2">[<a class="reference internal" href="../zreferences.html#id50">Girshick et al., 2014</a>]</span> was one of the first CNN-based architectures allowing object detection.</p>
<div class="figure align-default" id="id26">
<a class="reference internal image-reference" href="../_images/rcnn.png"><img alt="../_images/rcnn.png" src="../_images/rcnn.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.13 </span><span class="caption-text">R-CNN <span id="id3">[<a class="reference internal" href="../zreferences.html#id50">Girshick et al., 2014</a>]</span>.</span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
<p>It is a pipeline of 4 steps:</p>
<ol class="simple">
<li><p>Bottom-up region proposals  by searching bounding boxes based on pixel info (selective search  <a class="reference external" href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf">https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf</a>).</p></li>
<li><p>Feature extraction using a pre-trained CNN (AlexNet).</p></li>
<li><p>Classification using a SVM (object or not; if yes, which one?)</p></li>
<li><p>If an object is found, linear regression on the region proposal to generate tighter bounding box coordinates.</p></li>
</ol>
<p>Each region proposal is processed by the CNN, followed by a SVM and a bounding box regressor.</p>
<div class="figure align-default" id="id27">
<a class="reference internal image-reference" href="../_images/rcnn-detail.png"><img alt="../_images/rcnn-detail.png" src="../_images/rcnn-detail.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.14 </span><span class="caption-text">R-CNN <span id="id4">[<a class="reference internal" href="../zreferences.html#id50">Girshick et al., 2014</a>]</span>. Source: <a class="reference external" href="https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_wk1_rcnn.pdf">https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_wk1_rcnn.pdf</a></span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
<p>The CNN is pre-trained on ImageNet and fine-tuned on Pascal VOC (transfer learning).</p>
<div class="figure align-default" id="id28">
<a class="reference internal image-reference" href="../_images/rcnn-training.png"><img alt="../_images/rcnn-training.png" src="../_images/rcnn-training.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.15 </span><span class="caption-text">R-CNN <span id="id5">[<a class="reference internal" href="../zreferences.html#id50">Girshick et al., 2014</a>]</span>. Source: <a class="reference external" href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e">https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e</a></span><a class="headerlink" href="#id28" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="fast-r-cnn">
<h2><span class="section-number">4.3. </span>Fast R-CNN<a class="headerlink" href="#fast-r-cnn" title="Permalink to this headline">¶</a></h2>
<p>The main drawback of R-CNN is that each of the 2000 region proposals have to go through the CNN: extremely slow. The idea behind <strong>Fast R-CNN</strong> <span id="id6">[<a class="reference internal" href="../zreferences.html#id51">Girshick, 2015</a>]</span> is to extract region proposals in higher feature maps and to use transfer learning.</p>
<div class="figure align-default" id="id29">
<a class="reference internal image-reference" href="../_images/fast-rcnn.png"><img alt="../_images/fast-rcnn.png" src="../_images/fast-rcnn.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.16 </span><span class="caption-text">Fast R-CNN <span id="id7">[<a class="reference internal" href="../zreferences.html#id51">Girshick, 2015</a>]</span>.</span><a class="headerlink" href="#id29" title="Permalink to this image">¶</a></p>
</div>
<p>The network first processes the whole image with several convolutional and max pooling layers to produce a feature map. Each object proposal is projected to the feature map, where a region of interest (RoI) pooling layer extracts a fixed-length feature vector. Each feature vector is fed into a sequence of FC layers that finally branch into two sibling output layers:</p>
<ul class="simple">
<li><p>a softmax probability estimate over the K classes plus a catch-all “background” class.</p></li>
<li><p>a regression layer that outputs four real-valued numbers for each class.</p></li>
</ul>
<p>The loss function to minimize is a composition of different losses and penalty terms:</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\theta) = \lambda_1 \, \mathcal{L}_\text{classification}(\theta) + \lambda_2 \, \mathcal{L}_\text{regression}(\theta) + \lambda_3 \, \mathcal{L}_\text{regularization}(\theta)
\]</div>
</div>
<div class="section" id="faster-r-cnn">
<h2><span class="section-number">4.4. </span>Faster R-CNN<a class="headerlink" href="#faster-r-cnn" title="Permalink to this headline">¶</a></h2>
<p>Both R-CNN and Fast R-CNN use selective search to find out the region proposals: slow and time-consuming. Faster R-CNN <span id="id8">[<a class="reference internal" href="../zreferences.html#id159">Ren et al., 2016</a>]</span> introduces an object detection algorithm that lets the network learn the region proposals. The image is passed through a pretrained CNN to obtain a convolutional feature map. A separate network is used to predict the region proposals. The predicted region proposals are then reshaped using a RoI (region-of-interest) pooling layer which is then used to classify the object and predict the bounding box.</p>
<div class="figure align-default" id="id30">
<a class="reference internal image-reference" href="../_images/faster-rcnn.png"><img alt="../_images/faster-rcnn.png" src="../_images/faster-rcnn.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.17 </span><span class="caption-text">Faster R-CNN <span id="id9">[<a class="reference internal" href="../zreferences.html#id159">Ren et al., 2016</a>]</span>.</span><a class="headerlink" href="#id30" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="yolo">
<h2><span class="section-number">4.5. </span>YOLO<a class="headerlink" href="#yolo" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/EuAb0UdUnUM' frameborder='0' allowfullscreen></iframe></div>
<p>(Fast(er)) R-CNN perform classification for each region proposal sequentially: slow. YOLO (You Only Look Once) <span id="id10">[<a class="reference internal" href="../zreferences.html#id155">Redmon et al., 2016</a>]</span> applies a single neural network to the full image to predict all possible boxes and the corresponding classes. YOLO divides the image into a SxS grid of cells.</p>
<div class="figure align-default" id="id31">
<a class="reference internal image-reference" href="../_images/yolo.png"><img alt="../_images/yolo.png" src="../_images/yolo.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.18 </span><span class="caption-text">YOLO <span id="id11">[<a class="reference internal" href="../zreferences.html#id155">Redmon et al., 2016</a>]</span>.</span><a class="headerlink" href="#id31" title="Permalink to this image">¶</a></p>
</div>
<p>Each grid cell predicts a single object, with the corresponding <span class="math notranslate nohighlight">\(C\)</span> <strong>class probabilities</strong> (softmax). It also predicts the coordinates of <span class="math notranslate nohighlight">\(B\)</span> possible <strong>bounding boxes</strong> (x, y, w, h) as well as a box <strong>confidence score</strong>. The SxSxB predicted boxes are then pooled together to form the final prediction.</p>
<p>In the figure below, the yellow box predicts the presence of a <strong>person</strong> (the class) as well as a candidate <strong>bounding box</strong> (it may be bigger than the grid cell itself).</p>
<div class="figure align-default" id="id32">
<a class="reference internal image-reference" href="../_images/yolo1.jpeg"><img alt="../_images/yolo1.jpeg" src="../_images/yolo1.jpeg" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.19 </span><span class="caption-text">Each cell predicts a class (e.g. person) and the (x, y, w, h) coordinates of the bounding box. Source: <a class="reference external" href="https://medium.com/&#64;jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088">https://medium.com/&#64;jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></span><a class="headerlink" href="#id32" title="Permalink to this image">¶</a></p>
</div>
<p>In the original YOLO implementation, each grid cell proposes 2 bounding boxes:</p>
<div class="figure align-default" id="id33">
<a class="reference internal image-reference" href="../_images/yolo2.jpeg"><img alt="../_images/yolo2.jpeg" src="../_images/yolo2.jpeg" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.20 </span><span class="caption-text">Each cell predicts two bounding boxes per object. Source: <a class="reference external" href="https://medium.com/&#64;jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088">https://medium.com/&#64;jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></span><a class="headerlink" href="#id33" title="Permalink to this image">¶</a></p>
</div>
<p>Each grid cell predicts a probability for each of the 20 classes, two bounding boxes (4 coordinates  per bounding box) and their confidence scores. This makes C + B * 5 = 30 values to predict for each cell.</p>
<div class="figure align-default" id="id34">
<a class="reference internal image-reference" href="../_images/yolo3.jpeg"><img alt="../_images/yolo3.jpeg" src="../_images/yolo3.jpeg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.21 </span><span class="caption-text">Each cell outputs 30 values: 20 for the classes and 5 for each bounding box, including the confidence score. Source: <a class="reference external" href="https://medium.com/&#64;jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088">https://medium.com/&#64;jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></span><a class="headerlink" href="#id34" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="architecture-of-the-cnn">
<h3><span class="section-number">4.5.1. </span>Architecture of the CNN<a class="headerlink" href="#architecture-of-the-cnn" title="Permalink to this headline">¶</a></h3>
<p>YOLO uses a CNN with 24 convolutional layers and 4 max-pooling layers to obtain a 7x7 grid. The last convolution layer outputs a tensor with shape (7, 7, 1024). The tensor is then flattened and passed through 2 fully connected layers. The output is a tensor of shape (7, 7, 30), i.e. 7x7 grid cells, 20 classes and 2 boundary box predictions per cell.</p>
<div class="figure align-default" id="id35">
<a class="reference internal image-reference" href="../_images/YOLO.png"><img alt="../_images/YOLO.png" src="../_images/YOLO.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.22 </span><span class="caption-text">Architecture of the CNN used in YOLO <span id="id12">[<a class="reference internal" href="../zreferences.html#id155">Redmon et al., 2016</a>]</span>.</span><a class="headerlink" href="#id35" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="confidence-score">
<h3><span class="section-number">4.5.2. </span>Confidence score<a class="headerlink" href="#confidence-score" title="Permalink to this headline">¶</a></h3>
<p>The 7x7 grid cells predict 2 bounding boxes each: maximum of 98 bounding boxes on the whole image. Only the bounding boxes with the <strong>highest class confidence score</strong> are kept.</p>
<div class="math notranslate nohighlight">
\[
    \text{class confidence score = box confidence score * class probability}
\]</div>
<p>In practice, the class confidence score should be above 0.25.</p>
<div class="figure align-default" id="id36">
<a class="reference internal image-reference" href="../_images/yolo4.png"><img alt="../_images/yolo4.png" src="../_images/yolo4.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.23 </span><span class="caption-text">Only the bounding boxes with the highest class confidence scores are kept among the 98 possible ones. Source: <span id="id13">[<a class="reference internal" href="../zreferences.html#id155">Redmon et al., 2016</a>]</span>.</span><a class="headerlink" href="#id36" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="intersection-over-union-iou">
<h3><span class="section-number">4.5.3. </span>Intersection over Union (IoU)<a class="headerlink" href="#intersection-over-union-iou" title="Permalink to this headline">¶</a></h3>
<p>To ensure specialization, only one bounding box per grid cell should be responsible for detecting an object. During learning, we select the bounding box with the biggest overlap with the object. This can be measured by the <strong>Intersection over the Union</strong> (IoU).</p>
<div class="figure align-default" id="id37">
<a class="reference internal image-reference" href="../_images/iou1.jpg"><img alt="../_images/iou1.jpg" src="../_images/iou1.jpg" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.24 </span><span class="caption-text">The Intersection over Union (IoU) measures the overlap between bounding boxes. Source: <a class="reference external" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></span><a class="headerlink" href="#id37" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id38">
<a class="reference internal image-reference" href="../_images/iou2.png"><img alt="../_images/iou2.png" src="../_images/iou2.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.25 </span><span class="caption-text">The Intersection over Union (IoU) measures the overlap between bounding boxes. Source: <a class="reference external" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></span><a class="headerlink" href="#id38" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="loss-functions">
<h3><span class="section-number">4.5.4. </span>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h3>
<p>The output of the network is a 7x7x30 tensor, representing for each cell:</p>
<ul class="simple">
<li><p>the probability that an object of a given class is present.</p></li>
<li><p>the position of two bounding boxes.</p></li>
<li><p>the confidence that the proposed bounding boxes correspond to a real object (the IoU).</p></li>
</ul>
<p>We are going to combine three different loss functions:</p>
<ol class="simple">
<li><p>The <strong>categorization loss</strong>: each cell should predict the correct class.</p></li>
<li><p>The <strong>localization loss</strong>: error between the predicted boundary box and the ground truth for each object.</p></li>
<li><p>The <strong>confidence loss</strong>: do the predicted bounding boxes correspond to real objects?</p></li>
</ol>
<p><strong>Classification loss</strong></p>
<p>The classification loss is the <strong>mse</strong> between:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{p}_i(c)\)</span>: the one-hot encoded class <span class="math notranslate nohighlight">\(c\)</span> of the object present under each cell <span class="math notranslate nohighlight">\(i\)</span>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(p_i(c)\)</span>: the predicted class probabilities of cell <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}_\text{classification}(\theta) =  \sum_{i=0}^{S^2} \mathbb{1}_i^\text{obj} \sum_{c \in \text{classes}} (p_i(c) - \hat{p}_i(c))^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{1}_i^\text{obj}\)</span> is 1 when there actually is an object behind the cell <span class="math notranslate nohighlight">\(i\)</span>, 0 otherwise (background).</p>
<p>They could also have used the cross-entropy loss, but the output layer is not a regular softmax layer. Using mse is also more compatible with the other losses.</p>
<p><strong>Localization loss</strong></p>
<p>For all bounding boxes matching a real object, we want to minimize the <strong>mse</strong> between:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\hat{x}_i, \hat{y}_i, \hat{w}_i, \hat{h}_i)\)</span>: the coordinates of the ground truth bounding box, and</p></li>
<li><p><span class="math notranslate nohighlight">\((x_i, y_i, w_i, h_i)\)</span>: the coordinates of the predicted bounding box.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathcal{L}_\text{localization}(\theta) = \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} [ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2] \\
    \qquad\qquad\qquad\qquad + \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} [ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2]
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^\text{obj}\)</span> is 1 when the bounding box <span class="math notranslate nohighlight">\(j\)</span> of cell <span class="math notranslate nohighlight">\(i\)</span> “matches” with an object (IoU). The root square of the width and height of the bounding boxes is used. This allows to penalize more the errors on small boxes than on big boxes.</p>
<p><strong>Confidence loss</strong></p>
<p>Finally, we need to learn the confidence score of each bounding box, by minimizing the <strong>mse</strong> between:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(C_i\)</span>: the predicted confidence score of cell <span class="math notranslate nohighlight">\(i\)</span>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{C}_i\)</span>: the IoU between the ground truth bounding box and the predicted one.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathcal{L}_\text{confidence}(\theta) = \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} (C_{ij} - \hat{C}_{ij})^2  \\
    \qquad\qquad\qquad\qquad + \lambda^\text{noobj} \, \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{noobj} (C_{ij} - \hat{C}_{ij})^2
\end{split}\]</div>
<p>Two cases are considered:</p>
<ol class="simple">
<li><p>There was a real object at that location (<span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^\text{obj} = 1\)</span>): the confidences should be updated fully.</p></li>
<li><p>There was no real object (<span class="math notranslate nohighlight">\(\mathbb{1}_{ij}^\text{noobj} = 1\)</span>): the confidences should only be moderately updated (<span class="math notranslate nohighlight">\(\lambda^\text{noobj} = 0.5\)</span>)</p></li>
</ol>
<p>This is to deal with <strong>class imbalance</strong>: there are much more cells on the background than on real objects.</p>
<p>Put together, the loss function to minimize is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \mathcal{L}(\theta) &amp; = \mathcal{L}_\text{classification}(\theta) + \lambda_\text{coord} \, \mathcal{L}_\text{localization}(\theta) + \mathcal{L}_\text{confidence}(\theta) \\
              &amp; = \sum_{i=0}^{S^2} \mathbb{1}_i^\text{obj} \sum_{c \in \text{classes}} (p_i(c) - \hat{p}_i(c))^2 \\
              &amp; + \lambda_\text{coord} \, \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} [ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2] \\
              &amp; + \lambda_\text{coord} \, \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} [ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2] \\
              &amp; + \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} (C_{ij} - \hat{C}_{ij})^2  \\
              &amp; + \lambda^\text{noobj} \, \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{noobj} (C_{ij} - \hat{C}_{ij})^2 \\
\end{align}
\end{split}\]</div>
</div>
<div class="section" id="yolo-trained-on-pascal-voc">
<h3><span class="section-number">4.5.5. </span>YOLO  trained on PASCAL VOC<a class="headerlink" href="#yolo-trained-on-pascal-voc" title="Permalink to this headline">¶</a></h3>
<p>YOLO was trained on PASCAL VOC (natural images) but generalizes well to other datasets (paintings…). YOLO runs in real-time (60 fps) on a NVIDIA Titan X.
Faster and more accurate versions of YOLO have been developed: YOLO9000 <span id="id14">[<a class="reference internal" href="../zreferences.html#id156">Redmon &amp; Farhadi, 2016</a>]</span>, YOLOv3 <span id="id15">[<a class="reference internal" href="../zreferences.html#id157">Redmon &amp; Farhadi, 2018</a>]</span>, YOLOv5 (<a class="reference external" href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a>)…</p>
<div class="figure align-default" id="id39">
<a class="reference internal image-reference" href="../_images/yolo-result2.png"><img alt="../_images/yolo-result2.png" src="../_images/yolo-result2.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.26 </span><span class="caption-text">Performance of YOLO compared to the state of the art. Source: <span id="id16">[<a class="reference internal" href="../zreferences.html#id155">Redmon et al., 2016</a>]</span>.</span><a class="headerlink" href="#id39" title="Permalink to this image">¶</a></p>
</div>
<p>Refer to the website of the authors for additional information: <a class="reference external" href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a></p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/MPU2HistivI' frameborder='0' allowfullscreen></iframe></div>
</div>
</div>
<div class="section" id="ssd">
<h2><span class="section-number">4.6. </span>SSD<a class="headerlink" href="#ssd" title="Permalink to this headline">¶</a></h2>
<p>The idea of SSD (Single-Shot Detector, <span id="id17">[<a class="reference internal" href="../zreferences.html#id123">Liu et al., 2016</a>]</span>) is similar to YOLO, but:</p>
<ul class="simple">
<li><p>faster</p></li>
<li><p>more accurate</p></li>
<li><p>not limited to 98 objects per scene</p></li>
<li><p>multi-scale</p></li>
</ul>
<p>Contrary to YOLO, all convolutional layers are used to predict a bounding box, not just the final tensor: <strong>skip connections</strong>. This allows to detect boxes at multiple scales (pyramid).</p>
<div class="figure align-default" id="id40">
<a class="reference internal image-reference" href="../_images/ssd.png"><img alt="../_images/ssd.png" src="../_images/ssd.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.27 </span><span class="caption-text">Single-Shot Detector, <span id="id18">[<a class="reference internal" href="../zreferences.html#id123">Liu et al., 2016</a>]</span>.</span><a class="headerlink" href="#id40" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="d-object-detection">
<h2><span class="section-number">4.7. </span>3D object detection<a class="headerlink" href="#d-object-detection" title="Permalink to this headline">¶</a></h2>
<p>It is also possible to use <strong>depth</strong> information (e.g. from a Kinect) as an additional channel of the R-CNN. The depth information provides more information on the structure of the object, allowing to disambiguate certain situations (segmentation).</p>
<div class="figure align-default" id="id41">
<a class="reference internal image-reference" href="../_images/rcnn-rgbd.png"><img alt="../_images/rcnn-rgbd.png" src="../_images/rcnn-rgbd.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.28 </span><span class="caption-text">Learning Rich Features from RGB-D Images for Object Detection, <span id="id19">[<a class="reference internal" href="../zreferences.html#id64">Gupta et al., 2014</a>]</span>.</span><a class="headerlink" href="#id41" title="Permalink to this image">¶</a></p>
</div>
<p>Lidar point clouds can also be used for detecting objects, for example <strong>VoxelNet</strong>  <span id="id20">[<a class="reference internal" href="../zreferences.html#id209">Zhou &amp; Tuzel, 2017</a>]</span> trained in the KITTI dataset.</p>
<div class="figure align-default" id="id42">
<a class="reference internal image-reference" href="../_images/voxelnet.png"><img alt="../_images/voxelnet.png" src="../_images/voxelnet.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.29 </span><span class="caption-text">VoxelNet <span id="id21">[<a class="reference internal" href="../zreferences.html#id209">Zhou &amp; Tuzel, 2017</a>]</span>.</span><a class="headerlink" href="#id42" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id43">
<a class="reference internal image-reference" href="../_images/voxelnet-result.png"><img alt="../_images/voxelnet-result.png" src="../_images/voxelnet-result.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.30 </span><span class="caption-text">VoxelNet <span id="id22">[<a class="reference internal" href="../zreferences.html#id209">Zhou &amp; Tuzel, 2017</a>]</span>. Source: <a class="reference external" href="https://medium.com/&#64;SmartLabAI/3d-object-detection-from-lidar-data-with-deep-learning-95f6d400399a">https://medium.com/&#64;SmartLabAI/3d-object-detection-from-lidar-data-with-deep-learning-95f6d400399a</a></span><a class="headerlink" href="#id43" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition-additional-resources-on-object-detection admonition">
<p class="admonition-title">Additional resources on object detection</p>
<p><a class="reference external" href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852">https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852</a></p>
<p><a class="reference external" href="https://medium.com/&#64;smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8">https://medium.com/&#64;smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8</a></p>
<p><a class="reference external" href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e">https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e</a></p>
<p><a class="reference external" href="https://medium.com/&#64;jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088">https://medium.com/&#64;jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></p>
<p><a class="reference external" href="https://medium.com/&#64;jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06">https://medium.com/&#64;jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</a></p>
<p><a class="reference external" href="https://towardsdatascience.com/lidar-3d-object-detection-methods-f34cf3227aea">https://towardsdatascience.com/lidar-3d-object-detection-methods-f34cf3227aea</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./3-deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="3-CNN.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">3. </span>Convolutional neural networks</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="5-SemanticSegmentation.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">5. </span>Semantic segmentation</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>