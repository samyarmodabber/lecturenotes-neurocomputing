
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Neurons &#8212; Neurocomputing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/1-intro/3-Neurons.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1. Optimization" href="../2-linear/1-Optimization.html" />
    <link rel="prev" title="2. Math basics (optional)" href="2-Math.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tuc.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neurocomputing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/1-intro/3-Neurons.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#biological-neurons">
   3.1. Biological neurons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hodgkin-huxley-neurons">
   3.2. Hodgkin-Huxley neurons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#spiking-neurons">
   3.3. Spiking neurons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rate-coded-neurons">
   3.4. Rate-coded neurons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neurons">
   3.5. Artificial neurons
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neurons">
<h1><span class="section-number">3. </span>Neurons<a class="headerlink" href="#neurons" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/1.3-Neurons.pdf">pdf</a></p>
<div class="section" id="biological-neurons">
<h2><span class="section-number">3.1. </span>Biological neurons<a class="headerlink" href="#biological-neurons" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/ALBRu-AK53I' frameborder='0' allowfullscreen></iframe></div>
<p>The human brain is composed of 100 billion <strong>neurons</strong>. A biological neuron is a cell, composed of a cell body (<strong>soma</strong>), multiple <strong>dendrites</strong> and an <strong>axon</strong>. The axon of a neuron can contact the dendrites of another through <strong>synapses</strong> to transmit information. There are hundreds of different types of neurons, each with different properties.</p>
<div class="figure align-default" id="id3">
<a class="reference internal image-reference" href="../_images/biologicalneuron.png"><img alt="../_images/biologicalneuron.png" src="../_images/biologicalneuron.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.1 </span><span class="caption-text">Biological neuron. Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Neuron">https://en.wikipedia.org/wiki/Neuron</a></span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>Neurons are negatively charged: they have a resting potential at around -70 mV. When a neuron receives enough input currents, its <strong>membrane potential</strong> can exceed a threshold and the neuron emits an <strong>action potential</strong> (or <strong>spike</strong>) along its axon.</p>
<div class="figure align-default" id="id4">
<a class="reference internal image-reference" href="../_images/actionpotential.gif"><img alt="../_images/actionpotential.gif" src="../_images/actionpotential.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2 </span><span class="caption-text">Propagation of an action potential along the axon. Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Action_potential">https://en.wikipedia.org/wiki/Action_potential</a></span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>A spike has a very small duration (1 or 2 ms) and its amplitude is rather constant. It is followed by a <strong>refractory period</strong> where the neuron is hyperpolarized, limiting the number of spikes per second to 200.</p>
<div class="figure align-default" id="id5">
<a class="reference internal image-reference" href="../_images/actionpotential.png"><img alt="../_images/actionpotential.png" src="../_images/actionpotential.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.3 </span><span class="caption-text">Action potential or spike. Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Action_potential">https://en.wikipedia.org/wiki/Action_potential</a></span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>The action potential arrives at the synapses and releases <strong>neurotransmitters</strong> in the synaptic cleft: glutamate (AMPA, NMDA), GABA, dopamine, serotonin, nicotin, etc…
Neurotransmitters can enter the receiving neuron through <strong>receptors</strong> and change its potential: the neuron may emit a spike too. Synaptic currents change the membrane potential of the post.synaptic neuron. The change depends on the strength of the synapse called the <strong>synaptic efficiency</strong> or <strong>weight</strong>. Some synapses are stronger than others, and have a larger influence on the post-synaptic cell.</p>
<div class="figure align-default" id="id6">
<a class="reference internal image-reference" href="../_images/chemicalsynapse.jpg"><img alt="../_images/chemicalsynapse.jpg" src="../_images/chemicalsynapse.jpg" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.4 </span><span class="caption-text">Neurotransmitter release at the synapse. Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Neuron">https://en.wikipedia.org/wiki/Neuron</a></span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://www.youtube.com/embed/WCqNn9PEELw' frameborder='0' allowfullscreen></iframe></div>
<p>The two important dimensions of the information exchanged by neurons are:</p>
<ul class="simple">
<li><p>The instantaneous <strong>frequency</strong> or <strong>firing rate</strong>: number of spikes per second (Hz).</p></li>
<li><p>The precise <strong>timing</strong> of the spike trains.</p></li>
</ul>
<div class="figure align-default" id="id7">
<a class="reference internal image-reference" href="../_images/oscillations.png"><img alt="../_images/oscillations.png" src="../_images/oscillations.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.5 </span><span class="caption-text">Neurons emit spikes at varying frequencies (firing rate) and variable timings. Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Neural_oscillation">https://en.wikipedia.org/wiki/Neural_oscillation</a></span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>The shape of the spike (amplitude, duration) does not matter much for synaptic transission: spikes can be considered as binary signals (0 or 1) occuring at precise moments of time.</p>
<p>Some neuron models called <strong>rate-coded models</strong> only represent the firing rate of a neuron and ignore spike timing at all. Other models called <strong>spiking models</strong> represent explicitly the spiking behavior.</p>
</div>
<div class="section" id="hodgkin-huxley-neurons">
<h2><span class="section-number">3.2. </span>Hodgkin-Huxley neurons<a class="headerlink" href="#hodgkin-huxley-neurons" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/WAfOUZW4rq8' frameborder='0' allowfullscreen></iframe></div>
<p>Alan Hodgkin and Andrew Huxley (Nobel prize 1963) were the first to propose a detailed mathematical model of the giant squid neuron. The membrane potential <span class="math notranslate nohighlight">\(V\)</span> of the neuron is governed by an electrical circuit, including sodium and potassium channels. The membrane has a <strong>capacitance</strong> <span class="math notranslate nohighlight">\(C\)</span> that models the dynamics of the membrane (time constant). The <strong>conductance</strong> <span class="math notranslate nohighlight">\(g_L\)</span> allows the membrane potential to relax back to its resting potential <span class="math notranslate nohighlight">\(E_L\)</span> in the absence of external currents.  External currents (synaptic inputs) perturb the membrane potential and can bring the neuron to fire an action potential.</p>
<p>Their neuron model include:</p>
<ul class="simple">
<li><p>An ordinary differential equation (ODE) for the membrane potential <span class="math notranslate nohighlight">\(v\)</span>.</p></li>
<li><p>Three ODEs for <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(h\)</span> representing potassium channel activation, sodium channel activation, and sodium channel inactivation.</p></li>
<li><p>Several parameters determined experimentally.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    a_n &amp;= 0.01 \, (v + 60) / (1.0 - \exp(-0.1\, (v + 60) ) ) \\
    a_m &amp;= 0.1 \, (v + 45) / (1.0 - \exp (- 0.1 \, ( v + 45 ))) \\
    a_h &amp;= 0.07 \, \exp(- 0.05 \, ( v + 70 )) \\
    b_n &amp;= 0.125 \, \exp (- 0.0125 \, (v + 70)) \\
    b_m &amp;= 4 \,  \exp (- (v + 70) / 80) \\
    b_h &amp;= 1/(1 + \exp (- 0.1 \, ( v + 40 )) ) \\
    &amp; \\
    \frac{dn}{dt} &amp;= a_n \, (1 - n) - b_n \, n  \\
    \frac{dm}{dt} &amp;= a_m \, (1 - m) - b_m \, m  \\
    \frac{dh}{dt} &amp;= a_h \, (1 - h) - b_h \, h  \\
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    C \, \frac{dv}{dt} = g_L \, (V_L - v) &amp;+ g_K \, n^4 \, (V_K - v) \\
        &amp; + g_\text{Na} \, m^3 \, h \, (V_\text{Na} - v) + I \\
\end{aligned}
\end{split}\]</div>
<p>These equations allow to describe very precisely how an action potential is created from external currents.</p>
<div class="figure align-default" id="id8">
<a class="reference internal image-reference" href="../_images/hodgkinhuxley-data.png"><img alt="../_images/hodgkinhuxley-data.png" src="../_images/hodgkinhuxley-data.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.6 </span><span class="caption-text">Action potential for a Hodgkin-Huxley neuron.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="spiking-neurons">
<h2><span class="section-number">3.3. </span>Spiking neurons<a class="headerlink" href="#spiking-neurons" title="Permalink to this headline">¶</a></h2>
<p>As action potentials are stereotypical, it is a waste of computational resources to model their generation precisely. What actually matters are the <strong>sub-threshold dynamics</strong>, i.e. what happens before the spike is emitted.</p>
<p>The <strong>leaky integrate-and-fire</strong> (LIF; Lapicque, 1907) neuron integrates its input current and emits a spike if the membrane potential exceeds a threshold.</p>
<div class="math notranslate nohighlight">
\[
    C \, \frac{dv}{dt} = - g_L \, (v - V_L) + I
\]</div>
<div class="math notranslate nohighlight">
\[
    \text{if} \; v &gt; V_T \; \text{emit a spike and reset.}
\]</div>
<div class="figure align-default" id="id9">
<a class="reference internal image-reference" href="../_images/LIF-data.png"><img alt="../_images/LIF-data.png" src="../_images/LIF-data.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.7 </span><span class="caption-text">Spike emission for a LIF neuron.</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<p>Other well-known spiking neuron models include:</p>
<ul class="simple">
<li><p>Izhikevich quadratic IF <span id="id1">[<a class="reference internal" href="../zreferences.html#id91">Izhikevich, 2003</a>]</span>, using a quadratic function of the membrane potential and an adaptation variable <span class="math notranslate nohighlight">\(u\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \frac{dv}{dt} = 0.04 \, v^2 + 5 \, v + 140 - u + I 
\]</div>
<div class="math notranslate nohighlight">
\[
    \frac{du}{dt} = a \, (b \, v - u)
\]</div>
<ul class="simple">
<li><p>Adaptive exponential IF (AdEx, <span id="id2">[<a class="reference internal" href="../zreferences.html#id19">Brette &amp; Gerstner, 2005</a>]</span>), using an exponential function.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    C \, \frac{dv}{dt} = -g_L \ (v - E_L) + &amp; g_L \, \Delta_T \, \exp(\frac{v - v_T}{\Delta_T}) \\
                                            &amp; + I - w
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
    \tau_w \, \frac{dw}{dt} = a \, (v - E_L) - w
\]</div>
<div class="figure align-default" id="id10">
<a class="reference internal image-reference" href="../_images/LIF-Izhi-AdEx.png"><img alt="../_images/LIF-Izhi-AdEx.png" src="../_images/LIF-Izhi-AdEx.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.8 </span><span class="caption-text">Different subthreshold dynamics between the LIF, Izhikevich and AdEx neuron models.</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>Contrary to the simple LIF model, these realistic neuron models can reproduce a variety of dynamics, as biological neurons do not all respond the same to an input current. Some fire regularly, some slow down with time, while others emit bursts of spikes. Modern spiking neuron models allow to recreate these variety of dynamics by changing a few parameters.</p>
<div class="figure align-default" id="id11">
<a class="reference internal image-reference" href="../_images/adex.png"><img alt="../_images/adex.png" src="../_images/adex.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.9 </span><span class="caption-text">Different parameters of the AdEx neuron model produce different spiking patterns.</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="rate-coded-neurons">
<h2><span class="section-number">3.4. </span>Rate-coded neurons<a class="headerlink" href="#rate-coded-neurons" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/AFzYj1VUnCg' frameborder='0' allowfullscreen></iframe></div>
<p>At the population level, interconnected networks of spiking neurons tend to fire synchronously (code redundancy). What if the important information was not the precise spike timings, but the <strong>firing rate</strong> of a small population? The instantaneous firing rate is defined in Hz (number of spikes per second). It can be estimated by an histogram of the spikes emitted by a network of similar neurons, or by repeating the same experiment multiple times for a single neuron. One can also build neural models that directly model the <strong>firing rate</strong> of (a population of) neuron(s): the <strong>rate-coded</strong> neuron.</p>
<div class="figure align-default" id="id12">
<a class="reference internal image-reference" href="../_images/ratecoded-izhikevich.png"><img alt="../_images/ratecoded-izhikevich.png" src="../_images/ratecoded-izhikevich.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.10 </span><span class="caption-text">The spiking pattern (raster plot) of a population of interconnected neurons can be approximated by its mean firing rate.</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p>A rate-coded neuron is represented by two time-dependent variables:</p>
<ul class="simple">
<li><p>The <strong>“membrane potential”</strong> <span class="math notranslate nohighlight">\(v(t)\)</span> which evolves over time using an ODE.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \tau \, \frac{d v(t)}{dt} + v(t) = \sum_{i=1}^d w_{i, j} \, r_i(t) + b
\]</div>
<ul class="simple">
<li><p>The <strong>firing rate</strong> <span class="math notranslate nohighlight">\(r(t)\)</span> which transforms the membrane potential into a single continuous value using a <strong>transfer function</strong> or <strong>activation function</strong>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    r(t) = f(v(t))
\]</div>
<div class="figure align-default" id="id13">
<a class="reference internal image-reference" href="../_images/ratecoded-neuron.svg"><img alt="../_images/ratecoded-neuron.svg" src="../_images/ratecoded-neuron.svg" width="70%" /></a>
<p class="caption"><span class="caption-number">Fig. 3.11 </span><span class="caption-text">Rate-coded neuron.</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<p>The membrane potential uses a weighted sum of inputs (the firing rates <span class="math notranslate nohighlight">\(r_i(t)\)</span> of other neurons) by multiplying each rate with a <strong>weight</strong> <span class="math notranslate nohighlight">\(w_i\)</span> and adds a constant value <span class="math notranslate nohighlight">\(b\)</span> (the <strong>bias</strong>). The activation function can be any non-linear function, usually making sure that the firing rate is positive.</p>
<div class="figure align-default" id="id14">
<a class="reference internal image-reference" href="../_images/ratecoded-simple.png"><img alt="../_images/ratecoded-simple.png" src="../_images/ratecoded-simple.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.12 </span><span class="caption-text">Firing rate of a rate-coded neuron for a step input.</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Remarks on ODEs</strong></p>
<p>Let’s consider a simple rate-coded neuron taking a step signal <span class="math notranslate nohighlight">\(I(t)\)</span> as input:</p>
<div class="math notranslate nohighlight">
\[
    \tau \, \frac{d v(t)}{dt} + v(t) = I(t)
\]</div>
<div class="math notranslate nohighlight">
\[
    r(t) = (v(t))^+
\]</div>
<p>The “speed” of <span class="math notranslate nohighlight">\(v(t)\)</span> is given by its temporal derivative:</p>
<div class="math notranslate nohighlight">
\[
    \frac{d v(t)}{dt} = \frac{I(t) - v(t)}{\tau}
\]</div>
<p>When <span class="math notranslate nohighlight">\(v(t)\)</span> is quite different from <span class="math notranslate nohighlight">\(I(t)\)</span>, the membrane potential “accelerates” to reduce the difference. When <span class="math notranslate nohighlight">\(v(t)\)</span> is similar to <span class="math notranslate nohighlight">\(I(t)\)</span>, the membrane potential stays constant.</p>
<div class="figure align-default" id="id15">
<a class="reference internal image-reference" href="../_images/ratecoded-simple-multiple.png"><img alt="../_images/ratecoded-simple-multiple.png" src="../_images/ratecoded-simple-multiple.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.13 </span><span class="caption-text">The time constant <span class="math notranslate nohighlight">\(\tau\)</span> of a rate-coded neuron influences the speed at which it reacts to inputs.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p>The membrane potential follows an exponential function which tries to “match” its input with a speed determined by the <strong>time constant</strong> <span class="math notranslate nohighlight">\(\tau\)</span>. The time constant <span class="math notranslate nohighlight">\(\tau\)</span> determines how fast the rate-coded neuron matches its inputs. Biological neurons have time constants between 5 and 30 ms depending on the cell type.</p>
<p>There exists a significant number of transfer functions that can be used:</p>
<div class="figure align-default" id="id16">
<a class="reference internal image-reference" href="../_images/ratecoded-transferfunctions.png"><img alt="../_images/ratecoded-transferfunctions.png" src="../_images/ratecoded-transferfunctions.png" style="width: 70%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.14 </span><span class="caption-text">Typical transfer functions used in neural networks include the rectifier (ReLU), piece-wise linear, sigmoid (or logistic) and tanh functions..</span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
<p>When using the rectifier activation function (ReLU):</p>
<div class="math notranslate nohighlight">
\[
    f(x) = \max(0, x)
\]</div>
<p>the membrane potential <span class="math notranslate nohighlight">\(v(t)\)</span> can take any value, but the firing rate <span class="math notranslate nohighlight">\(r(t)\)</span> is only positive.</p>
<div class="figure align-default" id="id17">
<a class="reference internal image-reference" href="../_images/ratecoded-simple2.png"><img alt="../_images/ratecoded-simple2.png" src="../_images/ratecoded-simple2.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.15 </span><span class="caption-text">The rectifier function only keeps the positive part of the membrane potential.</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<p>When using the logistic (or sigmoid) activation function:</p>
<div class="math notranslate nohighlight">
\[
    f(x) = \frac{1}{1 + \exp(-x)}
\]</div>
<p>the firing rate <span class="math notranslate nohighlight">\(r(t)\)</span> is bounded between 0 and 1, but responds for negative membrane potentials.</p>
<div class="figure align-default" id="id18">
<a class="reference internal image-reference" href="../_images/ratecoded-simple3.png"><img alt="../_images/ratecoded-simple3.png" src="../_images/ratecoded-simple3.png" style="width: 80%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.16 </span><span class="caption-text">The sigmoid/logistic function bounds the firing rate between 0 and 1, even if the membrane potential is negative.</span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="artificial-neurons">
<h2><span class="section-number">3.5. </span>Artificial neurons<a class="headerlink" href="#artificial-neurons" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/NVj17dve8B8' frameborder='0' allowfullscreen></iframe></div>
<p>By omitting the dynamics of the rate-coded neuron, one obtains the very simple <strong>artificial neuron</strong> (McCulloch and Pitts, 1943):</p>
<div class="figure align-default" id="id19">
<a class="reference internal image-reference" href="../_images/artificialneuron.svg"><img alt="../_images/artificialneuron.svg" src="../_images/artificialneuron.svg" width="70%" /></a>
<p class="caption"><span class="caption-number">Fig. 3.17 </span><span class="caption-text">Artificial neuron.</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<p>An artificial neuron sums its inputs <span class="math notranslate nohighlight">\(x_1, \ldots, x_d\)</span> by multiplying them with weights <span class="math notranslate nohighlight">\(w_1, \ldots, w_d\)</span>, adds a bias <span class="math notranslate nohighlight">\(b\)</span> and transforms the result into an output <span class="math notranslate nohighlight">\(y\)</span> using an activation function <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    y = f( \sum_{i=1}^d w_i \, x_i + b)
\]</div>
<p>The output <span class="math notranslate nohighlight">\(y\)</span> directly reflects the input, without temporal integration. The weighted sum of inputs + bias <span class="math notranslate nohighlight">\(\sum_{i=1}^d w_i \, x_i + b\)</span> is called the <strong>net activation</strong>.</p>
<p>This overly simplified neuron model is the basic unit of the <strong>artificial neural networks</strong> (ANN) used in machine learning / deep learning.</p>
<p><strong>Artificial neurons and hyperplanes</strong></p>
<p>Let’s consider an artificial neuron with only two inputs <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>.</p>
<p>The net activation <span class="math notranslate nohighlight">\(w_1 \, x_1 + w_2 \, x_2 + b\)</span> is the equation of a line in the space <span class="math notranslate nohighlight">\((x_1, x_2)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    w_1 \, x_1 + w_2 \, x_2 + b = 0 \Leftrightarrow x_2 = - \frac{w_1}{w_2} \, x_1 - \frac{b}{w_2}
\]</div>
<div class="figure align-default" id="id20">
<a class="reference internal image-reference" href="../_images/artificialneuron-simple.png"><img alt="../_images/artificialneuron-simple.png" src="../_images/artificialneuron-simple.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.18 </span><span class="caption-text">The net activation represents an hyperplane in 2D.</span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</div>
<p>The net activation is a line in 2D, a plane in 3D, etc. Generally, the net activation describes an <strong>hyperplane</strong> in the input space with <span class="math notranslate nohighlight">\(d\)</span> dimensions <span class="math notranslate nohighlight">\((x_1, x_2, \ldots, x_d)\)</span>.  An hyperplane has one dimension less than the space.</p>
<div class="figure align-default" id="id21">
<a class="reference internal image-reference" href="../_images/hyperplane.gif"><img alt="../_images/hyperplane.gif" src="../_images/hyperplane.gif" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.19 </span><span class="caption-text">Hyperplane in 3D. Source: <a class="reference external" href="https://newvitruvian.com/explore/vector-planes/#gal_post_7186_nonzero-vector.gif">https://newvitruvian.com/explore/vector-planes/#gal_post_7186_nonzero-vector.gif</a></span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</div>
<p>We can write the net activation using a <strong>weight vector</strong> <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and a <strong>bias</strong> <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \sum_{i=1}^d w_i \, x_i + b  = \langle\mathbf{w} \cdot \mathbf{x} \rangle + b
\]</div>
<p>with:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \ldots \\ w_d \end{bmatrix} \qquad \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \ldots \\ x_d \end{bmatrix}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\langle \cdot \rangle\)</span> is the <strong>dot product</strong> (aka inner product, scalar product) between the <strong>input vector</strong> <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and the weight vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</p>
<p>The weight vector is orthogonal to the hyperplane <span class="math notranslate nohighlight">\((\mathbf{w}, b)\)</span> and defines its orientation. <span class="math notranslate nohighlight">\(b\)</span> is the “distance” between the hyperplane and the origin. The hyperplane separates the input space into two parts:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\langle\mathbf{w} \cdot \mathbf{x} \rangle + b &gt; 0\)</span> for all points <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> <strong>above</strong> the hyperplane.</p></li>
<li><p><span class="math notranslate nohighlight">\(\langle\mathbf{w} \cdot \mathbf{x} \rangle + b &lt; 0\)</span> for all points <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> <strong>below</strong> the hyperplane.</p></li>
</ul>
<p>By looking at the <strong>sign</strong> of the net activation, we can separate the input space into two classes. This will be the main principle of <strong>linear classification</strong>.</p>
<div class="figure align-default" id="id22">
<a class="reference internal image-reference" href="../_images/projection.svg"><img alt="../_images/projection.svg" src="../_images/projection.svg" width="80%" /></a>
<p class="caption"><span class="caption-number">Fig. 3.20 </span><span class="caption-text">The sign of the projection of an input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> on the hyperplane tells whether the input is above or below the hyperplane.</span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./1-intro"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="2-Math.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">2. </span>Math basics (optional)</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="../2-linear/1-Optimization.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">1. </span>Optimization</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>