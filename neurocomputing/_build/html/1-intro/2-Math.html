

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Math basics &#8212; Neurocomputing</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/1-intro/2-Math.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Neurons" href="3-Neurons.html" />
    <link rel="prev" title="1. Introduction" href="1-Introduction.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-neurocomputing/1-intro/2-Math.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Math basics" />
<meta property="og:description" content="Math basics  This chapter is not part of the course itself (there will not be questions at the exam on basic mathematics) but serves as a reminder of the import" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-neurocomputing/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neurocomputing</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Math basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  1 - Linear models
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-MaximumLikelihoodEstimation.html">
   5. Maximum Likelihood Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-MultiClassification.html">
   6. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/7-LearningTheory.html">
   7. Learning theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  2 - Deep learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/1-NN.html">
   1. Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/5-Autoencoders.html">
   5. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/6-RBM.html">
   6. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/7-Segnets.html">
   7. Segmentation networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/8-GAN.html">
   8. Generative Adversarial Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  3 - Neurocomputing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/3-NeuralFields.html">
   3. Neural Fields (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   4. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   5. Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   6. Spiking networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and matplotlib
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/1-intro/2-Math.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-algebra">
   2.1. Linear algebra
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vectors">
     2.1.1. Vectors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrices">
     2.1.2. Matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#calculus">
   2.2. Calculus
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#univariate-functions">
     2.2.1. Univariate functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariate-functions">
     2.2.2. Multivariate functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivatives-gradient">
     2.2.3. Derivatives, gradient
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-theory">
   2.3. Probability theory
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-probability-distributions">
     2.3.1. Discrete probability distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-probability-distributions">
     2.3.2. Continuous probability distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#standard-distributions">
     2.3.3. Standard distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#joint-and-conditional-probabilities">
     2.3.4. Joint and conditional probabilities
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-rule">
     2.3.5. Bayes’ rule
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistics">
   2.4. Statistics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo-sampling">
     2.4.1. Monte Carlo sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#central-limit-theorem">
     2.4.2. Central limit theorem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimators">
     2.4.3. Estimators
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#information-theory">
   2.5. Information theory
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#entropy">
     2.5.1. Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mutual-information-cross-entropy-and-kullback-leibler-divergence">
     2.5.2. Mutual Information, cross-entropy and Kullback-Leibler divergence
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="math-basics">
<h1><span class="section-number">2. </span>Math basics<a class="headerlink" href="#math-basics" title="Permalink to this headline">¶</a></h1>
<p>This chapter is not part of the course itself (there will not be questions at the exam on basic mathematics) but serves as a reminder of the important mathematical notions that are needed to understand this course. Students who have studied mathematics as a major can safely skip this part, as there is nothing fancy (although the section on information theory could be worth a read).</p>
<p>It is not supposed to replace any course in mathematics (we won’t show any proof and will skip what we do not need) but rather to provide a high-level understanding of the most important concepts and set the notations. Nothing should be really new to you, but it may be useful to have everything summarized at the same place.</p>
<p><strong>References:</strong> Part I of Goodfellow et al. (2016) <a class="bibtex reference internal" href="../zreferences.html#goodfellow2016" id="id1">[GBC16]</a>. Any mathematics textbook can be used in addition.</p>
<div class="section" id="linear-algebra">
<h2><span class="section-number">2.1. </span>Linear algebra<a class="headerlink" href="#linear-algebra" title="Permalink to this headline">¶</a></h2>
<p>Several mathematical objects are manipulated in linear algebra:</p>
<ul class="simple">
<li><p><strong>Scalars</strong> <span class="math notranslate nohighlight">\(x\)</span> are 0-dimensional values (single numbers, so to speak). They can either take real values (<span class="math notranslate nohighlight">\(x \in \Re\)</span>, e.g. <span class="math notranslate nohighlight">\(x = 1.4573\)</span>, floats in CS) or natural values (<span class="math notranslate nohighlight">\(x \in \mathbb{N}\)</span>, e.g. <span class="math notranslate nohighlight">\(x = 3\)</span>, integers in CS).</p></li>
<li><p><strong>Vectors</strong> <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are 1-dimensional arrays of length <span class="math notranslate nohighlight">\(d\)</span>. The bold notation <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> will be used in this course, but you may also be accustomed to the arrow notation <span class="math notranslate nohighlight">\(\overrightarrow{x}\)</span> used on the blackboard. When using real numbers, the <strong>vector space</strong> with <span class="math notranslate nohighlight">\(d\)</span> dimensions is noted <span class="math notranslate nohighlight">\(\Re^d\)</span>, so we can note <span class="math notranslate nohighlight">\(\mathbf{x} \in \Re^d\)</span>. Vectors are typically represented vertically to outline their <span class="math notranslate nohighlight">\(d\)</span> elements <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_d\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix}\end{split}\]</div>
<ul class="simple">
<li><p><strong>Matrices</strong> <span class="math notranslate nohighlight">\(A\)</span> are 2-dimensional arrays of size (or shape) <span class="math notranslate nohighlight">\(m \times n\)</span> (<span class="math notranslate nohighlight">\(m\)</span> rows, <span class="math notranslate nohighlight">\(n\)</span> columns, <span class="math notranslate nohighlight">\(A \in \Re^{m \times n}\)</span>). They are represented by a capital letter to distinguish them from scalars. The element <span class="math notranslate nohighlight">\(a_{ij}\)</span> of a matrix <span class="math notranslate nohighlight">\(A\)</span> is the element on the <span class="math notranslate nohighlight">\(i\)</span>-th row and <span class="math notranslate nohighlight">\(j\)</span>-th column.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{bmatrix}
 a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
 a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix}\end{split}\]</div>
<ul class="simple">
<li><p><strong>Tensors</strong> <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> are arrays with more than two dimensions. We will not really do math on these objects, but they are useful internally (hence the name of the <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> library).</p></li>
</ul>
<div class="section" id="vectors">
<h3><span class="section-number">2.1.1. </span>Vectors<a class="headerlink" href="#vectors" title="Permalink to this headline">¶</a></h3>
<p>A vector can be thought of as the <strong>coordinates of a point</strong> in an Euclidian space (such the 2D space), relative to the origin. A vector space relies on two fundamental operations, which are that:</p>
<ul class="simple">
<li><p>Vectors can be added:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x} + \mathbf{y} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} + \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_d \end{bmatrix} = \begin{bmatrix} x_1 + y_1 \\ x_2 + y_2 \\ \vdots \\ x_d + y_d \end{bmatrix}\end{split}\]</div>
<ul class="simple">
<li><p>Vectors can be multiplied by a scalar:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}a \, \mathbf{x} = a \, \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} = \begin{bmatrix} a \, x_1 \\ a \, x_2 \\ \vdots \\ a \, x_d \end{bmatrix}\end{split}\]</div>
<div class="figure align-default" id="id2">
<a class="reference internal image-reference" href="../_images/vectorspace.png"><img alt="../_images/vectorspace.png" src="../_images/vectorspace.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.1 </span><span class="caption-text">Vector spaces allow additions of vectors. Source: <a class="reference external" href="https://mathinsight.org/image/vector_2d_add">https://mathinsight.org/image/vector_2d_add</a></span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>These two operations generate of lot of nice properties (see <a class="reference external" href="https://en.wikipedia.org/wiki/Vector_space">https://en.wikipedia.org/wiki/Vector_space</a> for a full list), including:</p>
<ul class="simple">
<li><p>associativity: <span class="math notranslate nohighlight">\(\mathbf{x} + (\mathbf{y} + \mathbf{z}) = (\mathbf{x} + \mathbf{y}) + \mathbf{z}\)</span></p></li>
<li><p>commutativity: <span class="math notranslate nohighlight">\(\mathbf{x} + \mathbf{y} = \mathbf{y} + \mathbf{x}\)</span></p></li>
<li><p>the existence of a zero vector <span class="math notranslate nohighlight">\(\mathbf{x} + \mathbf{0} = \mathbf{x}\)</span></p></li>
<li><p>inversion: <span class="math notranslate nohighlight">\(\mathbf{x} + (-\mathbf{x}) = \mathbf{0}\)</span></p></li>
<li><p>distributivity: <span class="math notranslate nohighlight">\(a \, (\mathbf{x} + \mathbf{y}) = a \, \mathbf{x} + a \, \mathbf{y}\)</span></p></li>
</ul>
<p>Vectors have a <strong>norm</strong> (or length) <span class="math notranslate nohighlight">\(||\mathbf{x}||\)</span>. The most intuive one (if you know the Pythagoras theorem) is the <strong>Euclidian norm</strong> or <span class="math notranslate nohighlight">\(L^2\)</span>-norm, which sums the square of each element:</p>
<div class="math notranslate nohighlight">
\[||\mathbf{x}||_2 = \sqrt{x_1^2 + x_2^2 + \ldots + x_d^2}\]</div>
<p>Other norms exist, distinguished by the subscript. The <strong><span class="math notranslate nohighlight">\(L^1\)</span>-norm</strong> (also called Taxicab or Manhattan norm) sums the absolute value of each element:</p>
<div class="math notranslate nohighlight">
\[||\mathbf{x}||_1 = |x_1| + |x_2| + \ldots + |x_d|\]</div>
<p>The <strong>p-norm</strong> generalizes the Euclidian norm to other powers <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="math notranslate nohighlight">
\[||\mathbf{x}||_p = (|x_1|^p + |x_2|^p + \ldots + |x_d|^p)^{\frac{1}{p}}\]</div>
<p>The <strong>infinity norm</strong> (or maximum norm) <span class="math notranslate nohighlight">\(L^\infty\)</span> returns the maximum element of the vector:</p>
<div class="math notranslate nohighlight">
\[||\mathbf{x}||_\infty = \max(|x_1|, |x_2|, \ldots, |x_d|)\]</div>
<p>One important operation for vectors is the <strong>dot product</strong> (also called scalar product or inner product) between two vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\langle \mathbf{x} \cdot \mathbf{y} \rangle = \langle \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} \cdot \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_d \end{bmatrix} \rangle = x_1 \, y_1 + x_2 \, y_2 + \ldots + x_d \, y_d\end{split}\]</div>
<p>The dot product basically sums one by one the product of the elements of each vector. The angular brackets are sometimes omitted (<span class="math notranslate nohighlight">\(\mathbf{x} \cdot \mathbf{y}\)</span>) but we will use them in this course for clarity.</p>
<p>One can notice immediately that the dot product is <strong>symmetric</strong>:</p>
<div class="math notranslate nohighlight">
\[\langle \mathbf{x} \cdot \mathbf{y} \rangle = \langle \mathbf{y} \cdot \mathbf{x} \rangle\]</div>
<p>and <strong>linear</strong>:</p>
<div class="math notranslate nohighlight">
\[\langle (a \, \mathbf{x} + b\, \mathbf{y}) \cdot \mathbf{z} \rangle = a\, \langle \mathbf{x} \cdot \mathbf{z} \rangle + b \, \langle \mathbf{y} \cdot \mathbf{z} \rangle\]</div>
<p>The dot product is an indirect measurement of the <strong>angle</strong> <span class="math notranslate nohighlight">\(\theta\)</span> between two vectors:</p>
<div class="math notranslate nohighlight">
\[\langle \mathbf{x} \cdot \mathbf{y} \rangle = ||\mathbf{x}||_2 \, ||\mathbf{y}||_2 \, \cos(\theta)\]</div>
<div class="figure align-default" id="id3">
<a class="reference internal image-reference" href="../_images/dotproduct.png"><img alt="../_images/dotproduct.png" src="../_images/dotproduct.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.2 </span><span class="caption-text">The dot product between two vectors is proportional to the cosine of the angle between the two vectors. Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Dot_product">https://en.wikipedia.org/wiki/Dot_product</a></span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>If you normalize the two vectors by dividing them by their norm (which is a scalar), we indeed have the cosine of the angle between them: The higher the normalized dot product, the more the two vectors point towards the same direction (<strong>cosine distance</strong> between two vectors).</p>
<div class="math notranslate nohighlight">
\[\langle \displaystyle\frac{\mathbf{x}}{||\mathbf{x}||_2} \cdot \frac{\mathbf{y}}{||\mathbf{y}||_2} \rangle =  \cos(\theta)\]</div>
</div>
<div class="section" id="matrices">
<h3><span class="section-number">2.1.2. </span>Matrices<a class="headerlink" href="#matrices" title="Permalink to this headline">¶</a></h3>
<p>Matrices are derived from vectors, so most of the previous properties will be true. Let’s consider this 4x3 matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} \\
a_{41} &amp; a_{42} &amp; a_{43} \\
\end{bmatrix}\end{split}\]</div>
<p>Each column of the matrix is a vector with 4 elements:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{a}_1 = \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31} \\
a_{41} \\
\end{bmatrix} \qquad
\mathbf{a}_2 = \begin{bmatrix}
a_{12} \\
a_{22} \\
a_{32} \\
a_{42} \\
\end{bmatrix} \qquad
\mathbf{a}_3 = \begin{bmatrix}
a_{13} \\
a_{23} \\
a_{33} \\
a_{43} \\
\end{bmatrix} \qquad
\end{split}\]</div>
<p>A <span class="math notranslate nohighlight">\(m \times n\)</span> matrix is therefore a collection of <span class="math notranslate nohighlight">\(n\)</span> vectors of size <span class="math notranslate nohighlight">\(m\)</span> put side by side column-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{bmatrix}
\mathbf{a}_1 &amp; \mathbf{a}_2 &amp; \mathbf{a}_3\\
\end{bmatrix}\end{split}\]</div>
<p>So all properties of the vector spaces (associativity, commutativity, distributivity) also apply to matrices, as additions and multiplications with a scalar are defined.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\alpha \, A + \beta \, B = \begin{bmatrix}
\alpha\, a_{11} + \beta \, b_{11} &amp; \alpha\, a_{12} + \beta \, b_{12} &amp; \alpha\, a_{13} + \beta \, b_{13} \\
\alpha\, a_{21} + \beta \, b_{21} &amp; \alpha\, a_{22} + \beta \, b_{22} &amp; \alpha\, a_{23} + \beta \, b_{23} \\
\alpha\, a_{31} + \beta \, b_{31} &amp; \alpha\, a_{32} + \beta \, b_{32} &amp; \alpha\, a_{33} + \beta \, b_{33} \\
\alpha\, a_{41} + \beta \, b_{41} &amp; \alpha\, a_{42} + \beta \, b_{42} &amp; \alpha\, a_{43} + \beta \, b_{43} \\
\end{bmatrix}\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Beware, you can only add matrices of the same dimensions <span class="math notranslate nohighlight">\(m\times n\)</span>. You cannot add a <span class="math notranslate nohighlight">\(2\times 3\)</span> matrix to a <span class="math notranslate nohighlight">\(5 \times 4\)</span> one.</p>
</div>
<p>The <strong>transpose</strong> <span class="math notranslate nohighlight">\(A^T\)</span> of a <span class="math notranslate nohighlight">\(m \times n\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> is a <span class="math notranslate nohighlight">\(n \times m\)</span> matrix, where the row and column indices are swapped:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{bmatrix}
 a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
 a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix}, \qquad
A^T = \begin{bmatrix}
 a_{11} &amp; a_{21} &amp; \cdots &amp; a_{m1} \\
 a_{12} &amp; a_{22} &amp; \cdots &amp; a_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 a_{1n} &amp; a_{2n} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix}
\end{split}\]</div>
<p>This is also true for vectors, which become horizontal after transposition:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix}, \qquad
\mathbf{x}^T = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_d \end{bmatrix}
\end{split}\]</div>
<p>A very important operation is the <strong>matrix multiplication</strong>. If <span class="math notranslate nohighlight">\(A\)</span> is a <span class="math notranslate nohighlight">\(m\times n\)</span> matrix and <span class="math notranslate nohighlight">\(B\)</span> a <span class="math notranslate nohighlight">\(n \times p\)</span> matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A=\begin{bmatrix}
 a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
 a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix},\quad
B=\begin{bmatrix}
 b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1p} \\
 b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 b_{n1} &amp; b_{n2} &amp; \cdots &amp; b_{np} \\
\end{bmatrix}
\end{split}\]</div>
<p>we can multiply them to obtain a <span class="math notranslate nohighlight">\(m \times p\)</span> matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
C = A \times B =\begin{bmatrix}
 c_{11} &amp; c_{12} &amp; \cdots &amp; c_{1p} \\
 c_{21} &amp; c_{22} &amp; \cdots &amp; c_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 c_{m1} &amp; c_{m2} &amp; \cdots &amp; c_{mp} \\
\end{bmatrix}
\end{split}\]</div>
<p>where each element <span class="math notranslate nohighlight">\(c_{ij}\)</span> is the dot product of the <span class="math notranslate nohighlight">\(i\)</span>th row of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(j\)</span>th column of <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="math notranslate nohighlight">
\[c_{ij} = \langle A_{i, :} \cdot B_{:, j} \rangle = a_{i1}b_{1j} + a_{i2}b_{2j} +\cdots + a_{in}b_{nj}= \sum_{k=1}^n a_{ik}b_{kj}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(n\)</span>, the number of columns of <span class="math notranslate nohighlight">\(A\)</span> and rows of <span class="math notranslate nohighlight">\(B\)</span>, must be the same!</p>
</div>
<div class="figure align-default" id="id4">
<a class="reference internal image-reference" href="../_images/matrixmultiplication.jpg"><img alt="../_images/matrixmultiplication.jpg" src="../_images/matrixmultiplication.jpg" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.3 </span><span class="caption-text">The element <span class="math notranslate nohighlight">\(c_{ij}\)</span> of <span class="math notranslate nohighlight">\(C = A \times B\)</span> is the dot product between the <span class="math notranslate nohighlight">\(i\)</span>th row of <span class="math notranslate nohighlight">\(A\)</span> and the <span class="math notranslate nohighlight">\(j\)</span>th column of <span class="math notranslate nohighlight">\(B\)</span>. Source: <a class="reference external" href="https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_(Levitus)/15%3A_Matrices/15.03%3A_Matrix_Multiplication">https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_(Levitus)/15%3A_Matrices/15.03%3A_Matrix_Multiplication</a> CC BY-NC-SA; Marcia Levitus</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>Thinking of vectors as <span class="math notranslate nohighlight">\(n \times 1\)</span> matrices, we can multiply a matrix <span class="math notranslate nohighlight">\(m \times n\)</span> with a vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{y} = A \times \mathbf{x} = \begin{bmatrix}
 a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
 a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix}
\end{split}\]</div>
<p>The result <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is a vector of size <span class="math notranslate nohighlight">\(m\)</span>. In that sense, a matrix <span class="math notranslate nohighlight">\(A\)</span> can transform a vector of size <span class="math notranslate nohighlight">\(n\)</span> to a vector of size <span class="math notranslate nohighlight">\(m\)</span>: <span class="math notranslate nohighlight">\(A\)</span> represents a <strong>projection</strong> from <span class="math notranslate nohighlight">\(\Re^n\)</span> to <span class="math notranslate nohighlight">\(\Re^m\)</span>.</p>
<div class="figure align-default" id="id5">
<a class="reference internal image-reference" href="../_images/projection.png"><img alt="../_images/projection.png" src="../_images/projection.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 2.4 </span><span class="caption-text">A <span class="math notranslate nohighlight">\(2 \times 3\)</span> projection matrix allows to project any 3D vector onto a 2D plane. This is for example what happens inside a camera. Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Homogeneous_coordinate">https://en.wikipedia.org/wiki/Homogeneous_coordinate</a></span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>Note that the <strong>dot product</strong> between two vectors of size <span class="math notranslate nohighlight">\(n\)</span> is the matrix multiplication between the transpose of the first vector and the second one:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x}^T \times \mathbf{y} = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{bmatrix} \times \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} = x_1 \, y_1 + x_2 \, y_2 + \ldots + x_n \, y_n = \langle \mathbf{x} \cdot \mathbf{y} \rangle\end{split}\]</div>
<p>Square matrices of size <span class="math notranslate nohighlight">\(n \times n\)</span> can be inverted. The <strong>inverse</strong> <span class="math notranslate nohighlight">\(A^{-1}\)</span> of a matrix <span class="math notranslate nohighlight">\(A\)</span> is defined by:</p>
<div class="math notranslate nohighlight">
\[A \times A^{-1} = A^{-1} \times A = I\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> is the identity matrix (a matrix with ones on the diagonal and 0 otherwise). Not all matrices have an inverse (those who don’t are called singular or degenerate). There are plenty of conditions for a matrix to be invertible (for example its determinant is non-zero, see <a class="reference external" href="https://en.wikipedia.org/wiki/Invertible_matrix">https://en.wikipedia.org/wiki/Invertible_matrix</a>), but they will not matter in this course. Non-square matrices are generally not invertible, but see the pseudoinverse (<a class="reference external" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse</a>).</p>
<p>Matrix inversion allows to solve linear systems of equations. Given the problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    a_{11} \, x_1 + a_{12} \, x_2 + \ldots + a_{1n} \, x_n = b_1 \\
    a_{21} \, x_1 + a_{22} \, x_2 + \ldots + a_{2n} \, x_n = b_2 \\
    \ldots \\
    a_{n1} \, x_1 + a_{n2} \, x_2 + \ldots + a_{nn} \, x_n = b_n \\
\end{cases}
\end{split}\]</div>
<p>which is equivalent to:</p>
<div class="math notranslate nohighlight">
\[A \times \mathbf{x} = \mathbf{b}\]</div>
<p>we can multiply both sides to the left with <span class="math notranslate nohighlight">\(A^{-1}\)</span> (if it exists) and obtain:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x} = A^{-1} \times \mathbf{b}\]</div>
</div>
</div>
<div class="section" id="calculus">
<h2><span class="section-number">2.2. </span>Calculus<a class="headerlink" href="#calculus" title="Permalink to this headline">¶</a></h2>
<div class="section" id="univariate-functions">
<h3><span class="section-number">2.2.1. </span>Univariate functions<a class="headerlink" href="#univariate-functions" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="multivariate-functions">
<h3><span class="section-number">2.2.2. </span>Multivariate functions<a class="headerlink" href="#multivariate-functions" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="derivatives-gradient">
<h3><span class="section-number">2.2.3. </span>Derivatives, gradient<a class="headerlink" href="#derivatives-gradient" title="Permalink to this headline">¶</a></h3>
<p>Chain rule</p>
</div>
</div>
<div class="section" id="probability-theory">
<h2><span class="section-number">2.3. </span>Probability theory<a class="headerlink" href="#probability-theory" title="Permalink to this headline">¶</a></h2>
<div class="section" id="discrete-probability-distributions">
<h3><span class="section-number">2.3.1. </span>Discrete probability distributions<a class="headerlink" href="#discrete-probability-distributions" title="Permalink to this headline">¶</a></h3>
<p>Let’s note <span class="math notranslate nohighlight">\(X\)</span> a <strong>discrete random variable</strong> with <span class="math notranslate nohighlight">\(n\)</span> realizations <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>. The <strong>probability</strong> that <span class="math notranslate nohighlight">\(X\)</span> takes the value <span class="math notranslate nohighlight">\(x_i\)</span> is defined by the <strong>relative frequency of occurrence</strong>, i.e. the proportion of samples having the value <span class="math notranslate nohighlight">\(x_i\)</span>, when the total number <span class="math notranslate nohighlight">\(N\)</span> of samples tends to infinity:</p>
<div class="math notranslate nohighlight">
\[
    P(X = x_i) = \frac{\text{Number of favorable cases}}{\text{Total number of samples}}
\]</div>
<p>The set of probabilities <span class="math notranslate nohighlight">\(\{P(X = x_i)\}_{i=1}^n\)</span> define the <strong>probability distribution</strong> for the random variable (or probability mass function, pmf). By definition, we have <span class="math notranslate nohighlight">\(0 \leq P(X = x_i) \leq 1\)</span> and the probabilities <strong>have</strong> to respect:</p>
<div class="math notranslate nohighlight">
\[
    \sum_{i=1}^n P(X = x_i) = 1
\]</div>
<p>An important metric for a random variable is its <strong>mathematical expectation</strong> or expected value, i.e. its “mean” realization weighted by the probabilities:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[X] = \sum_{i=1}^n P(X = x_i) \, x_i
\]</div>
<p>The expectation does not even need to be a valid realization:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[\text{Coin}] = \frac{1}{2} \, 0 + \frac{1}{2} \, 1 = 0.5
\]</div>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[\text{Dice}] = \frac{1}{6} \, (1 + 2 + 3 + 4 + 5 + 6) = 3.5
\]</div>
<p>We can also compute the mathematical expectation of <strong>functions of</strong> a random variable:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[f(X)] = \sum_{i=1}^n P(X = x_i) \, f(x_i)
\]</div>
<p>The <strong>variance</strong> of a random variable is the squared deviation around the mean:</p>
<div class="math notranslate nohighlight">
\[
    \text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \sum_{i=1}^n P(X = x_i) \, (x_i - \mathbb{E}[X])^2
\]</div>
<p>Variance of a coin:</p>
<div class="math notranslate nohighlight">
\[
    \text{Var}(\text{Coin}) = \frac{1}{2} \, (0 - 0.5)^2 + \frac{1}{2} \, (1 - 0.5)^2 = 0.25
\]</div>
<p>Variance of a dice:</p>
<div class="math notranslate nohighlight">
\[
    \text{Var}(\text{Dice}) = \frac{1}{6} \, ((1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2) = \frac{105}{36}
\]</div>
</div>
<div class="section" id="continuous-probability-distributions">
<h3><span class="section-number">2.3.2. </span>Continuous probability distributions<a class="headerlink" href="#continuous-probability-distributions" title="Permalink to this headline">¶</a></h3>
<p><strong>Continuous random variables</strong> can take an infinity of continuous values, e.g. <span class="math notranslate nohighlight">\(\Re\)</span> or some subset. The closed set of values they can take is called the <strong>support</strong> <span class="math notranslate nohighlight">\(\mathcal{D}_X\)</span> of the probability distribution. The probability distribution is described by a <strong>probability density function</strong> (pdf) <span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
<p>The pdf of a distribution must be positive (<span class="math notranslate nohighlight">\(f(x) \geq 0 \, \forall x \in \mathcal{D}_X\)</span>) and its integral must be equal to 1:</p>
<div class="math notranslate nohighlight">
\[
    \int_{x \in \mathcal{D}_X} f(x) \, dx = 1
\]</div>
<p>The pdf does not give the probability of taking a particular value <span class="math notranslate nohighlight">\(x\)</span> (it is 0), but allows to get the probability that a value lies in a specific interval:</p>
<div class="math notranslate nohighlight">
\[
    P(a \leq X \leq b) = \int_{a}^b f(x) \, dx 
\]</div>
<p>One can however think of the pdf as the <strong>likelihood</strong> that a value <span class="math notranslate nohighlight">\(x\)</span> comes from that distribution.</p>
<p>For continuous distributions, the mathematical expectation is now defined by an integral instead of a sum:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[X] = \int_{x \in \mathcal{D}_X} f(x) \, x \, dx
\]</div>
<p>the variance also:</p>
<div class="math notranslate nohighlight">
\[
    \text{Var}(X) = \int_{x \in \mathcal{D}_X} f(x) \, (x - \mathbb{E}[X])^2 \, dx
\]</div>
<p>or a function of the random variable:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[g(X)] = \int_{x \in \mathcal{D}_X} f(x) \, g(x) \, dx
\]</div>
<p>Note that the expectation operator is <strong>linear</strong>:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[a \, X + b \, Y] = a \, \mathbb{E}[X] + b \, \mathbb{E}[Y]
\]</div>
<p>but not the variance, even when the distributions are independent:</p>
<div class="math notranslate nohighlight">
\[
    \text{Var}[a \, X + b \, Y] = a^2 \, \text{Var}[X] + b^2 \, \text{Var}[Y]
\]</div>
</div>
<div class="section" id="standard-distributions">
<h3><span class="section-number">2.3.3. </span>Standard distributions<a class="headerlink" href="#standard-distributions" title="Permalink to this headline">¶</a></h3>
<p>Probability distributions can in principle have any form: <span class="math notranslate nohighlight">\(f(x)\)</span> is unknown. However, specific parameterized distributions can be very useful: their pmf/pdf is fully determined by a couple of parameters.</p>
<ul class="simple">
<li><p>The <strong>Bernouilli</strong> distribution is a binary (discrete, 0 or 1) distribution with a parameter <span class="math notranslate nohighlight">\(p\)</span> specifying the probability to obtain the outcome 1 (e.g. a coin):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    P(X = 1) = p \; \text{and} \; P(X=0) = 1 - p 
\]</div>
<div class="math notranslate nohighlight">
\[P(X=x) = p^x \, (1-p)^{1-x}\]</div>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X] = p\]</div>
<ul class="simple">
<li><p>The <strong>Multinouilli</strong> or <strong>categorical</strong> distribution is a discrete distribution with <span class="math notranslate nohighlight">\(k\)</span> realizations (e.g. a dice). Each realization <span class="math notranslate nohighlight">\(x_i\)</span> is associated with a parameter <span class="math notranslate nohighlight">\(p_i &gt;0\)</span> representing its probability. We have <span class="math notranslate nohighlight">\(\sum_i p_i = 1\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(X = x_i) = p_i\]</div>
<ul class="simple">
<li><p>The <strong>uniform distribution</strong> has an equal and constant probability of returning values between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, never outside this range. It is parameterized by the start of the range <span class="math notranslate nohighlight">\(a\)</span> and the end of the range <span class="math notranslate nohighlight">\(b\)</span>. Its support is <span class="math notranslate nohighlight">\([a, b]\)</span>. The pdf of the uniform distribution <span class="math notranslate nohighlight">\(\mathcal{U}(a, b)\)</span> is defined on <span class="math notranslate nohighlight">\([a, b]\)</span> as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    f(x; a, b) = \frac{1}{b - a}
\]</div>
<ul class="simple">
<li><p>The <strong>normal distribution</strong> is the most frequently encountered continuous distribution. It is parameterized by two parameters: the mean <span class="math notranslate nohighlight">\(\mu\)</span> and the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> (or standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>). Its support is <span class="math notranslate nohighlight">\(\Re\)</span>. The pdf of the normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma)\)</span> is defined on <span class="math notranslate nohighlight">\(\Re\)</span> as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    f(x; \mu, \sigma) = \frac{1}{\sqrt{2\,\pi\,\sigma^2}} \, e^{-\displaystyle\frac{(x - \mu)^2}{2\,\sigma^2}}
\]</div>
<ul class="simple">
<li><p>The <strong>exponential distribution</strong> is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. It is parameterized by one parameter: the rate <span class="math notranslate nohighlight">\(\lambda\)</span>. Its support is <span class="math notranslate nohighlight">\(\Re^+\)</span> (<span class="math notranslate nohighlight">\(x &gt; 0\)</span>).
The pdf of the exponential distribution is defined on <span class="math notranslate nohighlight">\(\Re^+\)</span> as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    f(x; \lambda) = \lambda \, e^{-\lambda \, x}
\]</div>
</div>
<div class="section" id="joint-and-conditional-probabilities">
<h3><span class="section-number">2.3.4. </span>Joint and conditional probabilities<a class="headerlink" href="#joint-and-conditional-probabilities" title="Permalink to this headline">¶</a></h3>
<p>Let’s now suppose that we have two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> with different probability distributions <span class="math notranslate nohighlight">\(P(X)\)</span> and <span class="math notranslate nohighlight">\(P(Y)\)</span>.  The <strong>joint probability</strong> <span class="math notranslate nohighlight">\(P(X, Y)\)</span> denotes the probability of observing the realizations <span class="math notranslate nohighlight">\(x\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(y\)</span> at the same time:</p>
<div class="math notranslate nohighlight">
\[P(X=x, Y=y)\]</div>
<p>If the random variables are <strong>independent</strong>, we have:</p>
<div class="math notranslate nohighlight">
\[P(X=x, Y=y) = P(X=x) \, P(Y=y)\]</div>
<p>If you know the joint probability, you can compute the <strong>marginal probability distribution</strong> of each variable:</p>
<div class="math notranslate nohighlight">
\[P(X=x) = \sum_y P(X=x, Y=y)\]</div>
<p>The same is true for continuous probability distributions:</p>
<div class="math notranslate nohighlight">
\[
    f(x) = \int f(x, y) \, dy
\]</div>
<p>Some useful information between two random variables is the <strong>conditional probability</strong>. <span class="math notranslate nohighlight">\(P(X=x | Y=y)\)</span> is the conditional probability that <span class="math notranslate nohighlight">\(X=x\)</span>, <strong>given</strong> that <span class="math notranslate nohighlight">\(Y=y\)</span> is observed.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y=y\)</span> is not random anymore: it is a <strong>fact</strong> (at least theoretically).</p></li>
<li><p>You wonder what happens to the probability distribution of <span class="math notranslate nohighlight">\(X\)</span> now that you know the value of <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
</ul>
<p>Conditional probabilities are linked to the joint probability by:</p>
<div class="math notranslate nohighlight">
\[
    P(X=x | Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}
\]</div>
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>independent</strong>, we have <span class="math notranslate nohighlight">\(P(X=x | Y=y) = P(X=x)\)</span> (knowing <span class="math notranslate nohighlight">\(Y\)</span> does not change anything to the probability distribution of <span class="math notranslate nohighlight">\(X\)</span>). We can use the same notation for the complete probability distributions:</p>
<div class="math notranslate nohighlight">
\[
    P(X | Y) = \frac{P(X, Y)}{P(Y)}
\]</div>
<p><strong>Example</strong></p>
<p><img alt="" src="../_images/conditionalprobability.png" /></p>
<p><a class="reference external" href="https://www.elevise.co.uk/g-e-m-h-5-u.html">https://www.elevise.co.uk/g-e-m-h-5-u.html</a></p>
<p>You ask 50 people whether they like cats or dogs:</p>
<ul class="simple">
<li><p>18 like both cats and dogs.</p></li>
<li><p>21 like only dogs.</p></li>
<li><p>5 like only cats.</p></li>
<li><p>6 like none of them.</p></li>
</ul>
<p>We consider loving cats and dogs as random variables (and that our sample size is big enough to use probabilities…). Among the 23 who love cats, which proportion also loves dogs?</p>
<p>We have <span class="math notranslate nohighlight">\(P(\text{dog}) = \frac{18+21}{50}\)</span> and <span class="math notranslate nohighlight">\(P(\text{cat}) = \frac{18+5}{50}\)</span>.</p>
<p>The joint probability of loving both cats and dogs is <span class="math notranslate nohighlight">\(P(\text{cat}, \text{dog}) = \frac{18}{50}\)</span>.</p>
<p>The conditional probability of loving dogs given one loves cats is:</p>
<div class="math notranslate nohighlight">
\[P(\text{dog} | \text{cat}) = \frac{P(\text{cat}, \text{dog})}{P(\text{cat})} = \frac{\frac{18}{50}}{\frac{23}{50}} = \frac{18}{23}\]</div>
</div>
<div class="section" id="bayes-rule">
<h3><span class="section-number">2.3.5. </span>Bayes’ rule<a class="headerlink" href="#bayes-rule" title="Permalink to this headline">¶</a></h3>
<p>Noticing that the definition of conditional probabilities is symmetric:</p>
<div class="math notranslate nohighlight">
\[
    P(X, Y) = P(X | Y) \, P(Y) = P(Y | X) \, P(X)
\]</div>
<p>we can obtain the <strong>Bayes’ rule</strong>:</p>
<div class="math notranslate nohighlight">
\[
    P(Y | X) = \frac{P(X|Y) \, P(Y)}{P(X)}
\]</div>
<p>It is very useful when you already know <span class="math notranslate nohighlight">\(P(X|Y)\)</span> and want to obtain <span class="math notranslate nohighlight">\(P(Y|X)\)</span> (<strong>Bayesian inference</strong>).</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(Y | X)\)</span> is called the <strong>posterior probability</strong>.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(X | Y)\)</span> is called the <strong>likelihood</strong>.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(Y)\)</span> is called the <strong>prior probability</strong> (belief).</p></li>
<li><p><span class="math notranslate nohighlight">\(P(X)\)</span> is called the <strong>model evidence</strong> or <strong>marginal likelihood</strong>.</p></li>
</ul>
<p><strong>Example</strong></p>
<p>Let’s consider a disease <span class="math notranslate nohighlight">\(D\)</span> (binary random variable) and a medical test <span class="math notranslate nohighlight">\(T\)</span> (also binary). The disease affects 10% of the general population:</p>
<div class="math notranslate nohighlight">
\[P(D=1)= 0.1 \qquad \qquad P(D=0)=0.9\]</div>
<p>When a patient has the disease, the test is positive 80% of the time (true positives):</p>
<div class="math notranslate nohighlight">
\[P(T=1 | D=1) = 0.8 \qquad \qquad P(T=0 | D=1) = 0.2\]</div>
<p>When a patient does not have the disease, the test is still positive 10% of the time (false positives):</p>
<div class="math notranslate nohighlight">
\[P(T=1 | D=0) = 0.1 \qquad \qquad P(T=0 | D=0) = 0.9\]</div>
<p>Given that the test is positive, what is the probability that the patient is ill?</p>
<p><strong>Answer:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    P(D=1|T=1) &amp;= \frac{P(T=1 | D=1) \, P(D=1)}{P(T=1)} \\
               &amp;\\
               &amp;= \frac{P(T=1 | D=1) \, P(D=1)}{P(T=1 | D=1) \, P(D=1) + P(T=1 | D=0) \, P(D=0)} \\
               &amp;\\
               &amp;= \frac{0.8 \times 0.1}{0.8 \times 0.1 + 0.1 \times 0.9} \\
               &amp;\\
               &amp; = 0.47 \\
\end{aligned}
\end{split}\]</div>
</div>
</div>
<div class="section" id="statistics">
<h2><span class="section-number">2.4. </span>Statistics<a class="headerlink" href="#statistics" title="Permalink to this headline">¶</a></h2>
<div class="section" id="monte-carlo-sampling">
<h3><span class="section-number">2.4.1. </span>Monte Carlo sampling<a class="headerlink" href="#monte-carlo-sampling" title="Permalink to this headline">¶</a></h3>
<p><strong>Random sampling</strong> or <strong>Monte Carlo sampling</strong> consists of taking <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\(x_i\)</span> out of the distribution <span class="math notranslate nohighlight">\(X\)</span> (discrete or continuous) and computing the <strong>sample average</strong>:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[X] = \mathbb{E}_{x \sim X} [x] \approx \frac{1}{N} \, \sum_{i=1}^N x_i
\]</div>
<p><img alt="" src="../_images/normaldistribution.svg" /></p>
<p>More samples will be obtained where <span class="math notranslate nohighlight">\(f(x)\)</span> is high (<span class="math notranslate nohighlight">\(x\)</span> is probable), so the average of the sampled data will be close to the expected value of the distribution.</p>
<p><strong>Law of big numbers</strong></p>
<blockquote>
<div><p>As the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.</p>
</div></blockquote>
<p>MC estimates are only correct when:</p>
<ul class="simple">
<li><p>the samples are <strong>i.i.d</strong> (independent and identically distributed):</p>
<ul>
<li><p>independent: the samples must be unrelated with each other.</p></li>
<li><p>identically distributed: the samples must come from the same distribution <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
</ul>
</li>
<li><p>the number of samples is large enough. Usually <span class="math notranslate nohighlight">\(N &gt; 30\)</span> for simple distributions.</p></li>
</ul>
<p>One can estimate any function of the random variable with random sampling:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[f(X)] = \mathbb{E}_{x \sim X} [f(x)] \approx \frac{1}{N} \, \sum_{i=1}^N f(x_i)
\]</div>
<p>Example of Monte Carlo sampling to estimate <span class="math notranslate nohighlight">\(\pi/4\)</span>:</p>
<p><img alt="" src="../_images/montecarlo-1.jpeg" /></p>
<p><img alt="" src="../_images/montecarlo-2.jpeg" /></p>
<p><img alt="" src="../_images/montecarlo-3.jpeg" /></p>
<p><img alt="" src="../_images/montecarlo-4.jpeg" /></p>
<p>Credit <a class="reference external" href="https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694">https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694</a></p>
</div>
<div class="section" id="central-limit-theorem">
<h3><span class="section-number">2.4.2. </span>Central limit theorem<a class="headerlink" href="#central-limit-theorem" title="Permalink to this headline">¶</a></h3>
<p>Suppose we have an unknown distribution <span class="math notranslate nohighlight">\(X\)</span> with expected value <span class="math notranslate nohighlight">\(\mu = \mathbb{E}[X]\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. We can take randomly <span class="math notranslate nohighlight">\(N\)</span> samples from <span class="math notranslate nohighlight">\(X\)</span> to compute the sample average:</p>
<div class="math notranslate nohighlight">
\[
    S_N = \frac{1}{N} \, \sum_{i=1}^N x_i
\]</div>
<p>The <strong>Central Limit Theorem</strong> (CLT) states that:</p>
<blockquote>
<div><p>The distribution of sample averages is normally distributed with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\frac{\sigma^2}{N}\)</span>.</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[S_N \sim \mathcal{N}(\mu, \frac{\sigma}{\sqrt{N}})\]</div>
<p>If we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value. The more samples we get, the smaller the variance of the estimates. Although the distribution <span class="math notranslate nohighlight">\(X\)</span> can be anything, the sampling averages are normally distributed.</p>
<p><img alt="" src="../_images/IllustrationCentralTheorem.png" /></p>
<p>Credit: <a class="reference external" href="https://en.wikipedia.org/wiki/Central_limit_theorem">https://en.wikipedia.org/wiki/Central_limit_theorem</a></p>
</div>
<div class="section" id="estimators">
<h3><span class="section-number">2.4.3. </span>Estimators<a class="headerlink" href="#estimators" title="Permalink to this headline">¶</a></h3>
<p>CLT shows that the sampling average is an <strong>unbiased estimator</strong> of the expected value of a distribution:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(S_N) = \mathbb{E}(X)\]</div>
<p>An estimator is a random variable used to measure parameters of a distribution (e.g. its expectation). The problem is that estimators can generally be <strong>biased</strong>.</p>
<p>Take the example of a thermometer <span class="math notranslate nohighlight">\(M\)</span> measuring the temperature <span class="math notranslate nohighlight">\(T\)</span>. <span class="math notranslate nohighlight">\(T\)</span> is a random variable (normally distributed with <span class="math notranslate nohighlight">\(\mu=20\)</span> and <span class="math notranslate nohighlight">\(\sigma=10\)</span>) and the measurements <span class="math notranslate nohighlight">\(M\)</span> relate to the temperature with the relation:</p>
<div class="math notranslate nohighlight">
\[
    M = 0.95 \, T + 0.65
\]</div>
<p><img alt="" src="../_images/estimators-temperature.png" /></p>
<p>The thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature?</p>
<p>We could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:</p>
<p><img alt="" src="../_images/estimators-temperature2.png" /></p>
<p>But, as the expectation is linear, we actually have:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}[M] = \mathbb{E}[0.95 \, T + 0.65] = 0.95 \, \mathbb{E}[T] + 0.65 = 19.65 \neq \mathbb{E}[T]
\]</div>
<p>The thermometer is a <strong>biased estimator</strong> of the temperature.</p>
<p>Let’s note <span class="math notranslate nohighlight">\(\theta\)</span> a parameter of a probability distribution <span class="math notranslate nohighlight">\(X\)</span> that we want to estimate (it does not have to be its mean). An <strong>estimator</strong> <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is a random variable mapping the sample space of <span class="math notranslate nohighlight">\(X\)</span> to a set of sample estimates.</p>
<ul class="simple">
<li><p>The <strong>bias</strong> of an estimator is the mean error made by the estimator:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathcal{B}(\hat{\theta}) = \mathbb{E}[\hat{\theta} - \theta] = \mathbb{E}[\hat{\theta}] - \theta
\]</div>
<ul class="simple">
<li><p>The <strong>variance</strong> of an estimator is the deviation of the samples around the expected value:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \text{Var}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}] )^2]
\]</div>
<p>Ideally, we would like estimators with:</p>
<ul class="simple">
<li><p><strong>low bias</strong>: the estimations are correct on average (= equal to the true parameter).</p></li>
<li><p><strong>low variance</strong>: we do not need many estimates to get a correct estimate (CLT: <span class="math notranslate nohighlight">\(\frac{\sigma}{\sqrt{N}}\)</span>)</p></li>
</ul>
<p><img alt="" src="../_images/biasvariance.png" /></p>
<p>Unfortunately, the perfect estimator does not exist in practice.  One usually talks of a <strong>bias/variance</strong> trade-off: if you have a small bias, you will have a high variance, or vice versa. In machine learning, bias corresponds to underfitting, variance to overfitting.</p>
</div>
</div>
<div class="section" id="information-theory">
<h2><span class="section-number">2.5. </span>Information theory<a class="headerlink" href="#information-theory" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a">https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a</a></p>
<div class="section" id="entropy">
<h3><span class="section-number">2.5.1. </span>Entropy<a class="headerlink" href="#entropy" title="Permalink to this headline">¶</a></h3>
<p><strong>Information theory</strong> (a field founded by Claude Shannon) asks how much information is contained in a probability distribution. Information is related to <strong>surprise</strong> or <strong>uncertainty</strong>: are the outcomes of a random variable surprising?</p>
<ul class="simple">
<li><p>Almost certain outcomes (<span class="math notranslate nohighlight">\(P \sim 1\)</span>) are not surprising because they happen all the time.</p></li>
<li><p>Almost impossible outcomes (<span class="math notranslate nohighlight">\(P \sim 0\)</span>) are very surprising because they are very rare.</p></li>
</ul>
<p><img alt="" src="../_images/selfinformation.png" /></p>
<p>A useful measurement of how surprising is an outcome <span class="math notranslate nohighlight">\(x\)</span> is the <strong>self-information</strong>:</p>
<div class="math notranslate nohighlight">
\[
    I (x) = - \log P(X = x)
\]</div>
<p>Depending on which log is used, self-information has different units, but it is just a rescaling, the base never matters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\log_2\)</span>: bits or shannons.</p></li>
<li><p><span class="math notranslate nohighlight">\(\log_e = \ln\)</span>: nats.</p></li>
</ul>
<p>The <strong>entropy</strong> (or Shannon entropy) of a probability distribution is the expected value of the self-information of its outcomes:</p>
<div class="math notranslate nohighlight">
\[
    H(X) = \mathbb{E}_{x \sim X} [I(x)] = \mathbb{E}_{x \sim X} [- \log P(X = x)] 
\]</div>
<p>It measures the <strong>uncertainty</strong>, <strong>randomness</strong> or <strong>information content</strong> of the random variable.</p>
<p>In the discrete case:</p>
<div class="math notranslate nohighlight">
\[
    H(X) = - \sum_x P(x) \, \log P(x)
\]</div>
<p>In the continuous case:</p>
<div class="math notranslate nohighlight">
\[
    H(X) = - \int_x f(x) \, \log f(x) \, dx
\]</div>
<p>The entropy of a Bernouilli variable is maximal when both outcomes are <strong>equiprobable</strong>. If a variable is <strong>deterministic</strong>, its entropy is minimal and equal to zero.</p>
<p><img alt="" src="../_images/entropy-binomial.png" /></p>
<p>The <strong>joint entropy</strong> of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is defined by:</p>
<div class="math notranslate nohighlight">
\[
    H(X, Y) = \mathbb{E}_{x \sim X, y \sim Y} [- \log P(X=x, Y=y)] 
\]</div>
<p>The <strong>conditional entropy</strong> of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is defined by:</p>
<div class="math notranslate nohighlight">
\[
    H(X | Y) = \mathbb{E}_{x \sim X, y \sim Y} [- \log P(X=x | Y=y)]  = \mathbb{E}_{x \sim X, y \sim Y} [- \log \frac{P(X=x , Y=y)}{P(Y=y)}] 
\]</div>
<p>If the variables are <strong>independent</strong>, we have:</p>
<div class="math notranslate nohighlight">
\[
    H(X, Y) = H(X) + H(Y)
\]</div>
<div class="math notranslate nohighlight">
\[
    H(X | Y) = H(X)
\]</div>
<p>Both are related by:</p>
<div class="math notranslate nohighlight">
\[
    H(X | Y) = H(X, Y) - H(Y)
\]</div>
<p>The equivalent of Bayes’ rule is:</p>
<div class="math notranslate nohighlight">
\[
    H(Y |X) = H(X |Y) + H(Y) - H(X)
\]</div>
</div>
<div class="section" id="mutual-information-cross-entropy-and-kullback-leibler-divergence">
<h3><span class="section-number">2.5.2. </span>Mutual Information, cross-entropy and Kullback-Leibler divergence<a class="headerlink" href="#mutual-information-cross-entropy-and-kullback-leibler-divergence" title="Permalink to this headline">¶</a></h3>
<p>The most important information measurement between two variables is the <strong>mutual information</strong> MI (or information gain):</p>
<div class="math notranslate nohighlight">
\[
    I(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)
\]</div>
<p>It measures how much information the variable <span class="math notranslate nohighlight">\(X\)</span> holds on <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<ul class="simple">
<li><p>If the two variables are <strong>independent</strong>, the MI is 0 : <span class="math notranslate nohighlight">\(X\)</span> is as random, whether you know <span class="math notranslate nohighlight">\(Y\)</span> or not.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
        I (X, Y) = 0
\]</div>
<ul class="simple">
<li><p>If the two variables are <strong>dependent</strong>, knowing <span class="math notranslate nohighlight">\(Y\)</span> gives you information on <span class="math notranslate nohighlight">\(X\)</span>, which becomes less random, i.e. less uncertain / surprising.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
        I (X, Y) &gt; 0
\]</div>
<p>If you can fully predict <span class="math notranslate nohighlight">\(X\)</span> when you know <span class="math notranslate nohighlight">\(Y\)</span>, it becomes deterministic (<span class="math notranslate nohighlight">\(H(X|Y)=0\)</span>) so the mutual information is maximal (<span class="math notranslate nohighlight">\(I(X, Y) = H(X)\)</span>).</p>
<p>The <strong>cross-entropy</strong> between two distributions <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
    H(X, Y) = \mathbb{E}_{x \sim X}[- \log P(Y=x)]
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Beware that the notation <span class="math notranslate nohighlight">\(H(X, Y)\)</span> is the same as the joint entropy, but it is a different concept!</p>
</div>
<p>The cross-entropy measures the <strong>negative log-likelihood</strong> that a sample <span class="math notranslate nohighlight">\(x\)</span> taken from the distribution <span class="math notranslate nohighlight">\(X\)</span> could also come from the distribution <span class="math notranslate nohighlight">\(Y\)</span>. More exactly, it measures how many bits of information one would need to distinguish the two distributions <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p><img alt="" src="../_images/crossentropy.svg" /></p>
<div class="math notranslate nohighlight">
\[
    H(X, Y) = \mathbb{E}_{x \sim X}[- \log P(Y=x)]
\]</div>
<p>If the two distributions are the same <em>almost anywhere</em>, one cannot distinguish samples from the two distributions:</p>
<ul class="simple">
<li><p>The cross-entropy is the same as the entropy of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
</ul>
<p>If the two distributions are completely different, one can tell whether a sample <span class="math notranslate nohighlight">\(Z\)</span> comes from <span class="math notranslate nohighlight">\(X\)</span> or <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<ul class="simple">
<li><p>The cross-entropy is higher than the entropy of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
</ul>
<p>In practice, the <strong>Kullback-Leibler divergence</strong> <span class="math notranslate nohighlight">\(\text{KL}(X ||Y)\)</span> is a better measurement of the similarity (statistical distance) between two probability distributions:</p>
<div class="math notranslate nohighlight">
\[
    \text{KL}(X ||Y) = \mathbb{E}_{x \sim X}[- \log \frac{P(Y=x)}{P(X=x)}]
\]</div>
<p>It is linked to the cross-entropy by:</p>
<div class="math notranslate nohighlight">
\[
    \text{KL}(X ||Y) = H(X, Y) - H(X)
\]</div>
<p>If the two distributions are the same <em>almost anywhere</em>:</p>
<ul class="simple">
<li><p>The KL divergence is zero.</p></li>
</ul>
<p>If the two distributions are different:</p>
<ul class="simple">
<li><p>The KL divergence is positive.</p></li>
</ul>
<p>Minimizing the KL between two distributions is the same as making the two distributions “equal”.
The KL is not a metric, as it is not symmetric.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./1-intro"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="1-Introduction.html" title="previous page"><span class="section-number">1. </span>Introduction</a>
    <a class='right-next' id="next-link" href="3-Neurons.html" title="next page"><span class="section-number">3. </span>Neurons</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>