
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Introduction &#8212; Neurocomputing</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/1-intro/1-Introduction.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Math basics (optional)" href="2-Math.html" />
    <link rel="prev" title="Neurocomputing" href="../intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-neurocomputing/1-intro/1-Introduction.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Introduction" />
<meta property="og:description" content="Introduction  Slides: pdf  What is neurocomputing?  &lt;div class=&#39;embed-container&#39;&gt;&lt;iframe src=&#39;https://www.youtube.com/embed/Hy0FpFjdJGI&#39; frameborder=&#39;0&#39; allowfu" />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-neurocomputing/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neurocomputing</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/1-intro/1-Introduction.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-neurocomputing">
   1.1. What is neurocomputing?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications-of-deep-learning">
   1.2. Applications of deep learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#supervised-learning">
     1.2.1. Supervised learning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#feedforward-neural-networks">
       1.2.1.1. Feedforward neural networks
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#recurrent-neural-networks">
       1.2.1.2. Recurrent neural networks
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-learning">
     1.2.2. Unsupervised learning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#clustering-and-feature-extraction">
       1.2.2.1. Clustering and feature extraction
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dimensionality-reduction-and-autoencoders">
       1.2.2.2. Dimensionality reduction and autoencoders
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#generative-models">
       1.2.2.3. Generative models
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reinforcement-learning">
     1.2.3. Reinforcement learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#outlook">
   1.3. Outlook
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="introduction">
<h1><span class="section-number">1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>Slides: <a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing/lectures/pdf/1.1-Introduction.pdf">pdf</a></p>
<div class="section" id="what-is-neurocomputing">
<h2><span class="section-number">1.1. </span>What is neurocomputing?<a class="headerlink" href="#what-is-neurocomputing" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/Hy0FpFjdJGI' frameborder='0' allowfullscreen></iframe></div>
<p>Let’s first discuss the difference between Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL) and Neurocomputing. Nowadays, these terms are used almost interchangeably, but there are historical and methodological differences.</p>
<div class="figure align-default" id="id12">
<a class="reference internal image-reference" href="../_images/aimldl.png"><img alt="../_images/aimldl.png" src="../_images/aimldl.png" style="width: 60%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.1 </span><span class="caption-text">Source: <a class="reference external" href="https://data-science-blog.com/blog/2018/05/14/machine-learning-vs-deep-learning-wo-liegt-der-unterschied">https://data-science-blog.com/blog/2018/05/14/machine-learning-vs-deep-learning-wo-liegt-der-unterschied</a></span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p>The term <strong>Artificial Intelligence</strong> was coined by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence in <strong>1956</strong>:</p>
<blockquote>
<div><p>The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.</p>
</div></blockquote>
<p>Good old-fashion AI (GOFAI) approaches were purely symbolic (logical systems, knowledge-based systems) or using linear neural networks. They were able to play checkers, prove mathematical theorems, make simple conversations (ELIZA), translate languages…</p>
<p><strong>Machine learning</strong> (ML) is a branch of AI that focuses on <strong>learning from examples</strong> (data-driven AI). It is sometimes also referred to as big data, data science, operational research, pattern recognition… ML algorithms include:</p>
<ul class="simple">
<li><p>Artificial Neural Networks (multi-layer perceptrons)</p></li>
<li><p>Statistical analysis (Bayesian modeling, PCA)</p></li>
<li><p>Clustering algorithms (k-means, GMM, spectral clustering)</p></li>
<li><p>Support vector machines</p></li>
<li><p>Decision trees, random forests</p></li>
</ul>
<p><strong>Deep Learning</strong> is a recent re-branding of artificial neural networks. It focuses on learning high-level representations of the data, using highly non-linear neural networks. Many architectures have been developped, including:</p>
<ul class="simple">
<li><p>Deep neural networks (DNN)</p></li>
<li><p>Convolutional neural networks (CNN)</p></li>
<li><p>Recurrent neural networks (RNN)</p></li>
<li><p>Generative models (GAN, VAE)</p></li>
<li><p>Deep reinforcement learning (DQN, A3C, PPO)</p></li>
</ul>
<p><img alt="" src="../_images/neurocomputing.svg" /></p>
<p><strong>Neurocomputing</strong> is at the intersection between computational neuroscience and artificial neural networks (deep learning). <strong>Computational neuroscience</strong> studies the functioning of the brain (human or animal) through biologically detailed models, either at the functional level (e.g. visual attention, decision-making) or cellular level (individual neurons, synapses, neurotransmitters, etc). The goal of computational neuroscience is 1) to provide theoretical explanations to the experimental observations made by neuroscientists and 2) make predictions that can be verified experimentally. Moreover, understanding how the brain solves real-life problems might allow to design better AI algorithms. If you are interested in computational neuroscience, make sure to visit the courses <strong>Neurokognition</strong> I and II taught by Prof. Dr. Hamker:</p>
<p><a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/neurokognition/">https://www.tu-chemnitz.de/informatik/KI/edu/neurokognition/</a></p>
<p>Neurocomputing aims at bringing the mechanisms underlying human cognition into artificial intelligence. The first part of this course focuses on deep learning, while the second will discuss how more biologically realistic neural networks could help designing better AI systems.</p>
</div>
<div class="section" id="applications-of-deep-learning">
<h2><span class="section-number">1.2. </span>Applications of deep learning<a class="headerlink" href="#applications-of-deep-learning" title="Permalink to this headline">¶</a></h2>
<p>Machine Learning applications are generally divided into three main branches:</p>
<ul class="simple">
<li><p><strong>Supervised learning</strong>: The program is trained on a pre-defined set of training examples and used to make correct predictions when given new data.</p></li>
<li><p><strong>Unsupervised learning</strong>: The program is given a bunch of data and must find patterns and relationships therein.</p></li>
<li><p><strong>Reinforcement learning</strong>: The program explores its environment by producing actions and receiving rewards.</p></li>
</ul>
<div class="figure align-default" id="id13">
<a class="reference internal image-reference" href="../_images/ml-areas.png"><img alt="../_images/ml-areas.png" src="../_images/ml-areas.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.2 </span><span class="caption-text">Source: <a class="reference external" href="http://www.isaziconsulting.co.za/machinelearning.html">http://www.isaziconsulting.co.za/machinelearning.html</a></span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<p>Deep learning has recently revolutionized these types of machine learning, so let’s have a look at some concrete examples for motivation. At the end of the course, if you also perform all exercises, you should be able to reproduce these applications.</p>
<div class="section" id="supervised-learning">
<h3><span class="section-number">1.2.1. </span>Supervised learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h3>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/pbbzwFohH3I' frameborder='0' allowfullscreen></iframe></div>
<div class="figure align-default" id="id14">
<a class="reference internal image-reference" href="../_images/supervisedlearning.png"><img alt="../_images/supervisedlearning.png" src="../_images/supervisedlearning.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.3 </span><span class="caption-text">Principle of supervised learning. Source: Andrew Ng, Stanford CS229, <a class="reference external" href="https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf">https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf</a></span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p>In a supervised learning, we have a <strong>training set</strong> (or training data) consisting of <span class="math notranslate nohighlight">\(N\)</span> samples (or examples) from which we want to learn the underlying function or distribution. Each sample consists of an <strong>input</strong> <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> and an <strong>output</strong> (also called ground truth, desired output or target) <span class="math notranslate nohighlight">\(t_i\)</span>.</p>
<p>What we want to learn is <strong>parameterized model</strong> <span class="math notranslate nohighlight">\(y_i = f_\theta (\mathbf{x}_i)\)</span> which can predict the correct output for the inputs of the training set. The goal of <strong>learning</strong> (or training) is to find which value of the parameters <span class="math notranslate nohighlight">\(\theta\)</span> allows to reduce (<em>minimize</em>) the <strong>prediction error</strong>. i.e. the discrepancy between the prediction <span class="math notranslate nohighlight">\(y_i = f_\theta (\mathbf{x}_i)\)</span> and the desired output <span class="math notranslate nohighlight">\(t_i\)</span>.</p>
<p>Depending on the nature of the outputs <span class="math notranslate nohighlight">\(t\)</span>, we have two different supervised problems:</p>
<ul class="simple">
<li><p>In <strong>regression</strong> tasks, the outputs can take an infinity of values (e.g. real numbers). The following figure shows how examples of flat surfaces (input <span class="math notranslate nohighlight">\(x_i\)</span>)  and prices (output <span class="math notranslate nohighlight">\(t_i\)</span>) collected in the neighborhood can be used to predict the price of a new flat. After collecting enough samples, a model is trained to minimize its prediction error. Here, a linear model is used (black line) as we perform <strong>linear regression</strong>, but any other type of function could be used. The parameters of the line (slope and intercept) are adapted so that the line lies close to the data: the predicted price <span class="math notranslate nohighlight">\(y_i\)</span> is never far from the ground truth <span class="math notranslate nohighlight">\(t_i\)</span>. Using that line after learning, we can predict that a 60 square meters flat should be rented around 550 euros/month.</p></li>
</ul>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/regression-animation3.png"><img alt="../_images/regression-animation3.png" src="../_images/regression-animation3.png" style="width: 80%;" /></a>
</div>
<ul class="simple">
<li><p>In <strong>classification</strong> tasks, the outputs are discrete, i.e. take only a finite number of different values (called classes or labels). When there are only two classes, they are called the positive and negative classes and the problem is a <strong>binary classification</strong>. The two classes can represent yes/no binary values, such as when when a test is positive or negative. When there are more than two classes, they can for example represent different objects (car / bike / dog / cat…) that can be recognized on an image. The following figure depicts a binary classifiation problem, where two input features <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> (temperature and blood pressure) are used to predict the occurence of an illness (yes = ill, no = sane). The linear model is a line that separates the input space into two separate regions: all points above the line are categorized (classified) as ill, all points below as sane, even if they were not in the training data.</p></li>
</ul>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/classification-animation3.png"><img alt="../_images/classification-animation3.png" src="../_images/classification-animation3.png" style="width: 80%;" /></a>
</div>
<p>In practice, when using neural networks, the distinction between classification and regression is not very important, but it can be relevant for other ML techniques (decision trees only work for classification problems, for example).</p>
<div class="section" id="feedforward-neural-networks">
<h4><span class="section-number">1.2.1.1. </span>Feedforward neural networks<a class="headerlink" href="#feedforward-neural-networks" title="Permalink to this headline">¶</a></h4>
<p>As we will see later, an <strong>artificial neuron</strong> is a mathematical model able to perform <strong>linear</strong> classification or regression using weighted sums of inputs:</p>
<div class="math notranslate nohighlight">
\[y = f(\sum_{i=1}^d w_i \, x_i + b)\]</div>
<div class="figure align-default" id="id15">
<a class="reference internal image-reference" href="../_images/artificialneuron.svg"><img alt="../_images/artificialneuron.svg" src="../_images/artificialneuron.svg" width="60%" /></a>
<p class="caption"><span class="caption-number">Fig. 1.4 </span><span class="caption-text">Artificial neuron.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p>By stacking layers of artificial neurons, we obtain a <strong>feedforward neural network</strong> able to solve non-linear classification and regression problems.</p>
<div class="figure align-default" id="id16">
<a class="reference internal image-reference" href="../_images/deep.svg"><img alt="../_images/deep.svg" src="../_images/deep.svg" width="60%" /></a>
<p class="caption"><span class="caption-number">Fig. 1.5 </span><span class="caption-text">Feedforward neural network.</span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
<p><em>Fully-connected layers</em> of neurons can be replaced by <em>convolutional layers</em> when dealing with images as inputs, leading to the very successful <strong>convolutional neural networks</strong> (CNN).</p>
<div class="figure align-default" id="id17">
<a class="reference internal image-reference" href="../_images/dcn.png"><img alt="../_images/dcn.png" src="../_images/dcn.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.6 </span><span class="caption-text">Typical CNN architecture. Source: Albelwi S, Mahmood A. 2017. A Framework for Designing the Architectures of Deep Convolutional Neural Networks. Entropy 19:242. doi:10.3390/e19060242</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<p>The “only” thing to do is to feed these networks with a lot of training data (inputs and desired outputs) and let them adjust their weights to minimize their prediction error using the backpropagation algorithm <a class="bibtex reference internal" href="../zreferences.html#rumelhart1986a" id="id1">[Rumelhart et al., 1986]</a> (more on that later). Neural networks (including CNNs) are a very old technology, dating back from the 60’s, with a resurgence in the 80’s thanks to the backpropation algorithm. They had been able to learn small datasets, but their performance was limited by the availability of data and the computing power available at the time. One classical example is the use of a CNN <a class="bibtex reference internal" href="../zreferences.html#lecun1998" id="id2">[LeCun et al., 1998]</a> by Yann LeCun in 1998 to automatically classify single digits on ZIP postal codes (what led to the development of the MNIST dataset, the “Hello World!” of machine learning which we will use in the exercises).</p>
<div class="figure align-default" id="id18">
<a class="reference internal image-reference" href="../_images/lenet5.gif"><img alt="../_images/lenet5.gif" src="../_images/lenet5.gif" style="width: 50%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.7 </span><span class="caption-text">LeNet5 CNN trained on MNIST. Source: <a class="reference external" href="http://yann.lecun.com/exdb/lenet/">http://yann.lecun.com/exdb/lenet/</a></span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
<p>The revival of artificial neural networks marketed as <strong>deep learning</strong> at the end of the 2000’s was principally due the availability of massive amounts of training data (thanks to search engines and social networks) and the availability of consumer graphics GPUs able to perform scientific computations, especially using Nvidia’s CUDA programming framework.</p>
<p>The first badge of honour obtained by deep learning methods happened during the <a class="reference external" href="https://image-net.org">ImageNet</a> challenge in 2012. The challenge was made for computer vision (CV) scientists to compare their algorithms on a huge dataset of 14 billion annotated images for object recognition (what is on the image?), object detection (which objects are in the image and where?) and object segmentation (which pixels belong to which object?). The object recognition challenge was indeed quite hard, with 1000 different classes (sometimes exotic, such as “ladle” or “porcupine”) with a great variety of backgrounds or lightning conditions. Classical CV methods based on feature extraction and simple classifiers performed reasonably well, with an error rate around 30%.</p>
<p>However, Krizhevsky, Sutskever and Hinton <a class="bibtex reference internal" href="../zreferences.html#krizhevsky2012" id="id3">[Krizhevsky et al., 2012]</a> trained a CNN entirely on the images, without any form of preprocessing, and obtained an error rate of 15%, half of the other methods. This achievement marked the beginning of the deep learning era, attracted the attention of the major industrial players (Google, Facebook and soon the rest of the world) who have already invested hundreds of billions on AI research.</p>
<p>The whole field of computer vision was taken by storm, and CNNs were able to outperform the state-of-the-art of many vision-related tasks, such as object detection with the YOLO (You Only Look Once) network <a class="bibtex reference internal" href="../zreferences.html#redmon2016" id="id4">[Redmon et al., 2016]</a>:</p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/MPU2HistivI' frameborder='0' allowfullscreen></iframe></div>
<p>or semantic segmention with SegNet <a class="bibtex reference internal" href="../zreferences.html#badrinarayanan2016" id="id5">[Badrinarayanan et al., 2016]</a> or its variants such as Mask RCNN <a class="bibtex reference internal" href="../zreferences.html#he2018" id="id6">[He et al., 2018]</a>:</p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/OOT3UIXZztE' frameborder='0' allowfullscreen></iframe></div>
<p>CNNs can even be used to control autonomous cars, by learning to reproduce human commands for a given input image <a class="bibtex reference internal" href="../zreferences.html#bojarski2016" id="id7">[Bojarski et al., 2016]</a>:</p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/qhUvQiKec2U' frameborder='0' allowfullscreen></iframe></div>
<p>CNNs are also gaining an increasing importance in medical applications, for example to help histologists detect cancerous cells:</p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/9Mz84cwVmS0' frameborder='0' allowfullscreen></iframe></div>
</div>
<div class="section" id="recurrent-neural-networks">
<h4><span class="section-number">1.2.1.2. </span>Recurrent neural networks<a class="headerlink" href="#recurrent-neural-networks" title="Permalink to this headline">¶</a></h4>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/_809brCJaTM' frameborder='0' allowfullscreen></iframe></div>
<p>Another field that was heavily transformed by deep learning is <strong>natural language processing</strong> (NLP), i.e. the automatic processing of language, be it text understanding, translation, summarization, question answering or even speech recognition and synthesis. In short, everything needed under the hood when you talk to Siri or Alexa.</p>
<p>The key neural network involved in this paradigmatic change is the <strong>recurrent neural network</strong> (RNN), with the most prominent model being the <strong>long short-term memory</strong> (LSTM) network <a class="bibtex reference internal" href="../zreferences.html#hochreiter1997" id="id8">[Hochreiter &amp; Schmidhuber, 1997]</a>.</p>
<div class="figure align-default" id="id19">
<a class="reference internal image-reference" href="../_images/LSTM3-chain.png"><img alt="../_images/LSTM3-chain.png" src="../_images/LSTM3-chain.png" style="width: 90%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.8 </span><span class="caption-text">LSTM cell. Source: <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<p>The main difference with feedforward neural networks is that RNNs can be applied on sequences (of words, but it could also be video frames or any time-dependent signal). At each step, a RNN produces an output not only depending on its current input, but also on its previous output, implementing a form of memory of past events.</p>
<p>More recent advances furthermore introduce the concept of <strong>attention</strong> for processing sequences. This is now at the heart of all translation systems or in BERT, the language understanding module behind Google search. The neural architectures may seem complex, but we will break them down in this course.</p>
<div class="figure align-default" id="id20">
<a class="reference internal image-reference" href="../_images/google-nmt-lstm.png"><img alt="../_images/google-nmt-lstm.png" src="../_images/google-nmt-lstm.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.9 </span><span class="caption-text">Google Neural Machine Translation. Source: <a class="reference external" href="https://ai.google/research/pubs/pub45610">https://ai.google/research/pubs/pub45610</a></span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="unsupervised-learning">
<h3><span class="section-number">1.2.2. </span>Unsupervised learning<a class="headerlink" href="#unsupervised-learning" title="Permalink to this headline">¶</a></h3>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/uIrBgz4OPlM' frameborder='0' allowfullscreen></iframe></div>
<p>In supervised learning, we use <strong>annotated data</strong>, i.e. pairs <span class="math notranslate nohighlight">\((x_i, t_i)\)</span> of input/output examples. This requires to know the ground truth for each sample, what can be be very tedious and expensive if humans have to do it.</p>
<p>In <strong>unsupervised learning</strong>, we only have inputs. The goal of the algorithms is to make sense out of the data: extract regularities, model the underlying distribution, group examples into clusters, etc… It may seem much harder than supervised learning, as there is no ground truth to measure performance, but data is very cheap to obtain.</p>
<div class="section" id="clustering-and-feature-extraction">
<h4><span class="section-number">1.2.2.1. </span>Clustering and feature extraction<a class="headerlink" href="#clustering-and-feature-extraction" title="Permalink to this headline">¶</a></h4>
<p><strong>Clustering</strong> is a classical machine technique allowing to group examples in clusters based on their respective distances: close examples should belong to the same cluster. The most well-know algorithms are k-means and Gaussian mixture models (GMM). But the quality of the clustering depends on the space in which the inputs are represented: two images may be similar not because their pixels are similar (e.g. two dark images), but because they contain similar objects (fishes, birds). Neural networks can be used to learn a <strong>feature space</strong> where distances between inputs are meaningful.</p>
<div class="figure align-default" id="id21">
<a class="reference internal image-reference" href="../_images/unsupervised-learning.png"><img alt="../_images/unsupervised-learning.png" src="../_images/unsupervised-learning.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.10 </span><span class="caption-text">Clustering. Source: <a class="reference external" href="https://learn.g2.com/supervised-vs-unsupervised-learning">https://learn.g2.com/supervised-vs-unsupervised-learning</a></span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="dimensionality-reduction-and-autoencoders">
<h4><span class="section-number">1.2.2.2. </span>Dimensionality reduction and autoencoders<a class="headerlink" href="#dimensionality-reduction-and-autoencoders" title="Permalink to this headline">¶</a></h4>
<p>Data such as images have a lot of dimensions (one per pixel), most of which are redundant. <strong>Dimensionality reduction</strong> techniques allow to reduce this number of dimensions by projecting the data into a <strong>latent space</strong> while keeping the information.</p>
<p><strong>Autoencoders</strong> (AE) are neural networks that learn to reproduce their inputs (unsupervised learning, as there are no labels) by compressing information through a bottleneck. The <strong>encoder</strong> projects the input data onto the latent space, while the <strong>decoder</strong> recreates the input. The latent space has much less dimensions than the input images, but must contain enough information in order to reconstruct the image.</p>
<div class="figure align-default" id="id22">
<a class="reference internal image-reference" href="../_images/latent-space.png"><img alt="../_images/latent-space.png" src="../_images/latent-space.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.11 </span><span class="caption-text">Autoencoders. Source: <a class="reference external" href="https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694g">https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694g</a></span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</div>
<p>Apart from compression, one important application of dimensionality reduction is <strong>visualization</strong> when the latent space has 2 or 3 dimensions: you can visualize the distribution of your data and estimate how hard the classification/regression will be. Classical ML techniques include PCA (principal component analysis) and t-SNE, but autoencoders can also be used, for example the <strong>UMAP</strong> (Uniform Manifold Approximation and Projection for Dimension Reduction) architecture <a class="bibtex reference internal" href="../zreferences.html#mcinnes2020" id="id9">[McInnes et al., 2020]</a>.</p>
<p>Another application of autoencoders is the <strong>pretraining</strong> (feature extraction) of neural networks on unsupervised data before <strong>fine-tuning</strong> the resulting classifier on supervised data. This allows <strong>self-taught learning</strong> or <strong>semi-supervised learning</strong>, when the annotated data available for supervised learning is scarce, but a lot of unsupervised data from the same domain is available.</p>
</div>
<div class="section" id="generative-models">
<h4><span class="section-number">1.2.2.3. </span>Generative models<a class="headerlink" href="#generative-models" title="Permalink to this headline">¶</a></h4>
<p>The other major advantage of autoencoders is their <strong>decoder</strong>: from a low-dimensional latent representation, it is able after training to generate high-dimensional data such as images. By <strong>sampling</strong> the latent space, one could in principle generate an infinity of new images.</p>
<p>One particular form of autoencoder which is very useful for data generation is the <strong>variational autoencoder</strong> (VAE) <a class="bibtex reference internal" href="../zreferences.html#kingma2013" id="id10">[Kingma &amp; Welling, 2013]</a>. The main difference with a regular AE is that the latent encodes a <strong>probability distribution</strong> instead of a single latent vector, what allows to sample new but realistic outputs. For example, a VAE trained to reproduce faces can generate new hybrid faces depending on how the sampling is done:</p>
<div class="figure align-default" id="id23">
<a class="reference internal image-reference" href="../_images/vae-faces.jpg"><img alt="../_images/vae-faces.jpg" src="../_images/vae-faces.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.12 </span><span class="caption-text">Sampling the latent space of a VAE trained on faces allows to generate new but realistic faces. Source: <a class="reference external" href="https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df">https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df</a></span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</div>
<p>VAE are in particular central to <strong>DeepFakes</strong> which have widely reached the media because of their impressive possibilities but also ethical issues:</p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/JbzVhzNaTdI' frameborder='0' allowfullscreen></iframe></div>
<p>Another class of generative models are <strong>generative adversarial networks</strong> (GAN) <a class="bibtex reference internal" href="../zreferences.html#goodfellow2014" id="id11">[Goodfellow et al., 2014]</a> which consist of a <strong>generator</strong> (decoder) and a <strong>discriminator</strong> that compete to produce realistic images while trying to discriminate generated from real images.</p>
<div class="figure align-default" id="id24">
<a class="reference internal image-reference" href="../_images/gan.jpg"><img alt="../_images/gan.jpg" src="../_images/gan.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.13 </span><span class="caption-text">Generative adversarial network.</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</div>
<p>Several evolutions of GANs have allowed to produce increasingly realistic images, such as conditional GANs who permit to generate images of a desired class, or CycleGAN which allows to replace an object with another:</p>
<div class="figure align-default" id="id25">
<a class="reference internal image-reference" href="../_images/cycleGAN4.jpg"><img alt="../_images/cycleGAN4.jpg" src="../_images/cycleGAN4.jpg" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.14 </span><span class="caption-text">CycleGAN. <a class="reference external" href="https://github.com/junyanz/CycleGAN">https://github.com/junyanz/CycleGAN</a></span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="reinforcement-learning">
<h3><span class="section-number">1.2.3. </span>Reinforcement learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">¶</a></h3>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/fczritSOcSM' frameborder='0' allowfullscreen></iframe></div>
<p>Reinforcement learning (RL) is not part of this module, as we offer a complete course on it:</p>
<p><a class="reference external" href="https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/">https://www.tu-chemnitz.de/informatik/KI/edu/deeprl/</a></p>
<p>but it has recently gained a lot of importance when coupled with deep learning principles. Here we just present a couple of application of <strong>deep reinforcement learning</strong> to motivate you to also assist to this course.</p>
<p>RL models the sequential interaction between an <strong>agent</strong> (algorithm, robot) and its <strong>environment</strong>. At each time step <span class="math notranslate nohighlight">\(t\)</span>, the agent is a state <span class="math notranslate nohighlight">\(s_t\)</span> and selects an action <span class="math notranslate nohighlight">\(a_t\)</span> according to its policy (or strategy) <span class="math notranslate nohighlight">\(\pi\)</span>. This brings the agent in a new state <span class="math notranslate nohighlight">\(s_{t+1}\)</span> and provides a reward <span class="math notranslate nohighlight">\(r_{t+1}\)</span>. The reward is the only feedback that the agent receives about its action: when it is positive, it is good; when it is negative, it is bad. The goal of the of the agent is to maximize the sum of rewards that it receives <strong>on the long-term</strong>. For example in a video game, the states would correspond to each video frame, the actions are joystick movements and the rewards are scores increases and decreases. The goal is to move the joystick correctly so that the final cumulated score is maximal.</p>
<div class="figure align-default" id="id26">
<a class="reference internal image-reference" href="../_images/rl-loop.png"><img alt="../_images/rl-loop.png" src="../_images/rl-loop.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 1.15 </span><span class="caption-text">Agent-environment interaction in RL. Source: <a class="reference external" href="https://ieeexplore.ieee.org/document/7839568">https://ieeexplore.ieee.org/document/7839568</a></span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
<p>In deep RL, the policy <span class="math notranslate nohighlight">\(\pi\)</span> is implemented using a deep neural network whose job is to predict which action in a given state is the most likely to provide reward in the long-term. Contrary to supervised learning, we do not know which action should have been performed (ground truth), we only get rewards indicating if this was a good choice or not. This makes the learning problem much harder. But the deep RL methods are quite generic: any problem that can be described in terms of states, actions and rewards (formally, a Markov decision process) can be solved by deep RL techniques, at the cost of quite long training times. Let’s have a look at some applications:</p>
<ul class="simple">
<li><p>The first achievement of deep RL was the <strong>deep Q-network (DQN)</strong> of Deepmind able to solve a multitude of old Atari games from scratch using raw video inputs:</p></li>
</ul>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/rQIShnTz1kU' frameborder='0' allowfullscreen></iframe></div>
<ul class="simple">
<li><p>Deep RL methods have since then been applied to more complex games, such as <strong>Starcraft II</strong>:</p></li>
</ul>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/UuhECwm31dM' frameborder='0' allowfullscreen></iframe></div>
<p>or <strong>DotA 2</strong>:</p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/eHipy_j29Xw' frameborder='0' allowfullscreen></iframe></div>
<ul class="simple">
<li><p>Another famous achievement of deep RL is when Google Deepmind’s <strong>AlphaGo</strong> beat Lee Sedol, 19 times world champion, in 2016:</p></li>
</ul>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/8tq1C8spV_g' frameborder='0' allowfullscreen></iframe></div>
<ul class="simple">
<li><p>Deep RL is also a very promising to <strong>robotics</strong>, be it in simulation:</p></li>
</ul>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/faDKMMwOS2Q' frameborder='0' allowfullscreen></iframe></div>
<p>or in reality:</p>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/jwSbzNHGflM' frameborder='0' allowfullscreen></iframe></div>
<ul class="simple">
<li><p>It is also promising for <strong>autonomous driving</strong>:</p></li>
</ul>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/eRwTbRtnT1I' frameborder='0' allowfullscreen></iframe></div>
</div>
</div>
<div class="section" id="outlook">
<h2><span class="section-number">1.3. </span>Outlook<a class="headerlink" href="#outlook" title="Permalink to this headline">¶</a></h2>
<div class='embed-container'><iframe src='https://www.youtube.com/embed/KAE5mn7WFgU' frameborder='0' allowfullscreen></iframe></div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./1-intro"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../intro.html" title="previous page">Neurocomputing</a>
    <a class='right-next' id="next-link" href="2-Math.html" title="next page"><span class="section-number">2. </span>Math basics (optional)</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>