{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Polynomial regression\n",
    "\n",
    "Polynomial regression consists of fitting some data $(x, y)$ to a $n$-order polynomial of the form:\n",
    "\n",
    "$$\n",
    " y = f(x) = w_0 + w_1 \\cdot x + w_2 \\cdot x^2 + ... + w_n \\cdot x^n\n",
    "$$\n",
    " \n",
    "By rewriting the unidimensional input $x$ into the following vector:\n",
    "\n",
    "$$\n",
    " \\mathbf{x} = \\begin{bmatrix} 1 & x & x^2 & ... & x^n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the weight vector as:\n",
    "\n",
    "$$\n",
    " \\mathbf{w} = \\begin{bmatrix} w_0 & w_1 & w_2 & ... & w_n \\end{bmatrix}\n",
    "$$\n",
    " \n",
    "the problem can be reduced to linear regression:\n",
    "\n",
    "$$\n",
    " y = \\langle \\mathbf{w} \\cdot \\mathbf{x} \\rangle\n",
    "$$\n",
    " \n",
    " and we can apply the delta learning rule to find $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    " \\Delta \\mathbf{w} =  \\eta \\cdot (t_i - y_i ) \\cdot \\mathbf{x_i}\n",
    "$$\n",
    "\n",
    "A first method to perform polynomial regression would be to adapt the code you wrote in the last exercise session for linear classification. However, you saw that properly setting the correct learning rate and stop criteria can be quite tricky. \n",
    "\n",
    "The solution retained for this exercise is to use the built-in functions of Numpy which can already perform polynomial regression in an optimized and proved-sure manner (Note: NumPy does not use gradient descent, but rather directly minimizes the error-function by inversing the Gram matrix).\n",
    "\n",
    "```python\n",
    "w = np.polyfit(X, t, deg)\n",
    "```\n",
    "\n",
    "This function takes the inputs $X$, the desired outputs $t$ and the desired degree of the polynomial `deg`, performs the polynomial regression and returns the adequate set of weights (beware: the higher-order coefficient comes first).\n",
    "\n",
    "Once the weights are obtained, one can use them to predict the value of an example with the function:\n",
    "\n",
    "```python\n",
    "y = np.polyval(w, X)\n",
    "```\n",
    "\n",
    "Let's start by importing the usual stuff, define a visualization method and load the data `polynome.data` (16 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Load the data set\n",
    "data = np.loadtxt('polynome.data', delimiter=\";\")\n",
    "X = data[:, 0]\n",
    "t = data[:, 1]\n",
    "N = len(X)\n",
    "\n",
    "def visualize(X, t, w):\n",
    "    # Plot the data\n",
    "    plt.plot(X, t, 'r.')\n",
    "    # Plot the fitted curve \n",
    "    x = np.linspace(0., 1., 100)\n",
    "    y = np.polyval(w, x)\n",
    "    plt.plot(x, y, 'g-')\n",
    "    plt.title('Polynomial regression with order ' + str(len(w)-1))\n",
    "    plt.axis([-0.1, 1.1, -0.1, 1.1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** Apply the `np.polyfit()` function on the data and visualize the result for different degrees of the polynomial (from 1 to 10 or even more). What do you observe? Find a polynomial degree which clearly overfits.\n",
    "\n",
    "*Bonus question:* make sense of the warning you get when the degree of the polynomial is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 100\n",
    "w = np.polyfit(X, t, deg)\n",
    "visualize(X, t, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Plot the mean square error on the training set for all polynomial regressions from 1 to 10. How does the training error evolve when the degree of the polynomial is increased? What is the risk by taking the hypothesis with the smallest training error? \n",
    "\n",
    "*Hint:* use `np.polyval()` for the prediction, and compute the mse with `np.mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mse = []\n",
    "for deg in range(1, 11):\n",
    "    w = np.polyfit(X, t, deg)\n",
    "    y = np.polyval(w, X)\n",
    "    mse = np.mean((t-y)**2)\n",
    "    training_mse.append(mse)\n",
    "    print(\"Degree\", deg, \": training error\", mse)\n",
    "    \n",
    "plt.plot(range(1, 11), training_mse)\n",
    "plt.xlabel(\"Order of the polynomial\")\n",
    "plt.ylabel(\"Training mse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** the more complex the model, the smaller the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple hold-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now apply **simple hold-out cross-validation** to find the optimal degree for the polynomial regression. You will need to separate the data set into a training set $S_{\\text{train}}$ (70% of the data) and a test set $S_{\\text{test}}$ (the remaining 30%). \n",
    "\n",
    "The data (X, t) could be easily split into two sets of arrays using slices of indices, as the data is already randomized:\n",
    "\n",
    "```python\n",
    "N_train = int(0.7*N)\n",
    "X_train, t_train = X[:N_train], t[:N_train]\n",
    "X_test, t_test = X[N_train:], t[N_train:]\n",
    "```\n",
    "\n",
    "A much more generic approach is to use the library `scikit-learn` (<https://www.scikit-learn.org>), which provides a method able to split any dataset randomly. \n",
    "\n",
    "You can import the method `train_test_split()` from its module:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "The doc of the function is available at: <https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html>.\n",
    "\n",
    "**Q3:** Use scikit-learn to split the data into the corresponding training and test sets. \n",
    "\n",
    "**Q4:** Train each polynomial from degree 1 to 10 on $S_{\\text{train}}$ and plot the generalization error on $S_{\\text{test}}$. Which degree of the polynomial gives the minimal empirical error? Why?\n",
    "\n",
    "**Q5:** Run the cross-validation split multiple times. Do you always obtain the same optimal degree? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3)\n",
    "\n",
    "test_mse = []\n",
    "\n",
    "for deg in range(1, 11):\n",
    "    w = np.polyfit(X_train, t_train, deg)\n",
    "    y_test = np.polyval(w, X_test)\n",
    "    mse = np.mean((t_test-y_test)**2)\n",
    "    test_mse.append(mse)\n",
    "    print(\"Degree\", deg, \": empirical error\", mse)\n",
    "    \n",
    "plt.plot(range(1, 11), test_mse)\n",
    "plt.xlabel(\"Order of the polynomial\")\n",
    "plt.ylabel(\"Test mse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** depending on the split, the optimal degree is either 4, 5, or 6. This is because some samples of the test set might better fit with a given polynomial, depending on how the split was made.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we only have 16 samples, it is quite annoying to \"lose\" 5 of them for the test set. Here we can afford to use **k-fold cross-validation**, where the cross-validation split is performed $k$ times:\n",
    "\n",
    "* The dataset is split into $k$ subsets of equal size (if possible).\n",
    "* Each subset is iteratively used as the test set, while the $k-1$ other ones are used as a training set.\n",
    "* The final empirical error is the average of the mse on all subsets.\n",
    "\n",
    "It would be possible to make the splits using indices too. But it is much easier to use `scikit-learn` once again. You can import the `KFold` class like this:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 4\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "```\n",
    "\n",
    "`n_splits` corresponds to $k$: how many times the dataset is split. Takes $k=4$ for example (4 subsets of 4 samples).\n",
    "\n",
    "**Q6:** Check the doc of `KFold` (<https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html>). Print the indices of the examples of the training and test sets for each iteration of the algorithm. Change the value of $k$ to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 4\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(X, t):\n",
    "    print(train_index)\n",
    "    print(test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Apply k-fold cross-validation on the polynomial regression problem. Which polynomial degree is the best? Run the split multiple times: does the best polynomial degree change?\n",
    "\n",
    "**Q8:** Change $k$ to 16. How stable are the results between two runs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "test_mse = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, t):\n",
    "    \n",
    "    split_mse = []\n",
    "    for deg in range(1, 11):\n",
    "        w = np.polyfit(X[train_index], t[train_index], deg)\n",
    "        y = np.polyval(w, X[test_index])\n",
    "        mse = np.mean((t[test_index] - y)**2)\n",
    "        split_mse.append(mse)\n",
    "    \n",
    "    test_mse.append(split_mse)\n",
    "        \n",
    "test_mse = np.mean(test_mse, axis=0)\n",
    "\n",
    "print(test_mse)\n",
    "\n",
    "plt.plot(range(1, 11), test_mse)\n",
    "plt.xlabel(\"Degree of the polynome\")\n",
    "plt.ylabel(\"k-fold cross-validated mse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** a polynomial of order 4 seems the best (it was indeed used to generate the data). With $k=16$ (one sample in the test set every time), the results are the most stable. It is called **leave-one-out cross-validation** (LOOCV). It is the best cross-validation you can make in terms of bias (you use almost all your data to learn), but it is very expensive (you have to retrain your algorithm for each sample) and the empirical error has a high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
