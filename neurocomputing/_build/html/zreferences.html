
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Bibliography &#8212; Neurocomputing</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/zreferences.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="13.2. Recurrent neural networks" href="5-exercises/13-RNN-solution.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/tuc.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neurocomputing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1-intro/2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3-deeplearning/10-Attention.html">
   10. Attentional neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex1-Python.html">
   1. Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/11-TransferLearning.html">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="5-exercises/ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-exercises/13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/zreferences.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bibliography">
<h1><span class="section-number">1. </span>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h1>
<p id="id1"><dl class="citation">
<dt class="label" id="id5"><span class="brackets">Arjovsky et al., 2017</span></dt>
<dd><p>Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017 , January). Wasserstein GAN. <em>arXiv:1701.07875 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</a>, <a class="reference external" href="https://arxiv.org/abs/1701.07875">arXiv:1701.07875</a></p>
</dd>
<dt class="label" id="id8"><span class="brackets">Ba et al., 2016</span></dt>
<dd><p>Ba, J. L., Kiros, J. R., &amp; Hinton, G. E. (2016 , July). Layer Normalization. <em>arXiv:1607.06450 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1607.06450">http://arxiv.org/abs/1607.06450</a>, <a class="reference external" href="https://arxiv.org/abs/1607.06450">arXiv:1607.06450</a></p>
</dd>
<dt class="label" id="id9"><span class="brackets">Badrinarayanan et al., 2016</span></dt>
<dd><p>Badrinarayanan, V., Kendall, A., &amp; Cipolla, R. (2016 , October). SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation. <em>arXiv:1511.00561 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1511.00561">http://arxiv.org/abs/1511.00561</a>, <a class="reference external" href="https://arxiv.org/abs/1511.00561">arXiv:1511.00561</a></p>
</dd>
<dt class="label" id="id10"><span class="brackets">Bahdanau et al., 2016</span></dt>
<dd><p>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2016 , May). Neural Machine Translation by Jointly Learning to Align and Translate. <em>arXiv:1409.0473 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a>, <a class="reference external" href="https://arxiv.org/abs/1409.0473">arXiv:1409.0473</a></p>
</dd>
<dt class="label" id="id13"><span class="brackets">Bi &amp; Poo, 2001</span></dt>
<dd><p>Bi, G.-q., &amp; Poo, M.-m. (2001). Synaptic Modification by Correlated Activity: Hebb's Postulate Revisited. <em>Annual Review of Neuroscience</em>, <em>24</em>(1), 139–166. URL: <a class="reference external" href="https://doi.org/10.1146/annurev.neuro.24.1.139">https://doi.org/10.1146/annurev.neuro.24.1.139</a>, <a class="reference external" href="https://doi.org/10.1146/annurev.neuro.24.1.139">doi:10.1146/annurev.neuro.24.1.139</a></p>
</dd>
<dt class="label" id="id14"><span class="brackets">Bienenstock et al., 1982</span></dt>
<dd><p>Bienenstock, E. L., Cooper, L. N., &amp; Munro, P. W. (1982 , January). Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. <em>J Neurosci</em>, <em>2</em>(1), 32–48.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">Binder et al., 2016</span></dt>
<dd><p>Binder, A., Montavon, G., Bach, S., Müller, K.-R., &amp; Samek, W. (2016 , April). Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers. <em>arXiv:1604.00825 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1604.00825">http://arxiv.org/abs/1604.00825</a>, <a class="reference external" href="https://arxiv.org/abs/1604.00825">arXiv:1604.00825</a></p>
</dd>
<dt class="label" id="id16"><span class="brackets">Bojarski et al., 2016</span></dt>
<dd><p>Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., … Zieba, K. (2016 , April). End to End Learning for Self-Driving Cars. <em>arXiv:1604.07316 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1604.07316">http://arxiv.org/abs/1604.07316</a>, <a class="reference external" href="https://arxiv.org/abs/1604.07316">arXiv:1604.07316</a></p>
</dd>
<dt class="label" id="id19"><span class="brackets">Brette &amp; Gerstner, 2005</span></dt>
<dd><p>Brette, R., &amp; Gerstner, W. (2005 , November). Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity. <em>Journal of Neurophysiology</em>, <em>94</em>(5), 3637–3642. URL: <a class="reference external" href="https://journals.physiology.org/doi/full/10.1152/jn.00686.2005">https://journals.physiology.org/doi/full/10.1152/jn.00686.2005</a>, <a class="reference external" href="https://doi.org/10.1152/jn.00686.2005">doi:10.1152/jn.00686.2005</a></p>
</dd>
<dt class="label" id="id28"><span class="brackets">Chollet, 2017a</span></dt>
<dd><p>Chollet, F. (2017). <em>Deep Learning with Python</em>. Manning publications.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">Chollet, 2017b</span></dt>
<dd><p>Chollet, F. (2017 , April). Xception: Deep Learning with Depthwise Separable Convolutions. <em>arXiv:1610.02357 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1610.02357">http://arxiv.org/abs/1610.02357</a>, <a class="reference external" href="https://arxiv.org/abs/1610.02357">arXiv:1610.02357</a></p>
</dd>
<dt class="label" id="id30"><span class="brackets">Chung et al., 2014</span></dt>
<dd><p>Chung, J., Gulcehre, C., Cho, K., &amp; Bengio, Y. (2014 , December). Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. <em>arXiv:1412.3555 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1412.3555">http://arxiv.org/abs/1412.3555</a>, <a class="reference external" href="https://arxiv.org/abs/1412.3555">arXiv:1412.3555</a></p>
</dd>
<dt class="label" id="id31"><span class="brackets">Clopath et al., 2010</span></dt>
<dd><p>Clopath, C., Büsing, L., Vasilaki, E., &amp; Gerstner, W. (2010 , March). Connectivity reflects coding: a model of voltage-based STDP with homeostasis. <em>Nature Neuroscience</em>, <em>13</em>(3), 344–352. URL: <a class="reference external" href="https://www.nature.com/articles/nn.2479">https://www.nature.com/articles/nn.2479</a>, <a class="reference external" href="https://doi.org/10.1038/nn.2479">doi:10.1038/nn.2479</a></p>
</dd>
<dt class="label" id="id33"><span class="brackets">Dayan &amp; Abbott, 2001</span></dt>
<dd><p>Dayan, P., &amp; Abbott, L. F. (2001 , September). <em>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</em>. The MIT Press.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">Demircigil et al., 2017</span></dt>
<dd><p>Demircigil, M., Heusel, J., Löwe, M., Upgang, S., &amp; Vermet, F. (2017 , July). On a model of associative memory with huge storage capacity. <em>Journal of Statistical Physics</em>, <em>168</em>(2), 288–299. URL: <a class="reference external" href="http://arxiv.org/abs/1702.01929">http://arxiv.org/abs/1702.01929</a>, <a class="reference external" href="https://arxiv.org/abs/1702.01929">arXiv:1702.01929</a>, <a class="reference external" href="https://doi.org/10.1007/s10955-017-1806-y">doi:10.1007/s10955-017-1806-y</a></p>
</dd>
<dt class="label" id="id35"><span class="brackets">Devlin et al., 2019</span></dt>
<dd><p>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019 , May). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <em>arXiv:1810.04805 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>, <a class="reference external" href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></p>
</dd>
<dt class="label" id="id36"><span class="brackets">Dosovitskiy et al., 2021</span></dt>
<dd><p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … Houlsby, N. (2021 , June). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. <em>arXiv:2010.11929 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/2010.11929">http://arxiv.org/abs/2010.11929</a>, <a class="reference external" href="https://arxiv.org/abs/2010.11929">arXiv:2010.11929</a></p>
</dd>
<dt class="label" id="id44"><span class="brackets">Fukushima, 1980</span></dt>
<dd><p>Fukushima, K. (1980 , April). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. <em>Biological Cybernetics</em>, <em>36</em>(4), 193–202. URL: <a class="reference external" href="https://doi.org/10.1007/BF00344251">https://doi.org/10.1007/BF00344251</a>, <a class="reference external" href="https://doi.org/10.1007/BF00344251">doi:10.1007/BF00344251</a></p>
</dd>
<dt class="label" id="id47"><span class="brackets">Gers &amp; Schmidhuber, 2000</span></dt>
<dd><p>Gers, F. A., &amp; Schmidhuber, J. (2000 , July). Recurrent nets that time and count. <em>Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</em> (pp. 189–194 vol.3). <a class="reference external" href="https://doi.org/10.1109/IJCNN.2000.861302">doi:10.1109/IJCNN.2000.861302</a></p>
</dd>
<dt class="label" id="id49"><span class="brackets">Gerstner et al., 2014</span></dt>
<dd><p>Gerstner, W., Kistler, W., Naud, R., &amp; Paninski, L. (2014). <em>Neuronal Dynamics - a Neuroscience Textbook</em>. Cambridge University Press.</p>
</dd>
<dt class="label" id="id52"><span class="brackets">Girshick, 2015</span></dt>
<dd><p>Girshick, R. (2015 , September). Fast R-CNN. <em>arXiv:1504.08083 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1504.08083">http://arxiv.org/abs/1504.08083</a>, <a class="reference external" href="https://arxiv.org/abs/1504.08083">arXiv:1504.08083</a></p>
</dd>
<dt class="label" id="id51"><span class="brackets">Girshick et al., 2014</span></dt>
<dd><p>Girshick, R., Donahue, J., Darrell, T., &amp; Malik, J. (2014 , October). Rich feature hierarchies for accurate object detection and semantic segmentation. <em>arXiv:1311.2524 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1311.2524">http://arxiv.org/abs/1311.2524</a>, <a class="reference external" href="https://arxiv.org/abs/1311.2524">arXiv:1311.2524</a></p>
</dd>
<dt class="label" id="id54"><span class="brackets">Glorot &amp; Bengio, 2010</span></dt>
<dd><p>Glorot, X., &amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. <em>AISTATS</em> (p. 8).</p>
</dd>
<dt class="label" id="id57"><span class="brackets">Goodfellow et al., 2016</span></dt>
<dd><p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p>
</dd>
<dt class="label" id="id56"><span class="brackets">Goodfellow et al., 2015</span></dt>
<dd><p>Goodfellow, I. J., Shlens, J., &amp; Szegedy, C. (2015 , March). Explaining and Harnessing Adversarial Examples. <em>arXiv:1412.6572 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1412.6572">http://arxiv.org/abs/1412.6572</a>, <a class="reference external" href="https://arxiv.org/abs/1412.6572">arXiv:1412.6572</a></p>
</dd>
<dt class="label" id="id55"><span class="brackets">Goodfellow et al., 2014</span></dt>
<dd><p>Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014 , June). Generative Adversarial Networks. <em>arXiv:1406.2661 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a>, <a class="reference external" href="https://arxiv.org/abs/1406.2661">arXiv:1406.2661</a></p>
</dd>
<dt class="label" id="id58"><span class="brackets">Gou et al., 2020</span></dt>
<dd><p>Gou, J., Yu, B., Maybank, S. J., &amp; Tao, D. (2020 , June). Knowledge Distillation: A Survey. <em>arXiv:2006.05525 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/2006.05525">http://arxiv.org/abs/2006.05525</a>, <a class="reference external" href="https://arxiv.org/abs/2006.05525">arXiv:2006.05525</a></p>
</dd>
<dt class="label" id="id62"><span class="brackets">Guo et al., 2017</span></dt>
<dd><p>Guo, X., Liu, X., Zhu, E., &amp; Yin, J. (2017). Liu, D., Xie, S., Li, Y., Zhao, D., &amp; El-Alfy, E.-S. M. (Eds.). Deep Clustering with Convolutional Autoencoders. <em>Neural Information Processing</em> (pp. 373–382). Cham: Springer International Publishing. <a class="reference external" href="https://doi.org/10.1007/978-3-319-70096-0_39">doi:10.1007/978-3-319-70096-0_39</a></p>
</dd>
<dt class="label" id="id65"><span class="brackets">Gupta et al., 2014</span></dt>
<dd><p>Gupta, S., Girshick, R., Arbeláez, P., &amp; Malik, J. (2014 , July). Learning Rich Features from RGB-D Images for Object Detection and Segmentation. <em>arXiv:1407.5736 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1407.5736">http://arxiv.org/abs/1407.5736</a>, <a class="reference external" href="https://arxiv.org/abs/1407.5736">arXiv:1407.5736</a></p>
</dd>
<dt class="label" id="id68"><span class="brackets">Hannun et al., 2014</span></dt>
<dd><p>Hannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E., … Ng, A. Y. (2014 , December). Deep Speech: Scaling up end-to-end speech recognition. <em>arXiv:1412.5567 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1412.5567">http://arxiv.org/abs/1412.5567</a>, <a class="reference external" href="https://arxiv.org/abs/1412.5567">arXiv:1412.5567</a></p>
</dd>
<dt class="label" id="id70"><span class="brackets">Haykin, 2009</span></dt>
<dd><p>Haykin, S. S. (2009). <em>Neural Networks and Learning Machines, 3rd Edition</em>. Pearson.</p>
</dd>
<dt class="label" id="id73"><span class="brackets">He et al., 2018</span></dt>
<dd><p>He, K., Gkioxari, G., Dollár, P., &amp; Girshick, R. (2018 , January). Mask R-CNN. <em>arXiv:1703.06870 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1703.06870">http://arxiv.org/abs/1703.06870</a>, <a class="reference external" href="https://arxiv.org/abs/1703.06870">arXiv:1703.06870</a></p>
</dd>
<dt class="label" id="id71"><span class="brackets">He et al., 2015a</span></dt>
<dd><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015 , December). Deep Residual Learning for Image Recognition. <em>arXiv:1512.03385 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>, <a class="reference external" href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a></p>
</dd>
<dt class="label" id="id72"><span class="brackets">He et al., 2015b</span></dt>
<dd><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015 , February). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. <em>arXiv:1502.01852 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1502.01852">http://arxiv.org/abs/1502.01852</a>, <a class="reference external" href="https://arxiv.org/abs/1502.01852">arXiv:1502.01852</a></p>
</dd>
<dt class="label" id="id75"><span class="brackets">Higgins et al., 2016</span></dt>
<dd><p>Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., … Lerchner, A. (2016 , November). Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. <em>ICLR 2017</em>. URL: <a class="reference external" href="https://openreview.net/forum?id=Sy2fzU9gl">https://openreview.net/forum?id=Sy2fzU9gl</a></p>
</dd>
<dt class="label" id="id76"><span class="brackets">Hinton &amp; Salakhutdinov, 2006</span></dt>
<dd><p>Hinton, G. E., &amp; Salakhutdinov, R. R. (2006 , July). Reducing the Dimensionality of Data with Neural Networks. <em>Science</em>, <em>313</em>(5786), 504–507. URL: <a class="reference external" href="https://science.sciencemag.org/content/313/5786/504">https://science.sciencemag.org/content/313/5786/504</a>, <a class="reference external" href="https://doi.org/10.1126/science.1127647">doi:10.1126/science.1127647</a></p>
</dd>
<dt class="label" id="id78"><span class="brackets">Hinton et al., 2015</span></dt>
<dd><p>Hinton, G., Vinyals, O., &amp; Dean, J. (2015 , March). Distilling the Knowledge in a Neural Network. <em>arXiv:1503.02531 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1503.02531">http://arxiv.org/abs/1503.02531</a>, <a class="reference external" href="https://arxiv.org/abs/1503.02531">arXiv:1503.02531</a></p>
</dd>
<dt class="label" id="id77"><span class="brackets">Hinton et al., 2006</span></dt>
<dd><p>Hinton, G. E., Osindero, S., &amp; Teh, Y.-W. (2006 , July). A fast learning algorithm for deep belief nets. <em>Neural Computation</em>, <em>18</em>(7), 1527–1554. URL: <a class="reference external" href="https://doi.org/10.1162/neco.2006.18.7.1527">https://doi.org/10.1162/neco.2006.18.7.1527</a>, <a class="reference external" href="https://doi.org/10.1162/neco.2006.18.7.1527">doi:10.1162/neco.2006.18.7.1527</a></p>
</dd>
<dt class="label" id="id80"><span class="brackets">Hochreiter &amp; Schmidhuber, 1997</span></dt>
<dd><p>Hochreiter, S., &amp; Schmidhuber, J. (1997 , November). Long short-term memory. <em>Neural computation</em>, <em>9</em>(8), 1735–80. URL: <a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed/9377276">http://www.ncbi.nlm.nih.gov/pubmed/9377276</a></p>
</dd>
<dt class="label" id="id79"><span class="brackets">Hochreiter, 1991</span></dt>
<dd><p>Hochreiter, S. (1991). <em>Untersuchungen Zu Dynamischen Neuronalen Netzen</em> (Doctoral dissertation). TU München.</p>
</dd>
<dt class="label" id="id82"><span class="brackets">Hopfield et al., 1983</span></dt>
<dd><p>Hopfield, J. J., Feinstein, D. I., &amp; Palmer, R. G. (1983 , July). `Unlearning' has a stabilizing effect in collective memories. <em>Nature</em>, <em>304</em>(5922), 158–159. URL: <a class="reference external" href="https://www.nature.com/articles/304158a0">https://www.nature.com/articles/304158a0</a>, <a class="reference external" href="https://doi.org/10.1038/304158a0">doi:10.1038/304158a0</a></p>
</dd>
<dt class="label" id="id84"><span class="brackets">Huang et al., 2018</span></dt>
<dd><p>Huang, G., Liu, Z., van der Maaten, L., &amp; Weinberger, K. Q. (2018 , January). Densely Connected Convolutional Networks. <em>arXiv:1608.06993 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1608.06993">http://arxiv.org/abs/1608.06993</a>, <a class="reference external" href="https://arxiv.org/abs/1608.06993">arXiv:1608.06993</a></p>
</dd>
<dt class="label" id="id87"><span class="brackets">Intrator &amp; Cooper, 1992</span></dt>
<dd><p>Intrator, N., &amp; Cooper, L. N. (1992 , January). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. <em>Neural Networks</em>, <em>5</em>(1), 3–17. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0893608005800036">http://www.sciencedirect.com/science/article/pii/S0893608005800036</a>, <a class="reference external" href="https://doi.org/10.1016/S0893-6080(05)80003-6">doi:10.1016/S0893-6080(05)80003-6</a></p>
</dd>
<dt class="label" id="id88"><span class="brackets">Ioffe &amp; Szegedy, 2015</span></dt>
<dd><p>Ioffe, S., &amp; Szegedy, C. (2015 , February). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. <em>arXiv:1502.03167 [cs.LG]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</a>, <a class="reference external" href="https://arxiv.org/abs/1502.03167">arXiv:1502.03167</a></p>
</dd>
<dt class="label" id="id91"><span class="brackets">Isola et al., 2018</span></dt>
<dd><p>Isola, P., Zhu, J.-Y., Zhou, T., &amp; Efros, A. A. (2018 , November). Image-to-Image Translation with Conditional Adversarial Networks. <em>arXiv:1611.07004 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1611.07004">http://arxiv.org/abs/1611.07004</a>, <a class="reference external" href="https://arxiv.org/abs/1611.07004">arXiv:1611.07004</a></p>
</dd>
<dt class="label" id="id92"><span class="brackets">Izhikevich, 2003</span></dt>
<dd><p>Izhikevich, E. M. (2003 , January). Simple model of spiking neurons. <em>IEEE transactions on neural networks</em>, <em>14</em>(6), 1569–72. URL: <a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed/18244602">http://www.ncbi.nlm.nih.gov/pubmed/18244602</a>, <a class="reference external" href="https://doi.org/10.1109/TNN.2003.820440">doi:10.1109/TNN.2003.820440</a></p>
</dd>
<dt class="label" id="id93"><span class="brackets">Jaeger, 2001</span></dt>
<dd><p>Jaeger, H. (2001). <em>The &quot;Echo State&quot; Approach to Analysing and Training Recurrent Neural Networks</em>. Jacobs Universität Bremen.</p>
</dd>
<dt class="label" id="id96"><span class="brackets">Joshi &amp; Triesch, 2009</span></dt>
<dd><p>Joshi, P., &amp; Triesch, J. (2009 , June). Rules for information maximization in spiking neurons using intrinsic plasticity. <em>2009 International Joint Conference on Neural Networks</em> (pp. 1456–1461). <a class="reference external" href="https://doi.org/10.1109/IJCNN.2009.5178625">doi:10.1109/IJCNN.2009.5178625</a></p>
</dd>
<dt class="label" id="id99"><span class="brackets">Karras et al., 2020</span></dt>
<dd><p>Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., &amp; Aila, T. (2020 , March). Analyzing and Improving the Image Quality of StyleGAN. <em>arXiv:1912.04958 [cs, eess, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1912.04958">http://arxiv.org/abs/1912.04958</a>, <a class="reference external" href="https://arxiv.org/abs/1912.04958">arXiv:1912.04958</a></p>
</dd>
<dt class="label" id="id100"><span class="brackets">Kendall et al., 2016</span></dt>
<dd><p>Kendall, A., Grimes, M., &amp; Cipolla, R. (2016 , February). PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization. <em>arXiv:1505.07427 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1505.07427">http://arxiv.org/abs/1505.07427</a>, <a class="reference external" href="https://arxiv.org/abs/1505.07427">arXiv:1505.07427</a></p>
</dd>
<dt class="label" id="id101"><span class="brackets">Kheradpisheh et al., 2018</span></dt>
<dd><p>Kheradpisheh, S. R., Ganjtabesh, M., Thorpe, S. J., &amp; Masquelier, T. (2018 , March). STDP-based spiking deep convolutional neural networks for object recognition. <em>Neural Networks</em>, <em>99</em>, 56–67. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0893608017302903">http://www.sciencedirect.com/science/article/pii/S0893608017302903</a>, <a class="reference external" href="https://doi.org/10.1016/j.neunet.2017.12.005">doi:10.1016/j.neunet.2017.12.005</a></p>
</dd>
<dt class="label" id="id102"><span class="brackets">Kim, 2014</span></dt>
<dd><p>Kim, Y. (2014 , September). Convolutional Neural Networks for Sentence Classification. <em>arXiv:1408.5882 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1408.5882">http://arxiv.org/abs/1408.5882</a>, <a class="reference external" href="https://arxiv.org/abs/1408.5882">arXiv:1408.5882</a></p>
</dd>
<dt class="label" id="id106"><span class="brackets">Kingma &amp; Ba, 2014</span></dt>
<dd><p>Kingma, D., &amp; Ba, J. (2014). Adam: A Method for Stochastic Optimization. <em>Proc. ICLR</em> (pp. 1–13). URL: <a class="reference external" href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a>, <a class="reference external" href="https://doi.org/10.1145/1830483.1830503">doi:10.1145/1830483.1830503</a></p>
</dd>
<dt class="label" id="id105"><span class="brackets">Kingma &amp; Welling, 2013</span></dt>
<dd><p>Kingma, D. P., &amp; Welling, M. (2013 , December). Auto-Encoding Variational Bayes. <em>arXiv:1312.6114 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1312.6114">http://arxiv.org/abs/1312.6114</a>, <a class="reference external" href="https://arxiv.org/abs/1312.6114">arXiv:1312.6114</a></p>
</dd>
<dt class="label" id="id111"><span class="brackets">Krizhevsky et al., 2012</span></dt>
<dd><p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. <em>Advances in Neural Information Processing Systems (NIPS)</em>. URL: <a class="reference external" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
</dd>
<dt class="label" id="id112"><span class="brackets">Krotov &amp; Hopfield, 2016</span></dt>
<dd><p>Krotov, D., &amp; Hopfield, J. J. (2016 , September). Dense Associative Memory for Pattern Recognition. <em>arXiv:1606.01164 [cond-mat, q-bio, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1606.01164">http://arxiv.org/abs/1606.01164</a>, <a class="reference external" href="https://arxiv.org/abs/1606.01164">arXiv:1606.01164</a></p>
</dd>
<dt class="label" id="id114"><span class="brackets">Laje &amp; Buonomano, 2013</span></dt>
<dd><p>Laje, R., &amp; Buonomano, D. V. (2013 , July). Robust timing and motor patterns by taming chaos in recurrent neural networks. <em>Nature neuroscience</em>, <em>16</em>(7), 925–33. URL: <a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed/23708144">http://www.ncbi.nlm.nih.gov/pubmed/23708144</a>, <a class="reference external" href="https://doi.org/10.1038/nn.3405">doi:10.1038/nn.3405</a></p>
</dd>
<dt class="label" id="id116"><span class="brackets">Lapuschkin et al., 2019</span></dt>
<dd><p>Lapuschkin, S., Wäldchen, S., Binder, A., Montavon, G., Samek, W., &amp; Müller, K.-R. (2019 , March). Unmasking Clever Hans predictors and assessing what machines really learn. <em>Nature Communications</em>, <em>10</em>(1), 1096. URL: <a class="reference external" href="https://www.nature.com/articles/s41467-019-08987-4">https://www.nature.com/articles/s41467-019-08987-4</a>, <a class="reference external" href="https://doi.org/10.1038/s41467-019-08987-4">doi:10.1038/s41467-019-08987-4</a></p>
</dd>
<dt class="label" id="id117"><span class="brackets">Le, 2013</span></dt>
<dd><p>Le, Q. V. (2013 , May). Building high-level features using large scale unsupervised learning. <em>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</em> (pp. 8595–8598). Vancouver, BC, Canada: IEEE. URL: <a class="reference external" href="http://ieeexplore.ieee.org/document/6639343/">http://ieeexplore.ieee.org/document/6639343/</a>, <a class="reference external" href="https://doi.org/10.1109/ICASSP.2013.6639343">doi:10.1109/ICASSP.2013.6639343</a></p>
</dd>
<dt class="label" id="id119"><span class="brackets">LeCun et al., 1998</span></dt>
<dd><p>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient Based Learning Applied to Document Recognition. <em>Proceedings of the IEEE</em>, <em>86</em>(11), 2278–2324. <a class="reference external" href="https://doi.org/10.1109/5.726791">doi:10.1109/5.726791</a></p>
</dd>
<dt class="label" id="id120"><span class="brackets">Li et al., 2018</span></dt>
<dd><p>Li, H., Xu, Z., Taylor, G., Studer, C., &amp; Goldstein, T. (2018 , November). Visualizing the Loss Landscape of Neural Nets. <em>arXiv:1712.09913 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1712.09913">http://arxiv.org/abs/1712.09913</a>, <a class="reference external" href="https://arxiv.org/abs/1712.09913">arXiv:1712.09913</a></p>
</dd>
<dt class="label" id="id122"><span class="brackets">Lillicrap et al., 2016</span></dt>
<dd><p>Lillicrap, T. P., Cownden, D., Tweed, D. B., &amp; Akerman, C. J. (2016 , November). Random synaptic feedback weights support error backpropagation for deep learning. <em>Nature Communications</em>, <em>7</em>(1), 1–10. URL: <a class="reference external" href="https://www.nature.com/articles/ncomms13276">https://www.nature.com/articles/ncomms13276</a>, <a class="reference external" href="https://doi.org/10.1038/ncomms13276">doi:10.1038/ncomms13276</a></p>
</dd>
<dt class="label" id="id123"><span class="brackets">Linnainmaa, 1970</span></dt>
<dd><p>Linnainmaa, S. (1970). <em>The Representation of the Cumulative Rounding Error of an Algorithm as a Taylor Expansion of the Local Rounding Errors</em> (Master's thesis). Univ. Helsinki.</p>
</dd>
<dt class="label" id="id124"><span class="brackets">Liu et al., 2016</span></dt>
<dd><p>Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., &amp; Berg, A. C. (2016). SSD: Single Shot MultiBox Detector. <em>arXiv:1512.02325 [cs]</em>, <em>9905</em>, 21–37. URL: <a class="reference external" href="http://arxiv.org/abs/1512.02325">http://arxiv.org/abs/1512.02325</a>, <a class="reference external" href="https://arxiv.org/abs/1512.02325">arXiv:1512.02325</a>, <a class="reference external" href="https://doi.org/10.1007/978-3-319-46448-0_2">doi:10.1007/978-3-319-46448-0_2</a></p>
</dd>
<dt class="label" id="id125"><span class="brackets">Maas et al., 2013</span></dt>
<dd><p>Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models. <em>ICML</em> (p. 6).</p>
</dd>
<dt class="label" id="id126"><span class="brackets">Maass et al., 2002</span></dt>
<dd><p>Maass, W., Natschläger, T., &amp; Markram, H. (2002 , November). Real-time computing without stable states: a new framework for neural computation based on perturbations. <em>Neural computation</em>, <em>14</em>(11), 2531–60. URL: <a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed/12433288">http://www.ncbi.nlm.nih.gov/pubmed/12433288</a>, <a class="reference external" href="https://doi.org/10.1162/089976602760407955">doi:10.1162/089976602760407955</a></p>
</dd>
<dt class="label" id="id129"><span class="brackets">Malinowski et al., 2015</span></dt>
<dd><p>Malinowski, M., Rohrbach, M., &amp; Fritz, M. (2015 , October). Ask Your Neurons: A Neural-based Approach to Answering Questions about Images. <em>arXiv:1505.01121 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1505.01121">http://arxiv.org/abs/1505.01121</a>, <a class="reference external" href="https://arxiv.org/abs/1505.01121">arXiv:1505.01121</a></p>
</dd>
<dt class="label" id="id131"><span class="brackets">McEliece et al., 1987</span></dt>
<dd><p>McEliece, R., Posner, E., Rodemich, E., &amp; Venkatesh, S. (1987 , July). The capacity of the Hopfield associative memory. <em>IEEE Transactions on Information Theory</em>, <em>33</em>(4), 461–482. <a class="reference external" href="https://doi.org/10.1109/TIT.1987.1057328">doi:10.1109/TIT.1987.1057328</a></p>
</dd>
<dt class="label" id="id132"><span class="brackets">McInnes et al., 2020</span></dt>
<dd><p>McInnes, L., Healy, J., &amp; Melville, J. (2020 , September). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. <em>arXiv:1802.03426 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1802.03426">http://arxiv.org/abs/1802.03426</a>, <a class="reference external" href="https://arxiv.org/abs/1802.03426">arXiv:1802.03426</a></p>
</dd>
<dt class="label" id="id134"><span class="brackets">Mikolov et al., 2013</span></dt>
<dd><p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013 , September). Efficient Estimation of Word Representations in Vector Space. <em>arXiv:1301.3781 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>, <a class="reference external" href="https://arxiv.org/abs/1301.3781">arXiv:1301.3781</a></p>
</dd>
<dt class="label" id="id135"><span class="brackets">Mirza &amp; Osindero, 2014</span></dt>
<dd><p>Mirza, M., &amp; Osindero, S. (2014 , November). Conditional Generative Adversarial Nets. <em>arXiv:1411.1784 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1411.1784">http://arxiv.org/abs/1411.1784</a>, <a class="reference external" href="https://arxiv.org/abs/1411.1784">arXiv:1411.1784</a></p>
</dd>
<dt class="label" id="id141"><span class="brackets">Nowozin et al., 2016</span></dt>
<dd><p>Nowozin, S., Cseke, B., &amp; Tomioka, R. (2016 , June). F-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. <em>arXiv:1606.00709 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1606.00709">http://arxiv.org/abs/1606.00709</a>, <a class="reference external" href="https://arxiv.org/abs/1606.00709">arXiv:1606.00709</a></p>
</dd>
<dt class="label" id="id142"><span class="brackets">Oja, 1982</span></dt>
<dd><p>Oja, E. (1982 , January). A simplified neuron model as a principal component analyzer. <em>Journal of mathematical biology</em>, <em>15</em>(3), 267–73. URL: <a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed/7153672">http://www.ncbi.nlm.nih.gov/pubmed/7153672</a></p>
</dd>
<dt class="label" id="id143"><span class="brackets">Olshausen &amp; Field, 1997</span></dt>
<dd><p>Olshausen, B. A., &amp; Field, D. J. (1997 , December). Sparse coding with an overcomplete basis set: A strategy employed by V1? <em>Vision Research</em>, <em>37</em>(23), 3311–3325. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0042698997001697">http://www.sciencedirect.com/science/article/pii/S0042698997001697</a>, <a class="reference external" href="https://doi.org/10.1016/S0042-6989(97)00169-7">doi:10.1016/S0042-6989(97)00169-7</a></p>
</dd>
<dt class="label" id="id149"><span class="brackets">Radford et al., 2015</span></dt>
<dd><p>Radford, A., Metz, L., &amp; Chintala, S. (2015 , November). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. <em>arXiv:1511.06434 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1511.06434">http://arxiv.org/abs/1511.06434</a>, <a class="reference external" href="https://arxiv.org/abs/1511.06434">arXiv:1511.06434</a></p>
</dd>
<dt class="label" id="id152"><span class="brackets">Ramsauer et al., 2020</span></dt>
<dd><p>Ramsauer, H., Schäfl, B., Lehner, J., Seidl, P., Widrich, M., Adler, T., … Hochreiter, S. (2020 , December). Hopfield Networks is All You Need. <em>arXiv:2008.02217 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/2008.02217">http://arxiv.org/abs/2008.02217</a>, <a class="reference external" href="https://arxiv.org/abs/2008.02217">arXiv:2008.02217</a></p>
</dd>
<dt class="label" id="id155"><span class="brackets">Razavi et al., 2019</span></dt>
<dd><p>Razavi, A., van den Oord, A., &amp; Vinyals, O. (2019 , June). Generating Diverse High-Fidelity Images with VQ-VAE-2. <em>arXiv:1906.00446 [cs, stat]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1906.00446">http://arxiv.org/abs/1906.00446</a>, <a class="reference external" href="https://arxiv.org/abs/1906.00446">arXiv:1906.00446</a></p>
</dd>
<dt class="label" id="id156"><span class="brackets">Redmon et al., 2016</span></dt>
<dd><p>Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. (2016 , May). You Only Look Once: Unified, Real-Time Object Detection. <em>arXiv:1506.02640 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1506.02640">http://arxiv.org/abs/1506.02640</a>, <a class="reference external" href="https://arxiv.org/abs/1506.02640">arXiv:1506.02640</a></p>
</dd>
<dt class="label" id="id157"><span class="brackets">Redmon &amp; Farhadi, 2016</span></dt>
<dd><p>Redmon, J., &amp; Farhadi, A. (2016 , December). YOLO9000: Better, Faster, Stronger. <em>arXiv:1612.08242 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1612.08242">http://arxiv.org/abs/1612.08242</a>, <a class="reference external" href="https://arxiv.org/abs/1612.08242">arXiv:1612.08242</a></p>
</dd>
<dt class="label" id="id158"><span class="brackets">Redmon &amp; Farhadi, 2018</span></dt>
<dd><p>Redmon, J., &amp; Farhadi, A. (2018 , April). YOLOv3: An Incremental Improvement. <em>arXiv:1804.02767 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1804.02767">http://arxiv.org/abs/1804.02767</a>, <a class="reference external" href="https://arxiv.org/abs/1804.02767">arXiv:1804.02767</a></p>
</dd>
<dt class="label" id="id159"><span class="brackets">Reed et al., 2016</span></dt>
<dd><p>Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., &amp; Lee, H. (2016 , June). Generative Adversarial Text to Image Synthesis. <em>arXiv:1605.05396 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1605.05396">http://arxiv.org/abs/1605.05396</a>, <a class="reference external" href="https://arxiv.org/abs/1605.05396">arXiv:1605.05396</a></p>
</dd>
<dt class="label" id="id160"><span class="brackets">Ren et al., 2016</span></dt>
<dd><p>Ren, S., He, K., Girshick, R., &amp; Sun, J. (2016 , January). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. <em>arXiv:1506.01497 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1506.01497">http://arxiv.org/abs/1506.01497</a>, <a class="reference external" href="https://arxiv.org/abs/1506.01497">arXiv:1506.01497</a></p>
</dd>
<dt class="label" id="id161"><span class="brackets">Ronneberger et al., 2015</span></dt>
<dd><p>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015 , May). U-Net: Convolutional Networks for Biomedical Image Segmentation. <em>arXiv:1505.04597 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1505.04597">http://arxiv.org/abs/1505.04597</a>, <a class="reference external" href="https://arxiv.org/abs/1505.04597">arXiv:1505.04597</a></p>
</dd>
<dt class="label" id="id163"><span class="brackets">Rossant et al., 2011</span></dt>
<dd><p>Rossant, C., Goodman, D. F. M., Fontaine, B., Platkiewicz, J., Magnusson, A. K., &amp; Brette, R. (2011). Fitting Neuron Models to Spike Trains. <em>Frontiers in Neuroscience</em>, <em>5</em>. URL: <a class="reference external" href="https://www.frontiersin.org/articles/10.3389/fnins.2011.00009/full">https://www.frontiersin.org/articles/10.3389/fnins.2011.00009/full</a>, <a class="reference external" href="https://doi.org/10.3389/fnins.2011.00009">doi:10.3389/fnins.2011.00009</a></p>
</dd>
<dt class="label" id="id165"><span class="brackets">Rumelhart et al., 1986</span></dt>
<dd><p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986 , October). Learning representations by back-propagating errors. <em>Nature</em>, <em>323</em>(6088), 533–536. URL: <a class="reference external" href="https://www.nature.com/articles/323533a0">https://www.nature.com/articles/323533a0</a>, <a class="reference external" href="https://doi.org/10.1038/323533a0">doi:10.1038/323533a0</a></p>
</dd>
<dt class="label" id="id167"><span class="brackets">Salimans et al., 2016</span></dt>
<dd><p>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., &amp; Chen, X. (2016 , June). Improved Techniques for Training GANs. <em>arXiv:1606.03498 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1606.03498">http://arxiv.org/abs/1606.03498</a>, <a class="reference external" href="https://arxiv.org/abs/1606.03498">arXiv:1606.03498</a></p>
</dd>
<dt class="label" id="id175"><span class="brackets">Simoncelli &amp; Olshausen, 2001</span></dt>
<dd><p>Simoncelli, E. P., &amp; Olshausen, B. A. (2001 , March). Natural Image Statistics and Neural Representation. <em>Annual Review of Neuroscience</em>, <em>24</em>(1), 1193–1216. URL: <a class="reference external" href="https://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.1193">https://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.1193</a>, <a class="reference external" href="https://doi.org/10.1146/annurev.neuro.24.1.1193">doi:10.1146/annurev.neuro.24.1.1193</a></p>
</dd>
<dt class="label" id="id176"><span class="brackets">Simonyan &amp; Zisserman, 2015</span></dt>
<dd><p>Simonyan, K., &amp; Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>International Conference on Learning Representations (ICRL)</em>, pp. 1–14. URL: <a class="reference external" href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>, <a class="reference external" href="https://doi.org/10.1016/j.infsof.2008.09.005">doi:10.1016/j.infsof.2008.09.005</a></p>
</dd>
<dt class="label" id="id178"><span class="brackets">Sohn et al., 2015</span></dt>
<dd><p>Sohn, K., Lee, H., &amp; Yan, X. (2015). Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., &amp; Garnett, R. (Eds.). Learning Structured Output Representation using Deep Conditional Generative Models. <em>Advances in Neural Information Processing Systems 28</em> (pp. 3483–3491). Curran Associates, Inc.</p>
</dd>
<dt class="label" id="id179"><span class="brackets">Springenberg et al., 2015</span></dt>
<dd><p>Springenberg, J. T., Dosovitskiy, A., Brox, T., &amp; Riedmiller, M. (2015 , April). Striving for Simplicity: The All Convolutional Net. <em>arXiv:1412.6806 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1412.6806">http://arxiv.org/abs/1412.6806</a>, <a class="reference external" href="https://arxiv.org/abs/1412.6806">arXiv:1412.6806</a></p>
</dd>
<dt class="label" id="id180"><span class="brackets">Srivastava et al., 2014</span></dt>
<dd><p>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. <em>Journal of Machine Learning Research</em>, <em>15</em>(56), 1929–1958. URL: <a class="reference external" href="http://jmlr.org/papers/v15/srivastava14a.html">http://jmlr.org/papers/v15/srivastava14a.html</a></p>
</dd>
<dt class="label" id="id181"><span class="brackets">Srivastava et al., 2015</span></dt>
<dd><p>Srivastava, R. K., Greff, K., &amp; Schmidhuber, J. (2015 , November). Highway Networks. <em>arXiv:1505.00387 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1505.00387">http://arxiv.org/abs/1505.00387</a>, <a class="reference external" href="https://arxiv.org/abs/1505.00387">arXiv:1505.00387</a></p>
</dd>
<dt class="label" id="id183"><span class="brackets">Sussillo &amp; Abbott, 2009</span></dt>
<dd><p>Sussillo, D., &amp; Abbott, L. F. (2009 , August). Generating coherent patterns of activity from chaotic neural networks. <em>Neuron</em>, <em>63</em>(4), 544–57. URL: <a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed/19709635">http://www.ncbi.nlm.nih.gov/pubmed/19709635</a>, <a class="reference external" href="https://doi.org/10.1016/j.neuron.2009.07.018">doi:10.1016/j.neuron.2009.07.018</a></p>
</dd>
<dt class="label" id="id184"><span class="brackets">Sutskever et al., 2014</span></dt>
<dd><p>Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014 , December). Sequence to Sequence Learning with Neural Networks. <em>arXiv:1409.3215 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1409.3215">http://arxiv.org/abs/1409.3215</a>, <a class="reference external" href="https://arxiv.org/abs/1409.3215">arXiv:1409.3215</a></p>
</dd>
<dt class="label" id="id186"><span class="brackets">Szegedy et al., 2015</span></dt>
<dd><p>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &amp; Wojna, Z. (2015 , December). Rethinking the Inception Architecture for Computer Vision. <em>arXiv:1512.00567 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1512.00567">http://arxiv.org/abs/1512.00567</a>, <a class="reference external" href="https://arxiv.org/abs/1512.00567">arXiv:1512.00567</a></p>
</dd>
<dt class="label" id="id187"><span class="brackets">Taigman et al., 2014</span></dt>
<dd><p>Taigman, Y., Yang, M., Ranzato, Marc'Aurelio, &amp; Wolf, L. (2014 , June). DeepFace: Closing the Gap to Human-Level Performance in Face Verification. <em>2014 IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 1701–1708). Columbus, OH, USA: IEEE. URL: <a class="reference external" href="https://ieeexplore.ieee.org/document/6909616">https://ieeexplore.ieee.org/document/6909616</a>, <a class="reference external" href="https://doi.org/10.1109/CVPR.2014.220">doi:10.1109/CVPR.2014.220</a></p>
</dd>
<dt class="label" id="id188"><span class="brackets">Tanaka et al., 2019</span></dt>
<dd><p>Tanaka, G., Yamane, T., Héroux, J. B., Nakane, R., Kanazawa, N., Takeda, S., … Hirose, A. (2019 , July). Recent advances in physical reservoir computing: A review. <em>Neural Networks</em>, <em>115</em>, 100–123. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0893608019300784">http://www.sciencedirect.com/science/article/pii/S0893608019300784</a>, <a class="reference external" href="https://doi.org/10.1016/j.neunet.2019.03.005">doi:10.1016/j.neunet.2019.03.005</a></p>
</dd>
<dt class="label" id="id144"><span class="brackets">Oord et al., 2016</span></dt>
<dd><p>van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., … Kavukcuoglu, K. (2016 , September). WaveNet: A Generative Model for Raw Audio. <em>arXiv:1609.03499 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1609.03499">http://arxiv.org/abs/1609.03499</a>, <a class="reference external" href="https://arxiv.org/abs/1609.03499">arXiv:1609.03499</a></p>
</dd>
<dt class="label" id="id190"><span class="brackets">Vaswani et al., 2017</span></dt>
<dd><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017 , June). Attention Is All You Need. <em>arXiv:1706.03762 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>, <a class="reference external" href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></p>
</dd>
<dt class="label" id="id191"><span class="brackets">Vincent et al., 2010</span></dt>
<dd><p>Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., &amp; Manzagol, P.-A. (2010). Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. <em>Journal of Machine Learning Research</em>, p. 38.</p>
</dd>
<dt class="label" id="id192"><span class="brackets">Vinyals et al., 2015</span></dt>
<dd><p>Vinyals, O., Toshev, A., Bengio, S., &amp; Erhan, D. (2015 , April). Show and Tell: A Neural Image Caption Generator. <em>arXiv:1411.4555 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1411.4555">http://arxiv.org/abs/1411.4555</a>, <a class="reference external" href="https://arxiv.org/abs/1411.4555">arXiv:1411.4555</a></p>
</dd>
<dt class="label" id="id194"><span class="brackets">Vogels et al., 2011</span></dt>
<dd><p>Vogels, T. P., Sprekeler, H., Zenke, F., Clopath, C., &amp; Gerstner, W. (2011 , December). Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks. <em>Science</em>, <em>334</em>(6062), 1569–1573. URL: <a class="reference external" href="https://science.sciencemag.org/content/334/6062/1569">https://science.sciencemag.org/content/334/6062/1569</a>, <a class="reference external" href="https://doi.org/10.1126/science.1211095">doi:10.1126/science.1211095</a></p>
</dd>
<dt class="label" id="id195"><span class="brackets">Wang et al., 2018</span></dt>
<dd><p>Wang, B., Zheng, H., Liang, X., Chen, Y., Lin, L., &amp; Yang, M. (2018 , September). Toward Characteristic-Preserving Image-based Virtual Try-On Network. <em>arXiv:1807.07688 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1807.07688">http://arxiv.org/abs/1807.07688</a>, <a class="reference external" href="https://arxiv.org/abs/1807.07688">arXiv:1807.07688</a></p>
</dd>
<dt class="label" id="id197"><span class="brackets">Werbos, 1982</span></dt>
<dd><p>Werbos, P.J. (1982). Applications of advances in nonlinear sensitivity analysis. <em>System Modeling and Optimization: Proc. IFIP</em>. Springer.</p>
</dd>
<dt class="label" id="id198"><span class="brackets">Wu et al., 2016</span></dt>
<dd><p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., … Dean, J. (2016 , September). Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. <em>arXiv:1609.08144 [cs]</em>. URL: <a class="reference external" href="https://arxiv.org/abs/1609.08144v2">https://arxiv.org/abs/1609.08144v2</a>, <a class="reference external" href="https://arxiv.org/abs/1609.08144">arXiv:1609.08144</a></p>
</dd>
<dt class="label" id="id201"><span class="brackets">Xu et al., 2015</span></dt>
<dd><p>Xu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., … Bengio, Y. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. <em>Proceedings of the 32nd International Conference on Machine Learning - Volume 37</em> (pp. 2048–2057). JMLR.org. URL: <a class="reference external" href="http://dl.acm.org/citation.cfm?id=3045118.3045336">http://dl.acm.org/citation.cfm?id=3045118.3045336</a></p>
</dd>
<dt class="label" id="id211"><span class="brackets">Zhou &amp; Tuzel, 2017</span></dt>
<dd><p>Zhou, Y., &amp; Tuzel, O. (2017 , November). VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. <em>arXiv:1711.06396 [cs]</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1711.06396">http://arxiv.org/abs/1711.06396</a>, <a class="reference external" href="https://arxiv.org/abs/1711.06396">arXiv:1711.06396</a></p>
</dd>
<dt class="label" id="id212"><span class="brackets">Zhu et al., 2020</span></dt>
<dd><p>Zhu, Y., Gao, T., Fan, L., Huang, S., Edmonds, M., Liu, H., … Zhu, S.-C. (2020 , February). Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense. <em>Engineering</em>. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S2095809920300345">http://www.sciencedirect.com/science/article/pii/S2095809920300345</a>, <a class="reference external" href="https://doi.org/10.1016/j.eng.2020.01.011">doi:10.1016/j.eng.2020.01.011</a></p>
</dd>
</dl>
</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="5-exercises/13-RNN-solution.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">13.2. </span>Recurrent neural networks</p>
            </div>
        </a>
    </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>