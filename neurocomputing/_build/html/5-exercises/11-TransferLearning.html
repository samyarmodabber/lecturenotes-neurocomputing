
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>11.1. Transfer learning &#8212; Neurocomputing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/5-exercises/11-TransferLearning.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11.2. Transfer learning" href="11-TransferLearning-solution.html" />
    <link rel="prev" title="11. Transfer learning" href="ex11-TransferLearning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/tuc.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neurocomputing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/4-ObjectDetection.html">
   4. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/5-SemanticSegmentation.html">
   5. Semantic segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/6-Autoencoders.html">
   6. Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/7-RBM.html">
   7. Restricted Boltzmann machines (optional)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/8-GAN.html">
   8. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/9-RNN.html">
   9. Recurrent neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neurocomputing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/1-Limits.html">
   1. Limits of deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/2-Hopfield.html">
   2. Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/4-Reservoir.html">
   3. Reservoir computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/5-Hebbian.html">
   4. Unsupervised Hebbian learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4-neurocomputing/6-Spiking.html">
   5. Spiking neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex1-Python.html">
   1. Introduction to Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="1-Python.html">
     1.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="1-Python-solution.html">
     1.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="2-Numpy.html">
     2.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="2-Numpy-solution.html">
     2.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex3-LinearRegression.html">
   3. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="3-LinearRegression.html">
     3.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3-LinearRegression-solution.html">
     3.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="4-MLR.html">
     4.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4-MLR-solution.html">
     4.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="5-Crossvalidation.html">
     5.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5-Crossvalidation-solution.html">
     5.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex6-LinearClassification.html">
   6. Linear classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="6-LinearClassification.html">
     6.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="6-LinearClassification-solution.html">
     6.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="7-SoftmaxClassifier.html">
     7.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="7-SoftmaxClassifier-solution.html">
     7.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8-MLP-solution.html">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex9-MNIST.html">
   9. MNIST classification using keras
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="9-MNIST.html">
     9.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9-MNIST-solution.html">
     9.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex10-CNN.html">
   10. Convolutional neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="10-CNN.html">
     10.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="10-CNN-solution.html">
     10.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ex11-TransferLearning.html">
   11. Transfer learning
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     11.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="11-TransferLearning-solution.html">
     11.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex12-VAE.html">
   12. Variational autoencoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="12-VAE.html">
     12.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12-VAE-solution.html">
     12.2. Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ex13-RNN.html">
   13. Recurrent neural networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="13-RNN.html">
     13.1. Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13-RNN-solution.html">
     13.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/5-exercises/11-TransferLearning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/vitay/lecturenotes-neurocomputing/master?urlpath=lab/tree/neurocomputing/5-exercises/11-TransferLearning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/vitay/lecturenotes-neurocomputing/blob/master/neurocomputing/5-exercises/11-TransferLearning.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-the-cats-and-dogs-data">
   11.1.1. Loading the cats and dogs data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functional-api-of-keras">
   11.1.2. Functional API of Keras
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-a-cnn-from-scratch">
   11.1.3. Training a CNN from scratch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-augmentation">
   11.1.4. Data augmentation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   11.1.5. Transfer learning
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="transfer-learning">
<h1><span class="section-number">11.1. </span>Transfer learning<a class="headerlink" href="#transfer-learning" title="Permalink to this headline">¶</a></h1>
<p>The goal of the exercise is to investigate data augmentation and transfer learning on a very small dataset (2000 training images).</p>
<p>The code is based on the keras tutorial:</p>
<p><a class="reference external" href="https://keras.io/guides/transfer_learning/#an-endtoend-example-finetuning-an-image-classification-model-on-a-cats-vs-dogs">https://keras.io/guides/transfer_learning/#an-endtoend-example-finetuning-an-image-classification-model-on-a-cats-vs-dogs</a></p>
<p>The data is provided as part of a Google tutorial:</p>
<p><a class="reference external" href="https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb">https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="loading-the-cats-and-dogs-data">
<h2><span class="section-number">11.1.1. </span>Loading the cats and dogs data<a class="headerlink" href="#loading-the-cats-and-dogs-data" title="Permalink to this headline">¶</a></h2>
<p>The data we will use is a subset of the <a class="reference external" href="https://www.kaggle.com/c/dogs-vs-cats/data">“Dogs vs. Cats” dataset</a> dataset available on Kaggle, which contains 25,000 images. Here, we use only 1000 dogs and 1000 cats to decrease training time and make the problem harder.</p>
<p>The following cell downloads the data and decompresses it in <code class="docutils literal notranslate"><span class="pre">/tmp/</span></code> (it will be erased at the next restart of your computer).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>wget --no-check-certificate <span class="err">\</span>
    <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">storage</span><span class="o">.</span><span class="n">googleapis</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">mledu</span><span class="o">-</span><span class="n">datasets</span><span class="o">/</span><span class="n">cats_and_dogs_filtered</span><span class="o">.</span><span class="n">zip</span> \
    <span class="o">-</span><span class="n">O</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">cats_and_dogs_filtered</span><span class="o">.</span><span class="n">zip</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">zipfile</span>

<span class="n">local_zip</span> <span class="o">=</span> <span class="s1">&#39;/tmp/cats_and_dogs_filtered.zip&#39;</span>
<span class="n">zip_ref</span> <span class="o">=</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">local_zip</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">zip_ref</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="s1">&#39;/tmp&#39;</span><span class="p">)</span>
<span class="n">zip_ref</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--2020-12-23 11:23:19--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.63.128, 142.250.31.128, 172.217.7.176, ...
Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.63.128|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 68606236 (65M) [application/zip]
Saving to: ‘/tmp/cats_and_dogs_filtered.zip’

/tmp/cats_and_dogs_ 100%[===================&gt;]  65.43M   183MB/s    in 0.4s    

2020-12-23 11:23:20 (183 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]
</pre></div>
</div>
</div>
</div>
<p>All we have is a bunch of <code class="docutils literal notranslate"><span class="pre">*.jpg</span></code> images organized in a training and validation set, separated by their binary class dog vs. cat:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cats_and_dogs_filtered</span><span class="o">/</span>
    <span class="n">train</span><span class="o">/</span>
        <span class="n">dogs</span><span class="o">/</span>
            <span class="n">dog001</span><span class="o">.</span><span class="n">jpg</span>
            <span class="n">dog002</span><span class="o">.</span><span class="n">jpg</span>
            <span class="o">...</span>
        <span class="n">cats</span><span class="o">/</span>
            <span class="n">cat001</span><span class="o">.</span><span class="n">jpg</span>
            <span class="n">cat002</span><span class="o">.</span><span class="n">jpg</span>
            <span class="o">...</span>
    <span class="n">validation</span><span class="o">/</span>
        <span class="n">dogs</span><span class="o">/</span>
            <span class="n">dog001</span><span class="o">.</span><span class="n">jpg</span>
            <span class="n">dog002</span><span class="o">.</span><span class="n">jpg</span>
            <span class="o">...</span>
        <span class="n">cats</span><span class="o">/</span>
            <span class="n">cat001</span><span class="o">.</span><span class="n">jpg</span>
            <span class="n">cat002</span><span class="o">.</span><span class="n">jpg</span>
            <span class="o">...</span>
</pre></div>
</div>
<p>Feel free to download the data on your computer and have a look at the images directly.</p>
<p>The next cell checks the structure of the image directory:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">base_dir</span> <span class="o">=</span> <span class="s1">&#39;/tmp/cats_and_dogs_filtered&#39;</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">base_dir</span> <span class="o">+</span> <span class="s1">&#39;/train&#39;</span>
<span class="n">validation_dir</span> <span class="o">=</span> <span class="n">base_dir</span> <span class="o">+</span> <span class="s1">&#39;/validation&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;total training cat images:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_dir</span> <span class="o">+</span> <span class="s1">&#39;/cats&#39;</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;total training dog images:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_dir</span> <span class="o">+</span> <span class="s1">&#39;/dogs&#39;</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;total validation cat images:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">validation_dir</span> <span class="o">+</span> <span class="s1">&#39;/cats&#39;</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;total validation dog images:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">validation_dir</span> <span class="o">+</span> <span class="s1">&#39;/dogs&#39;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>total training cat images: 1000
total training dog images: 1000
total validation cat images: 500
total validation dog images: 500
</pre></div>
</div>
</div>
</div>
<p>The next cell visualizes some cats and dogs from the training set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="nn">mpimg</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">1000</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">next_cat_pix</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span> <span class="o">+</span> <span class="s1">&#39;/cats&#39;</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span> <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_dir</span> <span class="o">+</span> <span class="s1">&#39;/cats&#39;</span><span class="p">)[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">8</span><span class="p">]]</span>
<span class="n">next_dog_pix</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span> <span class="o">+</span> <span class="s1">&#39;/dogs&#39;</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span> <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_dir</span> <span class="o">+</span> <span class="s1">&#39;/dogs&#39;</span><span class="p">)[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">8</span><span class="p">]]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">next_cat_pix</span><span class="o">+</span><span class="n">next_dog_pix</span><span class="p">):</span>
  <span class="c1"># Set up subplot; subplot indices start at 1</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;Off&#39;</span><span class="p">)</span> <span class="c1"># Don&#39;t show axes (or gridlines)</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/11-TransferLearning_8_0.png" src="../_images/11-TransferLearning_8_0.png" />
</div>
</div>
<p>In order to train a binary classifier on this data, we would need to load the images and transform them into Numpy arrays that can be passed to tensorflow. Fortunately, keras provides an utility to do it automatically: <code class="docutils literal notranslate"><span class="pre">ImageDataGenerator</span></code>. Doc:</p>
<p><a class="reference external" href="https://keras.io/api/preprocessing/image/">https://keras.io/api/preprocessing/image/</a></p>
<p>The procedure is to create an <code class="docutils literal notranslate"><span class="pre">ImageDataGenerator</span></code> instance and to create an <strong>iterator</strong> with <code class="docutils literal notranslate"><span class="pre">flow_from_directory</span></code> that will return minibatches on demand when training the neural network. The main advantage of this approach is that you do not need to load the whole dataset in the RAM (not possible for most realistic datasets), but adds an overhead between each minibatch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">datagen</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">rescale</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">datagen</span><span class="o">.</span><span class="n">flow_from_directory</span><span class="p">(</span>
        <span class="n">directory</span><span class="p">,</span>  <span class="c1"># This is the source directory for training images</span>
        <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">),</span>  <span class="c1"># All images will be resized to 150x150</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="c1"># Since we use binary_crossentropy loss, we need binary labels</span>
        <span class="n">class_mode</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">rescale</span></code> argument makes sure that the pixels will be represented by float values between 0 and 1, not integers between 0 and 255. Unfortunately, it is not possible (or very hard) to perform mean-removal using this method. The image data generator accepts additional arguments that we will discuss in the section on data augmentation.
<code class="docutils literal notranslate"><span class="pre">directory</span></code> must be set to the folder containing the images. We ask the generator to resize all images to 150x150 and will use a batch size of 64. As there are only two classes cat and dog, the labels will be binary (0 and 1).</p>
<p><strong>Q:</strong> Create two generators <code class="docutils literal notranslate"><span class="pre">train_generator</span></code> and <code class="docutils literal notranslate"><span class="pre">validation_generator</span></code> for the training and validation sets respectively.</p>
<p><strong>Q:</strong> Sample a minibatch from the training generator by calling <code class="docutils literal notranslate"><span class="pre">next()</span></code> on it (<code class="docutils literal notranslate"><span class="pre">X,</span> <span class="pre">t</span> <span class="pre">=</span> <span class="pre">train_generator.next()</span></code>) and display the first image. Call the cell multiple times.</p>
</div>
<div class="section" id="functional-api-of-keras">
<h2><span class="section-number">11.1.2. </span>Functional API of Keras<a class="headerlink" href="#functional-api-of-keras" title="Permalink to this headline">¶</a></h2>
<p>In the previous exercises, we used the Sequential API of keras, which stacks layers on top of each other:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>In this exercise, we will use the Functional API of keras, which gives much more freedom to the programmer. The main difference is that you can explicitly specify from which layer a layer should take its inputs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
<p>This allows to create complex architectures, for examples with several output layers.</p>
<p><strong>Q:</strong> Modify your CNN of last exercise so that it is defined with the Functional API and train it on MNIST.</p>
</div>
<div class="section" id="training-a-cnn-from-scratch">
<h2><span class="section-number">11.1.3. </span>Training a CNN from scratch<a class="headerlink" href="#training-a-cnn-from-scratch" title="Permalink to this headline">¶</a></h2>
<p>Let’s now train a randomly-initialized CNN on the dog vs. cat data. You are free to choose any architecture you like, the only requirements are:</p>
<ul class="simple">
<li><p>The input image must be 150x150x3:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The output neuron must use the logistic/sigmoid activation function (binary classification:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The loss function must be <code class="docutils literal notranslate"><span class="pre">'binary_crossentropy'</span></code> and the metric <code class="docutils literal notranslate"><span class="pre">binary_accuracy</span></code>:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;binary_accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>There is not a lot of data, so you can safely go deep with your architecture (i.e. with convolutional layers and max-pooling until the image dimensions are around 7x7), especially if you use the GPU on Colab.</p>
<p>To train and validate the network on the generators, just pass them to <code class="docutils literal notranslate"><span class="pre">model.fit()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
      <span class="n">train_generator</span><span class="p">,</span>
      <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
      <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_generator</span><span class="p">,</span>
      <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">history</span><span class="p">])</span>
</pre></div>
</div>
<p><strong>Q:</strong> Design a CNN and train it on the data for 30 epochs. A final validation accuracy around 72% -  75% is already good, you can then go to the next question.</p>
</div>
<div class="section" id="data-augmentation">
<h2><span class="section-number">11.1.4. </span>Data augmentation<a class="headerlink" href="#data-augmentation" title="Permalink to this headline">¶</a></h2>
<p>The 2000 training images will never be enough to train a CNN from scratch without overfitting, no matter how much regularization you use. A first trick that may help is <strong>data augmentation</strong>, i.e. to artificially create variations of each training image (translation, rotation, scaling, flipping, etc) while preserving the class of the images (a cat stays a cat after rotating the image).</p>
<p><code class="docutils literal notranslate"><span class="pre">ImageDataGenerator</span></code> allows to automatically apply various transformations when retrieving a minibatch (beware, it can be slow).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">datagen</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">ImageDataGenerator</span><span class="p">(</span>
    <span class="n">rescale</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span>
    <span class="n">rotation_range</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">fill_mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Refer the doc for the meaning of the parameters.</p>
<p>To investigate the transformations, let’s apply them on a single image, for example the first cat of the training set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="s1">&#39;/tmp/cats_and_dogs_filtered/train/cats/cat.0.jpg&#39;</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>  
<span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p>We can pass this image to the data generator and retrieve minibatches of augmented images:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">datagen</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">augmented</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Q:</strong> Display various augmented images. Vary the parameters individually by setting all but one to their default value in order to understand their effect.</p>
<p><strong>Q:</strong> Create an augmented training set using the parameters defined in the previous question (feel free to experiment, but that can cost time). Leave the validation generator without data augmentation (only <code class="docutils literal notranslate"><span class="pre">rescale=1./255</span></code>). Train the exact same network as before on this augmented data. What happens? You may need to train much longer in order to see the effect.</p>
</div>
<div class="section" id="id1">
<h2><span class="section-number">11.1.5. </span>Transfer learning<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Data augmentation helps randomly initialized to learn from small datasets, but the best solution is to start training with already good weights.</p>
<p><strong>Transfer learning</strong> allows to reuse the weights of a CNN trained on a bigger dataset (e.g. ImageNet) to either extract features for a shallow classifier or to allow fine-tuning of all weights.</p>
<p>Keras provides a considerable number of pre-trained CNNs:</p>
<p><a class="reference external" href="https://keras.io/api/applications/">https://keras.io/api/applications/</a></p>
<p>In this exercise, we will use the Xception network for feature extraction, but feel free to experiment with other architectures. To download the weights and create the keras model, simply call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xception</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">Xception</span><span class="p">(</span>
        <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;imagenet&quot;</span><span class="p">,</span>  <span class="c1"># Load weights pre-trained on ImageNet.</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="c1"># Input shape</span>
        <span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># Only the convolutional layers, not the last fully-connected ones</span>
    <span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">include_top=False</span></code> removes the last fully-connected layers used to predict the ImageNet classes, as we only care about the binary cat/dog classification.</p>
<p><strong>Q:</strong> Download Xception and print its summary. Make sense of the various layers (the paper might help: <a class="reference external" href="http://arxiv.org/abs/1610.02357">http://arxiv.org/abs/1610.02357</a>). What is the size of the final tensor?</p>
<p>Let’s now use transfer learning using this network. The first thing to do is to freeze Xception to make sure that it does learn from the cats and dogs data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xception</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>We can then connect Xception to the inputs, making sure again that the network won’t learn (in particular, the parameters of batch normalization are kept):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">xception</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>We can now use the layer <code class="docutils literal notranslate"><span class="pre">x</span></code> and stack what we want on top of it. Instead of flattening the 5x5x2048 tensor, it is usually better to apply <strong>average-pooling</strong> (or mean-pooling) on each 5x5 feature map to obtain a vector with 2048 elements:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Q:</strong> Perform a soft linear classification on this vector with 2048 elements to recognize cats from dogs (using non-augmented data). Do not hesitate to use some dropout and to boost your learning rate, there are only 2049 trainable parameters. Conclude.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./5-exercises"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="ex11-TransferLearning.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">11. </span>Transfer learning</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="11-TransferLearning-solution.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">11.2. </span>Transfer learning</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>