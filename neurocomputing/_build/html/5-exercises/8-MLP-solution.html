

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.2. Multi-layer Perceptron &#8212; Neurocomputing</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julien-vitay.net/lecturenotes-neurocomputing/5-exercises/8-MLP-solution.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1. Bibliography" href="../zreferences.html" />
    <link rel="prev" title="8.1. Multi-layer Perceptron" href="8-MLP.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">


<!-- Opengraph tags -->
<meta property="og:url"         content="https://julien-vitay.net/lecturenotes-neurocomputing/5-exercises/8-MLP-solution.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Multi-layer Perceptron" />
<meta property="og:description" content="Multi-layer Perceptron  The goal of this exercise is to implement a shallow multi-layer perceptron to perform non-linear classification. Let’s start with usual " />
<meta property="og:image"       content="https://julien-vitay.net/lecturenotes-neurocomputing/_static/tuc.svg" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/tuc.svg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neurocomputing</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Neurocomputing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/1-Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/2-Math.html">
   2. Math basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1-intro/3-Neurons.html">
   3. Neurons
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/1-Optimization.html">
   1. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/2-LinearRegression.html">
   2. Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/3-Regularization.html">
   3. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/4-LinearClassification.html">
   4. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/5-Multiclassification.html">
   5. Multi-class classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2-linear/6-LearningTheory.html">
   6. Learning theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Deep learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/1-NN.html">
   1. Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/2-DNN.html">
   2. Deep neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../3-deeplearning/3-CNN.html">
   3. Convolutional neural networks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ex1-Python.html">
   1. Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex2-Numpy.html">
   2. Numpy and Matplotlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex3-LinearRegression.html">
   3. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex4-MLR.html">
   4. Multiple Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex5-Crossvalidation.html">
   5. Cross-validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex6-LinearClassification.html">
   6. Linear classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ex7-SoftmaxClassifier.html">
   7. Softmax classifier
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="ex8-MLP.html">
   8. Multi-layer perceptron
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="8-MLP.html">
     8.1. Notebook
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.2. Solution
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   1. Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/5-exercises/8-MLP-solution.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/vitay/lecturenotes-neurocomputing/master?urlpath=tree/neurocomputing/5-exercises/8-MLP-solution.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/vitay/lecturenotes-neurocomputing/blob/master/neurocomputing/5-exercises/8-MLP-solution.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#structure-of-the-mlp">
   8.2.1. Structure of the MLP
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   8.2.2. Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#class-definition">
   8.2.3. Class definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backpropagation">
   8.2.4. Backpropagation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiments">
   8.2.5. Experiments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influence-of-the-number-of-hidden-neurons">
     8.2.5.1. Influence of the number of hidden neurons
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influence-of-the-learning-rate">
     8.2.5.2. Influence of the learning rate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influence-of-weight-initialization">
     8.2.5.3. Influence of weight initialization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influence-of-the-transfer-function">
     8.2.5.4. Influence of the transfer function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influence-of-data-normalization">
     8.2.5.5. Influence of data normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influence-of-randomization">
     8.2.5.6. Influence of randomization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influence-of-weight-initialization-part-2">
     8.2.5.7. Influence of weight initialization - part 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     8.2.5.8. Summary
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="multi-layer-perceptron">
<h1><span class="section-number">8.2. </span>Multi-layer Perceptron<a class="headerlink" href="#multi-layer-perceptron" title="Permalink to this headline">¶</a></h1>
<p>The goal of this exercise is to implement a shallow multi-layer perceptron to perform non-linear classification. Let’s start with usual imports, including the logistic function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="structure-of-the-mlp">
<h2><span class="section-number">8.2.1. </span>Structure of the MLP<a class="headerlink" href="#structure-of-the-mlp" title="Permalink to this headline">¶</a></h2>
<p>In this exercise, we will consider a MLP for non-linear binary classification composed of 2 input neurons <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, one output neuron <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(K\)</span> hidden neurons in a single hidden layer (<span class="math notranslate nohighlight">\(\mathbf{h}\)</span>).</p>
<a class="reference internal image-reference" href="../_images/mlp-example1.svg"><img alt="../_images/mlp-example1.svg" src="../_images/mlp-example1.svg" width="600" /></a>
<p>The output neuron is a vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> with one element that sums its inputs with <span class="math notranslate nohighlight">\(K\)</span> weights <span class="math notranslate nohighlight">\(W^2\)</span> and a bias <span class="math notranslate nohighlight">\(\mathbf{b}^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \sigma( W^2 \times \mathbf{h} + \mathbf{b}^2)\]</div>
<p>It uses the logistic transfer function:</p>
<div class="math notranslate nohighlight">
\[\sigma(x) = \dfrac{1}{1 + \exp -x}\]</div>
<p>As in logistic regression for linear classification, we will interpret <span class="math notranslate nohighlight">\(y\)</span> as the probability that the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> belongs to the positive class.</p>
<p><span class="math notranslate nohighlight">\(W^2\)</span> is a <span class="math notranslate nohighlight">\(1 \times K\)</span> matrix (we could interpret it as a vector, but this will make the computations easier) and <span class="math notranslate nohighlight">\(\mathbf{b}^2\)</span> is a vector with one element.</p>
<p>Each of the <span class="math notranslate nohighlight">\(K\)</span> hidden neurons receives 2 weights from the input layer, what gives a <span class="math notranslate nohighlight">\(K \times 2\)</span> weight matrix <span class="math notranslate nohighlight">\(W^1\)</span>, and <span class="math notranslate nohighlight">\(K\)</span> biases in the vector <span class="math notranslate nohighlight">\(\mathbf{b}^1\)</span>. They will also use the logistic activation function at first:</p>
<div class="math notranslate nohighlight">
\[\mathbf{h} = \sigma(W^1 \times \mathbf{x} + \mathbf{b}^1)\]</div>
<p>The goal is to implement the backpropagation algorithm by comparing the desired output <span class="math notranslate nohighlight">\(t\)</span> with the prediction <span class="math notranslate nohighlight">\(y\)</span>:</p>
<ul class="simple">
<li><p>The output error is a vector with one element:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\delta =  (\mathbf{t} - \mathbf{y})\]</div>
<ul class="simple">
<li><p>The backpropagated error is a vector with <span class="math notranslate nohighlight">\(K\)</span> elements:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\delta_\text{hidden} = \sigma'(W^1 \times \mathbf{x} + \mathbf{b}^1) \, W_2^T \times \delta\]</div>
<p>(<span class="math notranslate nohighlight">\(W^2\)</span> is a <span class="math notranslate nohighlight">\(1 \times K\)</span> matrix, so <span class="math notranslate nohighlight">\(W_2^T \times \delta\)</span> is a <span class="math notranslate nohighlight">\(K \times 1\)</span> vector. The vector <span class="math notranslate nohighlight">\(\sigma'(W^1 \times \mathbf{x} + \mathbf{b}^1)\)</span> is multiplied element-wise.)</p>
<ul class="simple">
<li><p>Parameter updates follow the delta learning rule:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\Delta W^1 = \eta \,  \delta_\text{hidden} \times \mathbf{x}^T\]</div>
<div class="math notranslate nohighlight">
\[\Delta \mathbf{b}^1 = \eta \, \delta_\text{hidden} \]</div>
<div class="math notranslate nohighlight">
\[\Delta W^2 = \eta \, \delta \, \mathbf{h}^T\]</div>
<div class="math notranslate nohighlight">
\[\Delta \mathbf{b}^2 = \eta \, \delta\]</div>
<p>Notice the transpose operators to obtain the correct shapes. You will remember that the derivative of the logistic function is given by:</p>
<div class="math notranslate nohighlight">
\[\sigma'(x)= \sigma(x) \, (1- \sigma(x))\]</div>
<p><strong>Q:</strong> Why do not we use the derivative of the transfer function of the output neuron when computing the output error <span class="math notranslate nohighlight">\(\delta\)</span>?</p>
<p><strong>A:</strong> As in logistic regression, we will interpret the output of the network as the probability of belonging to the positive class. We therefore use (implicitly) the cross-entropy / negative log-likelihood loss function, whose gradient does not include the derivative of the logistic function.</p>
</div>
<div class="section" id="data">
<h2><span class="section-number">8.2.2. </span>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<p>The MLP will be trained on a non-linear dataset with samples of each class forming a circle. Each sample has two input dimensions. In the cell below, blue points represent the positive class (t=1), orange ones the negative class (t=0).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">X</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="mf">0.03</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_7_0.png" src="../_images/8-MLP-solution_7_0.png" />
</div>
</div>
<p><strong>Q:</strong> Split the data into a training and test set (80/20). Make sure to call them <code class="docutils literal notranslate"><span class="pre">X_train,</span> <span class="pre">X_test,</span> <span class="pre">t_train,</span> <span class="pre">t_test</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">t_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="class-definition">
<h2><span class="section-number">8.2.3. </span>Class definition<a class="headerlink" href="#class-definition" title="Permalink to this headline">¶</a></h2>
<p>The neural network is entirely defined by its parameters, i.e. the weight matrices and bias vectors, as well the transfer function of the hidden neurons. In order to make your code more reusable, the MLP will be implemented as a Python class. The following cell defines the class, but we will explain it step by step afterwards.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">d</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span>  <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>  <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    
        <span class="c1"># Make sure x has 2 rows</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Hidden layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span><span class="p">)</span>

        <span class="c1"># Output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">)</span> 
        
    
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
            
            <span class="n">nb_errors</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Epoch</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>

                <span class="c1"># Feedforward pass: sets self.h and self.y</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
        
                <span class="c1"># Backpropagation</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">t_train</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        
                <span class="c1"># Predict the class:         </span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
                    <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="c1"># Count the number of misclassifications</span>
                <span class="k">if</span> <span class="n">t_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">c</span><span class="p">:</span> 
                    <span class="n">nb_errors</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># Compute the error rate</span>
            <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nb_errors</span><span class="o">/</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                
            <span class="c1"># Plot the decision function every 10 epochs</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">visualize</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">plot_classification</span><span class="p">()</span> 

            <span class="c1"># Stop when the error rate is 0</span>
            <span class="k">if</span> <span class="n">nb_errors</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">visualize</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">plot_classification</span><span class="p">()</span> 
                <span class="k">break</span>
                
        <span class="k">return</span> <span class="n">errors</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span>

    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    
        <span class="c1"># Make sure x has 2 rows</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># TODO: implement backpropagation</span>
    
    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">):</span>
    
        <span class="n">nb_errors</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>

            <span class="c1"># Feedforward pass</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span> 

            <span class="c1"># Predict the class:         </span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Count the number of misclassifications</span>
            <span class="k">if</span> <span class="n">t_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">c</span><span class="p">:</span> 
                <span class="n">nb_errors</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">nb_errors</span><span class="o">/</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="nf">plot_classification</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="c1"># Allow redrawing </span>
        <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="o">.</span><span class="mi">02</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="o">.</span><span class="mi">02</span><span class="p">))</span>

        <span class="n">x1</span> <span class="o">=</span> <span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>    
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">Z</span><span class="p">[</span><span class="n">Z</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">Z</span><span class="p">[</span><span class="n">Z</span><span class="o">&lt;=</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
        <span class="n">cm_bright</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">])</span>

        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm_bright</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">t_train</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm_bright</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">t_test</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm_bright</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>    
</pre></div>
</div>
</div>
</div>
<p>The constructor <code class="docutils literal notranslate"><span class="pre">__init__</span></code> of the class accepts several arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">d</span></code> is the number inputs, here 2.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">K</span></code> is the number of hidden neurons.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activation_function</span></code> is the function to use for the hidden neurons, for example the <code class="docutils literal notranslate"><span class="pre">logistic</span></code> function defined at the beginning of the notebook. Note that the name of the method can be stored as a variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_val</span></code> is the maximum value used to initialize the weight matrices.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eta</span></code> is the learning rate.</p></li>
</ul>
<p>The constructor starts by saving these arguments as attributes, so that they can be used in other method as <code class="docutils literal notranslate"><span class="pre">self.K</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">d</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
</pre></div>
</div>
<p>The constructor then initializes randomly the weight matrices and bias vectors, uniformly between <code class="docutils literal notranslate"><span class="pre">-max_val</span></code> and <code class="docutils literal notranslate"><span class="pre">max_val</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">W1</span>  <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span> 
<span class="bp">self</span><span class="o">.</span><span class="n">b1</span>  <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span> 
<span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p>You can then already create the <code class="docutils literal notranslate"><span class="pre">MLP</span></code> object and observe how the parameters are initialized:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="n">logistic</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Q:</strong> Create the object and print the weight matrices and bias vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameters</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">logistic</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">W1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">b1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">W2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">b2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 0.2014109  -0.19636799]
 [ 0.42138161  0.9755589 ]
 [ 0.77559954  0.81245345]
 [-0.21490413  0.28564539]
 [ 0.89274768  0.41063541]
 [ 0.38764279 -0.07455153]
 [-0.71281523  0.34234706]
 [-0.49607694 -0.59039613]
 [-0.73570212  0.56091497]
 [-0.77048217  0.18852194]
 [-0.59543404  0.29708458]
 [-0.51415925 -0.20029593]
 [ 0.81591457 -0.27881334]
 [ 0.53908135 -0.29012141]
 [ 0.18105231  0.9280227 ]]
[[-0.18317145]
 [ 0.95417896]
 [ 0.8330806 ]
 [-0.92704178]
 [-0.71180121]
 [ 0.79742339]
 [-0.1209483 ]
 [ 0.44747499]
 [-0.18992665]
 [ 0.68787671]
 [ 0.96372907]
 [ 0.18287163]
 [-0.93101563]
 [ 0.89461567]
 [-0.54624369]]
[[ 0.43741751  0.54536352 -0.75960379 -0.41689015 -0.41928498 -0.62307463
  -0.84525025 -0.11611377 -0.93432309  0.79274479 -0.75258973 -0.02029646
  -0.77224901  0.34469859 -0.85758708]]
[[0.37381855]]
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">feedforward</span></code> method takes a vector <code class="docutils literal notranslate"><span class="pre">x</span></code> as input, reshapes it to make sure it has two rows, and computes the hidden activation <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> and the prediction <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

    <span class="c1"># Make sure x has 2 rows</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Hidden layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span><span class="p">)</span>

    <span class="c1"># Output layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">)</span> 
</pre></div>
</div>
<p>Notice the use of <code class="docutils literal notranslate"><span class="pre">self.</span></code> to access attributes, as well as the use of <code class="docutils literal notranslate"><span class="pre">np.dot()</span></code> to mulitply vectors and matrices.</p>
<p><strong>Q:</strong> Using the randomly initialized weights, apply the <code class="docutils literal notranslate"><span class="pre">feedforward()</span></code> method to an input vector (for example <span class="math notranslate nohighlight">\([0.5, 0.5]\)</span>) and print <code class="docutils literal notranslate"><span class="pre">h</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>. What is the predicted class of the example?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>

<span class="n">mlp</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">h</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>

<span class="k">if</span> <span class="n">mlp</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;positive class&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;negative class&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.45495992]
 [0.83924878]
 [0.83577295]
 [0.29076508]
 [0.48497711]
 [0.72191268]
 [0.42404685]
 [0.47607789]
 [0.43111088]
 [0.59794181]
 [0.69307916]
 [0.4565211 ]
 [0.3401861 ]
 [0.7347964 ]
 [0.50207344]]
[[0.15550589]]
negative class
</pre></div>
</div>
</div>
</div>
<p>The class also provides a visualization method. It is not import to understand the code for the exercise, so you can safely skip it. It displays the training data as plain points, the test data as semi-transparent points and displays the decision function as a background color (all points in the blue region will be classified as negative examples).</p>
<p><strong>Q:</strong> Plot the initial classification on the dataset with random weights. Is there a need for learning? Reinitialize the weights and biases multiple times. What do you observe?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">logistic</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
<span class="n">mlp</span><span class="o">.</span><span class="n">plot_classification</span><span class="p">()</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_18_0.png" src="../_images/8-MLP-solution_18_0.png" />
</div>
</div>
<p><strong>A:</strong> The output neurons answers the same class for the whole input space, or is linear at best depending on initialization. Let’s learn!</p>
</div>
<div class="section" id="backpropagation">
<h2><span class="section-number">8.2.4. </span>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">train()</span></code> method implements the training loop you have already implemented several times: several epochs over the training set, making a prediction for each input and modifying the parameters according to the prediction error:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>

        <span class="n">nb_errors</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Epoch</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>

            <span class="c1"># Feedforward pass: sets self.h and self.y</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>

            <span class="c1"># Backpropagation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">t_train</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

            <span class="c1"># Predict the class:         </span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Count the number of misclassifications</span>
            <span class="k">if</span> <span class="n">t_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">c</span><span class="p">:</span> 
                <span class="n">nb_errors</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Compute the error rate</span>
        <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nb_errors</span><span class="o">/</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Plot the decision function every 10 epochs</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">visualize</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">plot_classification</span><span class="p">()</span> 

        <span class="c1"># Stop when the error rate is 0</span>
        <span class="k">if</span> <span class="n">nb_errors</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">visualize</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">plot_classification</span><span class="p">()</span> 
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">errors</span><span class="p">,</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span>
</pre></div>
</div>
<p>The training methods stops after <code class="docutils literal notranslate"><span class="pre">nb_epochs</span></code> epochs or when no error is made during the last epoch. The decision function is visualized every 10 epochs to better understand what is happening. The method returns a list containing the error rate after each epoch, as well as the number of epochs needed to reach an error rate of 0.</p>
<p>The only thing missing is the <code class="docutils literal notranslate"><span class="pre">backprop(x,</span> <span class="pre">t)</span></code> method, which currently does nothing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>

    <span class="c1"># Make sure x has 2 rows</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># TODO: implement backpropagation</span>
</pre></div>
</div>
<p><strong>Q:</strong>  Implement the <em>online</em> backpropagation algorithm.</p>
<p>All you have to do is to backpropagate the output error and adapt the parameters using the delta learning rule:</p>
<ol class="simple">
<li><p>compute the output error <code class="docutils literal notranslate"><span class="pre">delta</span></code>.</p></li>
<li><p>compute the backpropagated error <code class="docutils literal notranslate"><span class="pre">delta_hidden</span></code>.</p></li>
<li><p>increment the parameters <code class="docutils literal notranslate"><span class="pre">self.W1,</span> <span class="pre">self.b1,</span> <span class="pre">self.W2,</span> <span class="pre">self.b2</span></code> accordingly.</p></li>
</ol>
<p>The only difficulty is to take care of the shape of each matrix (before multiplying two matrices or vectors, test what their shape is).</p>
<p><em>Note:</em> you can either edit directly the cell containing the definition of the class, or create a new class <code class="docutils literal notranslate"><span class="pre">TrainableMLP</span></code> inheriting from the class <code class="docutils literal notranslate"><span class="pre">MLP</span></code> and simply redefine the <code class="docutils literal notranslate"><span class="pre">backprop()</span></code> method. The solution will use the second option to be more readable, but it does not matter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TrainableMLP</span> <span class="p">(</span><span class="n">MLP</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    
        <span class="c1"># Make sure x has 2 rows</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Output error</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span> 

        <span class="c1"># Hidden error</span>
        <span class="n">delta_hidden</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">)</span>
        
        <span class="c1"># Learn the output weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># Learn the output bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta</span>

        <span class="c1"># Learn the hidden weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_hidden</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="c1"># Learn the hidden biases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta_hidden</span>
        
</pre></div>
</div>
</div>
</div>
<p><strong>Q:</strong> Train the MLP for 1000 epochs on the data using a learning rate of 0.05, 15 hidden neurons and weights initialized between -1 and 1. Plot the evolution of the training error.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">logistic</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_23_0.png" src="../_images/8-MLP-solution_23_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 976
Training accuracy: 1.0
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_23_2.png" src="../_images/8-MLP-solution_23_2.png" />
</div>
</div>
<p><strong>Q:</strong> Use the <code class="docutils literal notranslate"><span class="pre">test()</span></code> method to compute the error on the test set. What is the test accuracy of your network after training? Compare it to the training accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test accuracy: 1.0
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="experiments">
<h2><span class="section-number">8.2.5. </span>Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">¶</a></h2>
<div class="section" id="influence-of-the-number-of-hidden-neurons">
<h3><span class="section-number">8.2.5.1. </span>Influence of the number of hidden neurons<a class="headerlink" href="#influence-of-the-number-of-hidden-neurons" title="Permalink to this headline">¶</a></h3>
<p><strong>Q:</strong> Try different values for the number of hidden neurons <span class="math notranslate nohighlight">\(K\)</span> (e.g. 2, 5, 10, 15, 20, 25, 50…) and observe how the accuracy and speed of convergence evolve.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">logistic</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_27_0.png" src="../_images/8-MLP-solution_27_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 2000
Training accuracy: 0.9875
Test accuracy: 0.95
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_27_2.png" src="../_images/8-MLP-solution_27_2.png" />
</div>
</div>
<p><strong>A:</strong> Surprisingly, 3 hidden neurons are enough for the non-linear dataset, although you might need more epochs… This problem is really easy. The more hidden neurons, the faster it converges (in terms of epochs, not computation time…), as the model is more flexible.</p>
</div>
<div class="section" id="influence-of-the-learning-rate">
<h3><span class="section-number">8.2.5.2. </span>Influence of the learning rate<a class="headerlink" href="#influence-of-the-learning-rate" title="Permalink to this headline">¶</a></h3>
<p><strong>Q:</strong> Vary the learning rate between extreme values. How does the performance evolve?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">logistic</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_30_0.png" src="../_images/8-MLP-solution_30_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 1000
Training accuracy: 0.9875
Test accuracy: 1.0
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_30_2.png" src="../_images/8-MLP-solution_30_2.png" />
</div>
</div>
<p><strong>A:</strong> for such small networks and easy problems, a high learning rate of 0.2 works well (ca 200 epochs to converge). This won’t be true for deeper networks. Learning rates smaller than 0.01 take forever.</p>
</div>
<div class="section" id="influence-of-weight-initialization">
<h3><span class="section-number">8.2.5.3. </span>Influence of weight initialization<a class="headerlink" href="#influence-of-weight-initialization" title="Permalink to this headline">¶</a></h3>
<p><strong>Q:</strong> The weights are initialized randomly between -1 and 1. Try to initialize them to 0. Does it work? Why?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">logistic</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_33_0.png" src="../_images/8-MLP-solution_33_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 1000
Training accuracy: 0.525
Test accuracy: 0.30000000000000004
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_33_2.png" src="../_images/8-MLP-solution_33_2.png" />
</div>
</div>
<p><strong>A:</strong> This is a simple example of <strong>vanishing gradient</strong>. The backpropagated error gets multiplied by <code class="docutils literal notranslate"><span class="pre">W2</span></code>, which is initially zero. There is therefore no backpropagated error, the first layer cannot learn anything for a quite long time. The global function stays linear.</p>
<p><strong>Q:</strong> For a fixed number of hidden neurons (e.g. <span class="math notranslate nohighlight">\(K=15\)</span>) and a correct value of <code class="docutils literal notranslate"><span class="pre">eta</span></code>, train 10 times the network with different initial weights and superimpose on the same plot the evolution of the training error. Conclude.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Create the MLP</span>
    <span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">logistic</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

    <span class="c1"># Train the MLP</span>
    <span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_36_0.png" src="../_images/8-MLP-solution_36_0.png" />
</div>
</div>
<p><strong>A:</strong> Because of the random initialization of the weights, some networks converge faster than others, as their weights are initially closer to the solution and gradient descent is an iterative method.</p>
<p>With the current configuration, a maximum of 1000 epochs is not too much, but we can clearly do better.</p>
</div>
<div class="section" id="influence-of-the-transfer-function">
<h3><span class="section-number">8.2.5.4. </span>Influence of the transfer function<a class="headerlink" href="#influence-of-the-transfer-function" title="Permalink to this headline">¶</a></h3>
<p><strong>Q:</strong> Modify the <code class="docutils literal notranslate"><span class="pre">backprop()</span></code> method so that it applies backpropagation correctly for any of the four activation functions:</p>
<ul class="simple">
<li><p>linear</p></li>
<li><p>logistic</p></li>
<li><p>tanh</p></li>
<li><p>relu</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Linear transfer function</span>
<span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># tanh transfer function </span>
<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># ReLU transfer function</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Remember that the derivatives of these activations functions are easy to compute:</p>
<ul class="simple">
<li><p>linear: <span class="math notranslate nohighlight">\(f'(x) = 1\)</span></p></li>
<li><p>logistic: <span class="math notranslate nohighlight">\(f'(x) = f(x) \, (1 - f(x))\)</span></p></li>
<li><p>tanh: <span class="math notranslate nohighlight">\(f'(x) = 1 - f^2(x)\)</span></p></li>
<li><p>relu: <span class="math notranslate nohighlight">\(f'(x) = \begin{cases}1 \; \text{if} \; x&gt;0\\ 0 \; \text{if} \; x \leq 0\\ \end{cases}\)</span></p></li>
</ul>
<p><em>Hint:</em> <code class="docutils literal notranslate"><span class="pre">activation_function</span></code> is a variable like others, although it is the name of a method. You can apply comparisons on it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">==</span> <span class="n">linear</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">something</span>
<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">==</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">something</span>
<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">==</span> <span class="n">tanh</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">something</span>
<span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">==</span> <span class="n">relu</span><span class="p">:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">something</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TrainableMLP</span> <span class="p">(</span><span class="n">MLP</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    
        <span class="c1"># Make sure x has 2 rows</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Output error</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span> 
        
        <span class="c1"># Derivative of the transfer function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">==</span> <span class="n">linear</span><span class="p">:</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">==</span> <span class="n">logistic</span><span class="p">:</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">==</span> <span class="n">tanh</span><span class="p">:</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">==</span> <span class="n">relu</span><span class="p">:</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">diff</span><span class="p">[</span><span class="n">diff</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">diff</span><span class="p">[</span><span class="n">diff</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    
        <span class="c1"># Hidden error</span>
        <span class="n">delta_hidden</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span>
        
        <span class="c1"># Learn the output weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># Learn the output bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta</span>

        <span class="c1"># Learn the hidden weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_hidden</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="c1"># Learn the hidden biases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta_hidden</span>
        
</pre></div>
</div>
</div>
</div>
<p><strong>Q:</strong> Use a linear transfer function for the hidden neurons. How does performance evolve? Is the non-linearity of the transfer function important for learning?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">activation_function</span><span class="o">=</span><span class="n">linear</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_43_0.png" src="../_images/8-MLP-solution_43_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 1000
Training accuracy: 0.5125
Test accuracy: 0.30000000000000004
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_43_2.png" src="../_images/8-MLP-solution_43_2.png" />
</div>
</div>
<p><strong>Q:</strong> Use this time the hyperbolic tangent function as a transfer function for the hidden neurons. Does it improve learning?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">activation_function</span><span class="o">=</span><span class="n">tanh</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_45_0.png" src="../_images/8-MLP-solution_45_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 262
Training accuracy: 1.0
Test accuracy: 0.9
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_45_2.png" src="../_images/8-MLP-solution_45_2.png" />
</div>
</div>
<p><strong>Q:</strong> Use the Rectified Linear Unit (ReLU) transfer function. What does it change? Conclude on the importance of the transfer function for the hidden neurons. Select the best one from now on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">activation_function</span><span class="o">=</span><span class="n">relu</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_47_0.png" src="../_images/8-MLP-solution_47_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 117
Training accuracy: 1.0
Test accuracy: 0.85
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_47_2.png" src="../_images/8-MLP-solution_47_2.png" />
</div>
</div>
<p><strong>A:</strong> Using the linear function does not work at all, as the equivalent input/output model would be linear, and the data is non-linear. logistic and tanh work approximately in the same way. ReLU is the best.</p>
</div>
<div class="section" id="influence-of-data-normalization">
<h3><span class="section-number">8.2.5.5. </span>Influence of data normalization<a class="headerlink" href="#influence-of-data-normalization" title="Permalink to this headline">¶</a></h3>
<p>The input data returned by <code class="docutils literal notranslate"><span class="pre">make_circles</span></code> is nicely center around 0, with values between -1 and 1. What happens if this is not the case with your data?</p>
<p><strong>Q:</strong> Shift the input data <code class="docutils literal notranslate"><span class="pre">X</span></code> using the formula:</p>
<div class="math notranslate nohighlight">
\[X_\text{shifted} = 8 \, X + 2\]</div>
<p>regenerate the training and test sets and train the MLP on them. What do you observe?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_shifted</span> <span class="o">=</span> <span class="mi">8</span><span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="mi">2</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_shifted</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_shifted</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_shifted</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_shifted</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">t_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_shifted</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_50_0.png" src="../_images/8-MLP-solution_50_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">activation_function</span><span class="o">=</span><span class="n">relu</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_51_0.png" src="../_images/8-MLP-solution_51_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 1000
Training accuracy: 0.7
Test accuracy: 0.6666666666666667
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_51_2.png" src="../_images/8-MLP-solution_51_2.png" />
</div>
</div>
<p><strong>A:</strong> It does not work anymore…</p>
<p><strong>Q:</strong> Normalize the shifted data so that it has a mean of 0 and a variance of 1 in each dimension, using the formula:</p>
<div class="math notranslate nohighlight">
\[X_\text{normalized} = \dfrac{X_\text{shifted} - \text{mean}(X_\text{shifted})}{\text{std}(X_\text{shifted})}\]</div>
<p>and retrain the network. Conclude.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_shifted</span> <span class="o">-</span> <span class="n">X_shifted</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">X_shifted</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_scaled</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_scaled</span><span class="p">[</span><span class="n">t</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">t_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_54_0.png" src="../_images/8-MLP-solution_54_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">activation_function</span><span class="o">=</span><span class="n">relu</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">TrainableMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_55_0.png" src="../_images/8-MLP-solution_55_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 291
Training accuracy: 1.0
Test accuracy: 0.9666666666666667
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_55_2.png" src="../_images/8-MLP-solution_55_2.png" />
</div>
</div>
<p><strong>A:</strong> Now it works again. Conclusion: <strong>always</strong> normalize the data before training so that each input dimension has a mean of 0 and variance of 1. Refer batch normalization later.</p>
</div>
<div class="section" id="influence-of-randomization">
<h3><span class="section-number">8.2.5.6. </span>Influence of randomization<a class="headerlink" href="#influence-of-randomization" title="Permalink to this headline">¶</a></h3>
<p>The training loop we used until now iterated over the training samples in the exact same order at every epoch. The samples are therefore not i.i.d (independent and identically distributed) as they follow the same sequence.</p>
<p><strong>Q:</strong> Modify the <code class="docutils literal notranslate"><span class="pre">train()</span></code> method so that the indices of the training samples are randomized between two epochs. Check the doc of <code class="docutils literal notranslate"><span class="pre">rng.permutation()</span></code> for help.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">StochasticMLP</span> <span class="p">(</span><span class="n">TrainableMLP</span><span class="p">):</span>
        
    
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
            
            <span class="n">nb_errors</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Epoch</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">rng</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>

                <span class="c1"># Feedforward pass: sets self.h and self.y</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
        
                <span class="c1"># Backpropagation</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">t_train</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        
                <span class="c1"># Predict the class:         </span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
                    <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="c1"># Count the number of misclassifications</span>
                <span class="k">if</span> <span class="n">t_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">c</span><span class="p">:</span> 
                    <span class="n">nb_errors</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># Compute the error rate</span>
            <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nb_errors</span><span class="o">/</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                
            <span class="c1"># Plot the decision function every 10 epochs</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">visualize</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">plot_classification</span><span class="p">()</span> 

            <span class="c1"># Stop when the error rate is 0</span>
            <span class="k">if</span> <span class="n">nb_errors</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">visualize</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">plot_classification</span><span class="p">()</span> 
                <span class="k">break</span>
                
        <span class="k">return</span> <span class="n">errors</span><span class="p">,</span> <span class="n">epoch</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">activation_function</span><span class="o">=</span><span class="n">relu</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">StochasticMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_59_0.png" src="../_images/8-MLP-solution_59_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 61
Training accuracy: 1.0
Test accuracy: 0.9
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_59_2.png" src="../_images/8-MLP-solution_59_2.png" />
</div>
</div>
</div>
<div class="section" id="influence-of-weight-initialization-part-2">
<h3><span class="section-number">8.2.5.7. </span>Influence of weight initialization - part 2<a class="headerlink" href="#influence-of-weight-initialization-part-2" title="Permalink to this headline">¶</a></h3>
<p>According to the empirical analysis by Glorot and Bengio in “Understanding the difficulty of training deep feedforward neural networks”, the optimal initial values for the weights between two layers of a MLP are uniformly taken in the range:</p>
<div class="math notranslate nohighlight">
\[
   \mathcal{U} ( - \sqrt{\frac{6}{N_{\text{in}}+N_{\text{out}}}} , \sqrt{\frac{6}{N_{\text{in}}+N_{\text{out}}}} )
\]</div>
<p>where <span class="math notranslate nohighlight">\(N_{\text{in}}\)</span> is the number of neurons in the first layer and <span class="math notranslate nohighlight">\(N_{\text{out}}\)</span> the number of neurons in the second layer.</p>
<p><strong>Q:</strong> Modify the constructor of your class to initialize both hidden and output weights with this new range. The biases should be initialized to 0. What is the effect?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GlorotMLP</span> <span class="p">(</span><span class="n">StochasticMLP</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">d</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        
        <span class="n">max_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span><span class="o">/</span><span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="n">K</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span>  <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
        <span class="n">max_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span><span class="o">/</span><span class="p">(</span><span class="n">K</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">max_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">activation_function</span><span class="o">=</span><span class="n">relu</span>

<span class="c1"># Create the MLP</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">GlorotMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="c1"># Train the MLP</span>
<span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Test the MLP</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Training error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8-MLP-solution_62_0.png" src="../_images/8-MLP-solution_62_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of epochs needed: 72
Training accuracy: 1.0
Test accuracy: 0.9666666666666667
</pre></div>
</div>
<img alt="../_images/8-MLP-solution_62_2.png" src="../_images/8-MLP-solution_62_2.png" />
</div>
</div>
<p><strong>A:</strong> the decision function seems already quite centered at the beginning (closer to the solution), so it converges faster.</p>
</div>
<div class="section" id="summary">
<h3><span class="section-number">8.2.5.8. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p><strong>Q:</strong> Now that we optimized the MLP, it is time to cross-validate again the number of hidden neurons and the learning rate. As the networks always get a training error rate of 0 and the test set is not very relevant, the maincriteria will be the number of epochs needed on average to converge. Find the best MLP for the dataset (there is not a single solution), for example by iterating over multiple values of <code class="docutils literal notranslate"><span class="pre">K</span></code> and <code class="docutils literal notranslate"><span class="pre">eta</span></code>. What do you think of the change in performance between the first naive implementation and the final one? What were the most critical changes?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">relu</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
    <span class="c1"># Create the MLP</span>
    <span class="n">mlp</span> <span class="o">=</span> <span class="n">GlorotMLP</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

    <span class="c1"># Train the MLP</span>
    <span class="n">training_error</span><span class="p">,</span> <span class="n">nb_epochs</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Test the MLP</span>
    <span class="n">test_error</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;K:&#39;</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="s1">&#39;eta:&#39;</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of epochs needed:&#39;</span><span class="p">,</span> <span class="n">nb_epochs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">training_error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">test_error</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>

<span class="k">for</span> <span class="n">K</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">25</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">eta</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]:</span>
        <span class="n">run</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>K: 10 eta: 0.01
Number of epochs needed: 266
Training accuracy: 1.0
Test accuracy: 0.7
--------------------
K: 10 eta: 0.05
Number of epochs needed: 86
Training accuracy: 1.0
Test accuracy: 0.8666666666666667
--------------------
K: 10 eta: 0.1
Number of epochs needed: 53
Training accuracy: 1.0
Test accuracy: 0.9
--------------------
K: 10 eta: 0.2
Number of epochs needed: 67
Training accuracy: 1.0
Test accuracy: 0.9666666666666667
--------------------
K: 10 eta: 0.3
Number of epochs needed: 84
Training accuracy: 1.0
Test accuracy: 0.9666666666666667
--------------------
K: 15 eta: 0.01
Number of epochs needed: 194
Training accuracy: 1.0
Test accuracy: 0.8666666666666667
--------------------
K: 15 eta: 0.05
Number of epochs needed: 89
Training accuracy: 1.0
Test accuracy: 0.9666666666666667
--------------------
K: 15 eta: 0.1
Number of epochs needed: 68
Training accuracy: 1.0
Test accuracy: 0.9666666666666667
--------------------
K: 15 eta: 0.2
Number of epochs needed: 56
Training accuracy: 1.0
Test accuracy: 0.9666666666666667
--------------------
K: 15 eta: 0.3
Number of epochs needed: 84
Training accuracy: 1.0
Test accuracy: 1.0
--------------------
K: 20 eta: 0.01
Number of epochs needed: 219
Training accuracy: 1.0
Test accuracy: 0.8666666666666667
--------------------
K: 20 eta: 0.05
Number of epochs needed: 90
Training accuracy: 1.0
Test accuracy: 0.8333333333333334
--------------------
K: 20 eta: 0.1
Number of epochs needed: 66
Training accuracy: 1.0
Test accuracy: 0.9666666666666667
--------------------
K: 20 eta: 0.2
Number of epochs needed: 63
Training accuracy: 1.0
Test accuracy: 0.9333333333333333
--------------------
K: 20 eta: 0.3
Number of epochs needed: 51
Training accuracy: 1.0
Test accuracy: 0.7333333333333334
--------------------
K: 25 eta: 0.01
Number of epochs needed: 148
Training accuracy: 1.0
Test accuracy: 0.8333333333333334
--------------------
K: 25 eta: 0.05
Number of epochs needed: 92
Training accuracy: 1.0
Test accuracy: 0.8333333333333334
--------------------
K: 25 eta: 0.1
Number of epochs needed: 58
Training accuracy: 1.0
Test accuracy: 1.0
--------------------
K: 25 eta: 0.2
Number of epochs needed: 58
Training accuracy: 1.0
Test accuracy: 0.8666666666666667
--------------------
K: 25 eta: 0.3
Number of epochs needed: 63
Training accuracy: 1.0
Test accuracy: 0.9666666666666667
--------------------
</pre></div>
</div>
</div>
</div>
<p><strong>A:</strong> Quite small changes can drastically change the performance of the network, both in terms of accuracy and training time. Data normalization, Glorot initialization and the use of ReLU are the most determinant here.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./5-exercises"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="8-MLP.html" title="previous page"><span class="section-number">8.1. </span>Multi-layer Perceptron</a>
    <a class='right-next' id="next-link" href="../zreferences.html" title="next page"><span class="section-number">1. </span>Bibliography</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Julien Vitay - julien.vitay@informatik.tu-chemnitz.de<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            Technische Universität Chemnitz - Faculty of Computer Science - Professorship for Artificial Intelligence
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>